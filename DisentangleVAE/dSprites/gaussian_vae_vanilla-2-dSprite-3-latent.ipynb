{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7ff4d1aaccc0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2376x2376 with 121 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz'\n",
    "                      , encoding='bytes')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]\n",
    "\n",
    "# Define number of values per latents and functions to convert to indices\n",
    "latents_sizes =  np.array([ 1,  3,  6, 40, 32, 32])\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "  return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "# Helper function to show images\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "  ncols = int(np.ceil(num_images**0.5))\n",
    "  nrows = int(np.ceil(num_images / ncols))\n",
    "  _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for ax_i, ax in enumerate(axes):\n",
    "    if ax_i < num_images:\n",
    "      ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "    else:\n",
    "      ax.axis('off')\n",
    "\n",
    "def show_density(imgs):\n",
    "  _, ax = plt.subplots()\n",
    "  ax.imshow(imgs.mean(axis=0), interpolation='nearest', cmap='Greys_r')\n",
    "  ax.grid('off')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "from copy import deepcopy\n",
    "## Fix posX latent to left\n",
    "#latents_sampled = sample_latent(size=5000)\n",
    "latents_sampled = deepcopy(latents_classes)\n",
    "latents_sampled[:, [4,5]] = 15\n",
    "latents_sampled[:,2]= 5\n",
    "\n",
    "\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[np.unique(indices_sampled)]\n",
    "\n",
    "#np.unique(indices_sampled)\n",
    "\n",
    "# Samples\n",
    "show_images_grid(imgs_sampled,len(np.unique(indices_sampled)))\n",
    "\n",
    "# Show the density too to check\n",
    "show_density(imgs_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(69)\n",
    "ff = imgs_sampled\n",
    "n_data =  ff.shape[0]\n",
    "n_train = int(np.ceil(n_data*0.8))\n",
    "\n",
    "print(n_train)\n",
    "idx_train = random.sample(range(n_data), n_train)\n",
    "idx_test = np.delete(range(n_data),idx_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   6,  14,  15,  20,  22,  23,  24,  28,  32,  43,  46,\n",
       "        51,  55,  67,  85,  92,  93,  96, 104, 105, 110, 118])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape[0]*0.8\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train / validation folds\n",
    "#np.random.seed(42)\n",
    "\n",
    "img_rows = ff.shape[1]\n",
    "img_cols = ff.shape[2]\n",
    "\n",
    "n_pixels = img_rows * img_cols\n",
    "x_train = ff[idx_train]\n",
    "x_test = ff[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1,30]\n",
    "x_train[1,30].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32') \n",
    "x_train = x_train.reshape((len(x_train), n_pixels))\n",
    "x_test = x_test.reshape((len(x_test), n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1,2400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4608x4608 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_examples(data, n=None, n_cols=20, thumbnail_cb=None):\n",
    "    if n is None:\n",
    "        n = len(data)    \n",
    "    n_rows = int(np.ceil(n / float(n_cols)))\n",
    "    figure = np.zeros((img_rows * n_rows, img_cols * n_cols))\n",
    "    for k, x in enumerate(data[:n]):\n",
    "        r = k // n_cols\n",
    "        c = k % n_cols\n",
    "        figure[r * img_rows: (r + 1) * img_rows,\n",
    "               c * img_cols: (c + 1) * img_cols] = x\n",
    "        if thumbnail_cb is not None:\n",
    "            thumbnail_cb(locals())\n",
    "        \n",
    "    plt.figure(figsize=(64, 64))\n",
    "    plt.imshow(figure)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "show_examples(ff, n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "#     return z_mean + K.exp(0.5 ) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num,\n",
    "                 z_m_m, \n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_face\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    #x_test = data\n",
    "    latent_dim = latent_dim\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(model_name, \"face_over_latent.png\")\n",
    "    n = 20\n",
    "    #digit_size = 28\n",
    "    img_rows, img_cols = 64, 64\n",
    "    figure = np.zeros((img_rows , img_cols * n))\n",
    "    grid_x = np.linspace(-5, 5, n)\n",
    "    #grid_y = np.linspace(-5, 5, n)[::-1]\n",
    "    z_sample = np.zeros((1,latent_dim))\n",
    "    z_sample[0,:] = z_m_m \n",
    "    \n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample[0,latent_num] = xi\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_rows, img_cols)\n",
    "        figure[0: img_rows,j * img_cols: (j + 1) * img_cols] = digit\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #start_range = digit_size // 2\n",
    "    #end_range = n * digit_size + start_range + 1\n",
    "    #pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    #sample_range_x = np.round(grid_x, 1)\n",
    "    #sample_range_y = np.round(grid_y, 1)\n",
    "    #plt.xticks(pixel_range, sample_range_x)\n",
    "    #plt.yticks(pixel_range, sample_range_y)\n",
    "    #plt.xlabel(\"z[0]\")\n",
    "    #plt.ylabel(\"z[1]\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "original_dim = n_pixels\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim1 = 512\n",
    "intermediate_dim2 = 256\n",
    "intermediate_dim3 = 64\n",
    "\n",
    "batch_size = 20\n",
    "latent_dim = 3\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x1)\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(x2)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x3)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the mean of z, so that mean(m_z)=0 and cov(m_z)=I\n",
    "def standardize(z_mean):\n",
    "    z_m_m = K.mean(z_mean,axis=0, keepdims=True)\n",
    "    z1 = z_mean - z_m_m\n",
    "    n = tf.cast(K.shape(z_mean)[0], tf.float32)\n",
    "    cov = K.transpose(z1) @ z1 /n\n",
    "    \n",
    "    D = tf.diag(tf.diag_part(cov)) ** 0.5\n",
    "    \n",
    "    L = tf.linalg.inv(tf.transpose(tf.cholesky(cov)))\n",
    "       \n",
    "#     z2 = z1 @ L @ D +  z_m_m\n",
    "    z2 = z1 @ L  +  z_m_m\n",
    "\n",
    "    return( z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_mean_std = Lambda(standardize, output_shape=(latent_dim,), name='z_mean_std')(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 3)            195         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 3)            195         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 3)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,245,830\n",
      "Trainable params: 2,245,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 2,249,728\n",
      "Trainable params: 2,249,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z')\n",
    "\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(latent_inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x3)\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(x2)\n",
    "\n",
    "# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x1)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 3), (None, 3), (N 2245830   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 4096)              2249728   \n",
      "=================================================================\n",
      "Total params: 4,495,558\n",
      "Trainable params: 4,495,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test )\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "\n",
    "#     reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(1E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/1000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 2687.4142 - val_loss: 1838.1569\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1838.15692, saving model to weights.hdf5\n",
      "Epoch 2/1000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 1081.5974 - val_loss: 792.5629\n",
      "\n",
      "Epoch 00002: val_loss improved from 1838.15692 to 792.56292, saving model to weights.hdf5\n",
      "Epoch 3/1000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 607.2873 - val_loss: 407.9268\n",
      "\n",
      "Epoch 00003: val_loss improved from 792.56292 to 407.92676, saving model to weights.hdf5\n",
      "Epoch 4/1000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 330.5405 - val_loss: 312.5671\n",
      "\n",
      "Epoch 00004: val_loss improved from 407.92676 to 312.56710, saving model to weights.hdf5\n",
      "Epoch 5/1000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 294.5449 - val_loss: 253.2273\n",
      "\n",
      "Epoch 00005: val_loss improved from 312.56710 to 253.22729, saving model to weights.hdf5\n",
      "Epoch 6/1000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 247.0062 - val_loss: 254.2288\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 253.22729\n",
      "Epoch 7/1000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 240.4340 - val_loss: 237.6001\n",
      "\n",
      "Epoch 00007: val_loss improved from 253.22729 to 237.60010, saving model to weights.hdf5\n",
      "Epoch 8/1000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 225.8098 - val_loss: 238.8492\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 237.60010\n",
      "Epoch 9/1000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 225.3634 - val_loss: 226.7585\n",
      "\n",
      "Epoch 00009: val_loss improved from 237.60010 to 226.75853, saving model to weights.hdf5\n",
      "Epoch 10/1000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 220.2668 - val_loss: 226.3250\n",
      "\n",
      "Epoch 00010: val_loss improved from 226.75853 to 226.32498, saving model to weights.hdf5\n",
      "Epoch 11/1000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 219.8359 - val_loss: 227.5581\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 226.32498\n",
      "Epoch 12/1000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 218.2336 - val_loss: 224.5736\n",
      "\n",
      "Epoch 00012: val_loss improved from 226.32498 to 224.57361, saving model to weights.hdf5\n",
      "Epoch 13/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 217.1017 - val_loss: 225.7579\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 224.57361\n",
      "Epoch 14/1000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 215.8072 - val_loss: 223.1434\n",
      "\n",
      "Epoch 00014: val_loss improved from 224.57361 to 223.14342, saving model to weights.hdf5\n",
      "Epoch 15/1000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 214.4908 - val_loss: 222.9050\n",
      "\n",
      "Epoch 00015: val_loss improved from 223.14342 to 222.90503, saving model to weights.hdf5\n",
      "Epoch 16/1000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 213.7570 - val_loss: 221.3767\n",
      "\n",
      "Epoch 00016: val_loss improved from 222.90503 to 221.37668, saving model to weights.hdf5\n",
      "Epoch 17/1000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 213.7043 - val_loss: 222.3550\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 221.37668\n",
      "Epoch 18/1000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 213.5974 - val_loss: 220.6090\n",
      "\n",
      "Epoch 00018: val_loss improved from 221.37668 to 220.60903, saving model to weights.hdf5\n",
      "Epoch 19/1000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 214.0402 - val_loss: 223.4695\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 220.60903\n",
      "Epoch 20/1000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 213.2673 - val_loss: 221.5385\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 220.60903\n",
      "Epoch 21/1000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 212.6722 - val_loss: 221.0008\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 220.60903\n",
      "Epoch 22/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 212.8073 - val_loss: 220.3503\n",
      "\n",
      "Epoch 00022: val_loss improved from 220.60903 to 220.35027, saving model to weights.hdf5\n",
      "Epoch 23/1000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 212.4537 - val_loss: 223.0556\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 220.35027\n",
      "Epoch 24/1000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 212.1184 - val_loss: 220.2625\n",
      "\n",
      "Epoch 00024: val_loss improved from 220.35027 to 220.26253, saving model to weights.hdf5\n",
      "Epoch 25/1000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 211.8610 - val_loss: 220.0599\n",
      "\n",
      "Epoch 00025: val_loss improved from 220.26253 to 220.05992, saving model to weights.hdf5\n",
      "Epoch 26/1000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 211.7498 - val_loss: 220.8980\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 220.05992\n",
      "Epoch 27/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 211.6005 - val_loss: 218.1764\n",
      "\n",
      "Epoch 00027: val_loss improved from 220.05992 to 218.17636, saving model to weights.hdf5\n",
      "Epoch 28/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 210.9783 - val_loss: 219.1487\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 218.17636\n",
      "Epoch 29/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 210.5023 - val_loss: 219.8964\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 218.17636\n",
      "Epoch 30/1000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 209.7283 - val_loss: 219.4782\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 218.17636\n",
      "Epoch 31/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 210.7234 - val_loss: 219.2819\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 218.17636\n",
      "Epoch 32/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 209.4142 - val_loss: 217.2172\n",
      "\n",
      "Epoch 00032: val_loss improved from 218.17636 to 217.21720, saving model to weights.hdf5\n",
      "Epoch 33/1000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 208.6049 - val_loss: 216.2084\n",
      "\n",
      "Epoch 00033: val_loss improved from 217.21720 to 216.20837, saving model to weights.hdf5\n",
      "Epoch 34/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 207.5260 - val_loss: 216.8850\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 216.20837\n",
      "Epoch 35/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 204.4467 - val_loss: 211.9915\n",
      "\n",
      "Epoch 00035: val_loss improved from 216.20837 to 211.99147, saving model to weights.hdf5\n",
      "Epoch 36/1000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 198.4776 - val_loss: 208.7113\n",
      "\n",
      "Epoch 00036: val_loss improved from 211.99147 to 208.71129, saving model to weights.hdf5\n",
      "Epoch 37/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 192.9206 - val_loss: 203.1939\n",
      "\n",
      "Epoch 00037: val_loss improved from 208.71129 to 203.19386, saving model to weights.hdf5\n",
      "Epoch 38/1000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 190.6826 - val_loss: 202.5357\n",
      "\n",
      "Epoch 00038: val_loss improved from 203.19386 to 202.53571, saving model to weights.hdf5\n",
      "Epoch 39/1000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 187.2717 - val_loss: 198.0567\n",
      "\n",
      "Epoch 00039: val_loss improved from 202.53571 to 198.05668, saving model to weights.hdf5\n",
      "Epoch 40/1000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 186.7237 - val_loss: 197.0282\n",
      "\n",
      "Epoch 00040: val_loss improved from 198.05668 to 197.02821, saving model to weights.hdf5\n",
      "Epoch 41/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 184.1413 - val_loss: 194.1179\n",
      "\n",
      "Epoch 00041: val_loss improved from 197.02821 to 194.11786, saving model to weights.hdf5\n",
      "Epoch 42/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 184.5260 - val_loss: 195.4567\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 194.11786\n",
      "Epoch 43/1000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 182.2350 - val_loss: 192.8844\n",
      "\n",
      "Epoch 00043: val_loss improved from 194.11786 to 192.88437, saving model to weights.hdf5\n",
      "Epoch 44/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 182.6742 - val_loss: 191.1211\n",
      "\n",
      "Epoch 00044: val_loss improved from 192.88437 to 191.12110, saving model to weights.hdf5\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 449us/step - loss: 180.1587 - val_loss: 189.4681\n",
      "\n",
      "Epoch 00045: val_loss improved from 191.12110 to 189.46811, saving model to weights.hdf5\n",
      "Epoch 46/1000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 181.2884 - val_loss: 188.8204\n",
      "\n",
      "Epoch 00046: val_loss improved from 189.46811 to 188.82044, saving model to weights.hdf5\n",
      "Epoch 47/1000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 178.5674 - val_loss: 187.4672\n",
      "\n",
      "Epoch 00047: val_loss improved from 188.82044 to 187.46724, saving model to weights.hdf5\n",
      "Epoch 48/1000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 178.2035 - val_loss: 186.3403\n",
      "\n",
      "Epoch 00048: val_loss improved from 187.46724 to 186.34027, saving model to weights.hdf5\n",
      "Epoch 49/1000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 176.7290 - val_loss: 186.3687\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 186.34027\n",
      "Epoch 50/1000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 171.6425 - val_loss: 183.1537\n",
      "\n",
      "Epoch 00050: val_loss improved from 186.34027 to 183.15370, saving model to weights.hdf5\n",
      "Epoch 51/1000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 168.2843 - val_loss: 179.3747\n",
      "\n",
      "Epoch 00051: val_loss improved from 183.15370 to 179.37474, saving model to weights.hdf5\n",
      "Epoch 52/1000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 166.2647 - val_loss: 173.1220\n",
      "\n",
      "Epoch 00052: val_loss improved from 179.37474 to 173.12200, saving model to weights.hdf5\n",
      "Epoch 53/1000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 160.8066 - val_loss: 169.7736\n",
      "\n",
      "Epoch 00053: val_loss improved from 173.12200 to 169.77365, saving model to weights.hdf5\n",
      "Epoch 54/1000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 157.0033 - val_loss: 169.6171\n",
      "\n",
      "Epoch 00054: val_loss improved from 169.77365 to 169.61712, saving model to weights.hdf5\n",
      "Epoch 55/1000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 153.6596 - val_loss: 170.2680\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 169.61712\n",
      "Epoch 56/1000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 152.8007 - val_loss: 167.6483\n",
      "\n",
      "Epoch 00056: val_loss improved from 169.61712 to 167.64828, saving model to weights.hdf5\n",
      "Epoch 57/1000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 149.5570 - val_loss: 166.5995\n",
      "\n",
      "Epoch 00057: val_loss improved from 167.64828 to 166.59954, saving model to weights.hdf5\n",
      "Epoch 58/1000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 147.1192 - val_loss: 161.2914\n",
      "\n",
      "Epoch 00058: val_loss improved from 166.59954 to 161.29136, saving model to weights.hdf5\n",
      "Epoch 59/1000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 143.3430 - val_loss: 162.4146\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 161.29136\n",
      "Epoch 60/1000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 142.3576 - val_loss: 164.1129\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 161.29136\n",
      "Epoch 61/1000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 139.9849 - val_loss: 157.8398\n",
      "\n",
      "Epoch 00061: val_loss improved from 161.29136 to 157.83984, saving model to weights.hdf5\n",
      "Epoch 62/1000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 137.0716 - val_loss: 152.7218\n",
      "\n",
      "Epoch 00062: val_loss improved from 157.83984 to 152.72179, saving model to weights.hdf5\n",
      "Epoch 63/1000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 134.3090 - val_loss: 155.4477\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 152.72179\n",
      "Epoch 64/1000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 131.4593 - val_loss: 153.8319\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 152.72179\n",
      "Epoch 65/1000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 129.3474 - val_loss: 149.3141\n",
      "\n",
      "Epoch 00065: val_loss improved from 152.72179 to 149.31408, saving model to weights.hdf5\n",
      "Epoch 66/1000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 127.2600 - val_loss: 150.3816\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 149.31408\n",
      "Epoch 67/1000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 123.3195 - val_loss: 145.9830\n",
      "\n",
      "Epoch 00067: val_loss improved from 149.31408 to 145.98297, saving model to weights.hdf5\n",
      "Epoch 68/1000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 121.3427 - val_loss: 143.2219\n",
      "\n",
      "Epoch 00068: val_loss improved from 145.98297 to 143.22191, saving model to weights.hdf5\n",
      "Epoch 69/1000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 116.9209 - val_loss: 143.6623\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 143.22191\n",
      "Epoch 70/1000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 115.5940 - val_loss: 142.0864\n",
      "\n",
      "Epoch 00070: val_loss improved from 143.22191 to 142.08644, saving model to weights.hdf5\n",
      "Epoch 71/1000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 113.2193 - val_loss: 134.4834\n",
      "\n",
      "Epoch 00071: val_loss improved from 142.08644 to 134.48337, saving model to weights.hdf5\n",
      "Epoch 72/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 109.6807 - val_loss: 132.4922\n",
      "\n",
      "Epoch 00072: val_loss improved from 134.48337 to 132.49221, saving model to weights.hdf5\n",
      "Epoch 73/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 106.8907 - val_loss: 135.0621\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 132.49221\n",
      "Epoch 74/1000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 105.7807 - val_loss: 132.6600\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 132.49221\n",
      "Epoch 75/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 102.4945 - val_loss: 126.3530\n",
      "\n",
      "Epoch 00075: val_loss improved from 132.49221 to 126.35304, saving model to weights.hdf5\n",
      "Epoch 76/1000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 100.3731 - val_loss: 121.3146\n",
      "\n",
      "Epoch 00076: val_loss improved from 126.35304 to 121.31459, saving model to weights.hdf5\n",
      "Epoch 77/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 97.5328 - val_loss: 120.1319\n",
      "\n",
      "Epoch 00077: val_loss improved from 121.31459 to 120.13189, saving model to weights.hdf5\n",
      "Epoch 78/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 97.1330 - val_loss: 116.1812\n",
      "\n",
      "Epoch 00078: val_loss improved from 120.13189 to 116.18121, saving model to weights.hdf5\n",
      "Epoch 79/1000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 94.8519 - val_loss: 114.0604\n",
      "\n",
      "Epoch 00079: val_loss improved from 116.18121 to 114.06040, saving model to weights.hdf5\n",
      "Epoch 80/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 95.0275 - val_loss: 121.2015\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 114.06040\n",
      "Epoch 81/1000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 93.8657 - val_loss: 115.3676\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 114.06040\n",
      "Epoch 82/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 90.3479 - val_loss: 116.8975\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 114.06040\n",
      "Epoch 83/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 89.9365 - val_loss: 106.2691\n",
      "\n",
      "Epoch 00083: val_loss improved from 114.06040 to 106.26913, saving model to weights.hdf5\n",
      "Epoch 84/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 86.4130 - val_loss: 106.7076\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 106.26913\n",
      "Epoch 85/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 84.9360 - val_loss: 104.5411\n",
      "\n",
      "Epoch 00085: val_loss improved from 106.26913 to 104.54109, saving model to weights.hdf5\n",
      "Epoch 86/1000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 83.2593 - val_loss: 104.3364\n",
      "\n",
      "Epoch 00086: val_loss improved from 104.54109 to 104.33639, saving model to weights.hdf5\n",
      "Epoch 87/1000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 82.0157 - val_loss: 109.1585\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 104.33639\n",
      "Epoch 88/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 79.1746 - val_loss: 103.6218\n",
      "\n",
      "Epoch 00088: val_loss improved from 104.33639 to 103.62184, saving model to weights.hdf5\n",
      "Epoch 89/1000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 78.2570 - val_loss: 105.5226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00089: val_loss did not improve from 103.62184\n",
      "Epoch 90/1000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 76.7404 - val_loss: 102.9592\n",
      "\n",
      "Epoch 00090: val_loss improved from 103.62184 to 102.95919, saving model to weights.hdf5\n",
      "Epoch 91/1000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 75.5697 - val_loss: 103.4564\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 102.95919\n",
      "Epoch 92/1000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 74.0695 - val_loss: 101.3452\n",
      "\n",
      "Epoch 00092: val_loss improved from 102.95919 to 101.34520, saving model to weights.hdf5\n",
      "Epoch 93/1000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 72.7655 - val_loss: 101.2195\n",
      "\n",
      "Epoch 00093: val_loss improved from 101.34520 to 101.21945, saving model to weights.hdf5\n",
      "Epoch 94/1000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 74.0440 - val_loss: 101.2881\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 101.21945\n",
      "Epoch 95/1000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 72.0247 - val_loss: 99.5927\n",
      "\n",
      "Epoch 00095: val_loss improved from 101.21945 to 99.59270, saving model to weights.hdf5\n",
      "Epoch 96/1000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 70.5764 - val_loss: 101.4758\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 99.59270\n",
      "Epoch 97/1000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 68.5979 - val_loss: 95.6997\n",
      "\n",
      "Epoch 00097: val_loss improved from 99.59270 to 95.69974, saving model to weights.hdf5\n",
      "Epoch 98/1000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 68.3095 - val_loss: 96.8595\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 95.69974\n",
      "Epoch 99/1000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 66.5350 - val_loss: 96.6470\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 95.69974\n",
      "Epoch 100/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 66.2950 - val_loss: 98.5596\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 95.69974\n",
      "Epoch 101/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 65.9496 - val_loss: 98.5646\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 95.69974\n",
      "Epoch 102/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 65.0599 - val_loss: 97.1129\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 95.69974\n",
      "Epoch 103/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 65.2540 - val_loss: 101.7692\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 95.69974\n",
      "Epoch 104/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 66.6067 - val_loss: 99.4498\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 95.69974\n",
      "Epoch 105/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 67.0329 - val_loss: 103.0761\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 95.69974\n",
      "Epoch 106/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 64.9641 - val_loss: 103.5738\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 95.69974\n",
      "Epoch 107/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 68.6274 - val_loss: 95.9299\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 95.69974\n",
      "Epoch 108/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 68.6927 - val_loss: 102.8967\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 95.69974\n",
      "Epoch 109/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 67.6095 - val_loss: 98.5521\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 95.69974\n",
      "Epoch 110/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 64.7667 - val_loss: 100.2916\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 95.69974\n",
      "Epoch 111/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 65.6162 - val_loss: 105.2151\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 95.69974\n",
      "Epoch 112/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 66.5473 - val_loss: 97.5200\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 95.69974\n",
      "Epoch 113/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 66.3219 - val_loss: 101.3137\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 95.69974\n",
      "Epoch 114/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 61.4109 - val_loss: 97.2461\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 95.69974\n",
      "Epoch 115/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 63.9108 - val_loss: 104.5643\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 95.69974\n",
      "Epoch 116/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 65.9188 - val_loss: 98.1603\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 95.69974\n",
      "Epoch 117/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 61.6813 - val_loss: 101.2875\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 95.69974\n",
      "Epoch 118/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 62.5411 - val_loss: 97.7541\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 95.69974\n",
      "Epoch 119/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 63.1724 - val_loss: 104.5727\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 95.69974\n",
      "Epoch 120/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 59.9925 - val_loss: 94.5317\n",
      "\n",
      "Epoch 00120: val_loss improved from 95.69974 to 94.53172, saving model to weights.hdf5\n",
      "Epoch 121/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 58.9636 - val_loss: 104.8021\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 94.53172\n",
      "Epoch 122/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 58.9051 - val_loss: 94.3327\n",
      "\n",
      "Epoch 00122: val_loss improved from 94.53172 to 94.33270, saving model to weights.hdf5\n",
      "Epoch 123/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 57.1229 - val_loss: 100.6862\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 94.33270\n",
      "Epoch 124/1000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 57.7334 - val_loss: 92.8323\n",
      "\n",
      "Epoch 00124: val_loss improved from 94.33270 to 92.83227, saving model to weights.hdf5\n",
      "Epoch 125/1000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 58.7932 - val_loss: 98.5906\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 92.83227\n",
      "Epoch 126/1000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 59.5145 - val_loss: 107.4986\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 92.83227\n",
      "Epoch 127/1000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 62.0655 - val_loss: 96.9731\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 92.83227\n",
      "Epoch 128/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 60.1190 - val_loss: 98.1935\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 92.83227\n",
      "Epoch 129/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 59.7500 - val_loss: 95.6779\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 92.83227\n",
      "Epoch 130/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 59.2522 - val_loss: 110.9772\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 92.83227\n",
      "Epoch 131/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 61.5617 - val_loss: 99.6146\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 92.83227\n",
      "Epoch 132/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 55.7776 - val_loss: 93.5275\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 92.83227\n",
      "Epoch 133/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 57.5545 - val_loss: 93.8354\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 92.83227\n",
      "Epoch 134/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 53.8974 - val_loss: 94.0411\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 92.83227\n",
      "Epoch 135/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 53.0472 - val_loss: 91.0513\n",
      "\n",
      "Epoch 00135: val_loss improved from 92.83227 to 91.05134, saving model to weights.hdf5\n",
      "Epoch 136/1000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 54.2699 - val_loss: 94.2337\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 91.05134\n",
      "Epoch 137/1000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 52.5462 - val_loss: 95.0052\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 91.05134\n",
      "Epoch 138/1000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 51.6579 - val_loss: 92.5467\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 91.05134\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 532us/step - loss: 51.3007 - val_loss: 91.7568\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 91.05134\n",
      "Epoch 140/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 51.2402 - val_loss: 94.7694\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 91.05134\n",
      "Epoch 141/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 50.6290 - val_loss: 90.4698\n",
      "\n",
      "Epoch 00141: val_loss improved from 91.05134 to 90.46977, saving model to weights.hdf5\n",
      "Epoch 142/1000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 49.9418 - val_loss: 88.9884\n",
      "\n",
      "Epoch 00142: val_loss improved from 90.46977 to 88.98845, saving model to weights.hdf5\n",
      "Epoch 143/1000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 48.5216 - val_loss: 91.9493\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 88.98845\n",
      "Epoch 144/1000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 47.8479 - val_loss: 92.2962\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 88.98845\n",
      "Epoch 145/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 48.3556 - val_loss: 90.6249\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 88.98845\n",
      "Epoch 146/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 47.6725 - val_loss: 90.3323\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 88.98845\n",
      "Epoch 147/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 47.4675 - val_loss: 89.7456\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 88.98845\n",
      "Epoch 148/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 46.6259 - val_loss: 89.8996\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 88.98845\n",
      "Epoch 149/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 47.1618 - val_loss: 90.7837\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 88.98845\n",
      "Epoch 150/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 46.9901 - val_loss: 91.1041\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 88.98845\n",
      "Epoch 151/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 46.9439 - val_loss: 90.3821\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 88.98845\n",
      "Epoch 152/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 46.9012 - val_loss: 94.8297\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 88.98845\n",
      "Epoch 153/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 47.4699 - val_loss: 90.4411\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 88.98845\n",
      "Epoch 154/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 46.1737 - val_loss: 87.8187\n",
      "\n",
      "Epoch 00154: val_loss improved from 88.98845 to 87.81872, saving model to weights.hdf5\n",
      "Epoch 155/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 47.2936 - val_loss: 98.2503\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 87.81872\n",
      "Epoch 156/1000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 47.4270 - val_loss: 94.2594\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 87.81872\n",
      "Epoch 157/1000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 46.4231 - val_loss: 88.6736\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 87.81872\n",
      "Epoch 158/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 46.9901 - val_loss: 94.7555\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 87.81872\n",
      "Epoch 159/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 46.9145 - val_loss: 92.0891\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 87.81872\n",
      "Epoch 160/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 44.7341 - val_loss: 93.3544\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 87.81872\n",
      "Epoch 161/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 44.9635 - val_loss: 91.1662\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 87.81872\n",
      "Epoch 162/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 44.8337 - val_loss: 91.5494\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 87.81872\n",
      "Epoch 163/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 45.2411 - val_loss: 102.7640\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 87.81872\n",
      "Epoch 164/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 46.8181 - val_loss: 91.1629\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 87.81872\n",
      "Epoch 165/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 44.3621 - val_loss: 94.1779\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 87.81872\n",
      "Epoch 166/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 43.3035 - val_loss: 90.6344\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 87.81872\n",
      "Epoch 167/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 43.1514 - val_loss: 90.2601\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 87.81872\n",
      "Epoch 168/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 42.2139 - val_loss: 96.0174\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 87.81872\n",
      "Epoch 169/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 43.4953 - val_loss: 92.3138\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 87.81872\n",
      "Epoch 170/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 42.8606 - val_loss: 95.1026\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 87.81872\n",
      "Epoch 171/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 43.7945 - val_loss: 91.8321\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 87.81872\n",
      "Epoch 172/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 45.9005 - val_loss: 92.2640\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 87.81872\n",
      "Epoch 173/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 43.4899 - val_loss: 92.4396\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 87.81872\n",
      "Epoch 174/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 42.1270 - val_loss: 90.4424\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 87.81872\n",
      "Epoch 175/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 41.7205 - val_loss: 91.9316\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 87.81872\n",
      "Epoch 176/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 40.2695 - val_loss: 90.4432\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 87.81872\n",
      "Epoch 177/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 40.8374 - val_loss: 94.3607\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 87.81872\n",
      "Epoch 178/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 42.4093 - val_loss: 96.9768\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 87.81872\n",
      "Epoch 179/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 40.8660 - val_loss: 96.3385\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 87.81872\n",
      "Epoch 180/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 41.1985 - val_loss: 91.4375\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 87.81872\n",
      "Epoch 181/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 41.0253 - val_loss: 90.3711\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 87.81872\n",
      "Epoch 182/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 39.3390 - val_loss: 92.4475\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 87.81872\n",
      "Epoch 183/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 39.8728 - val_loss: 88.2237\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 87.81872\n",
      "Epoch 184/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 41.0438 - val_loss: 97.7848\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 87.81872\n",
      "Epoch 185/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 42.6339 - val_loss: 97.0861\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 87.81872\n",
      "Epoch 186/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 43.6550 - val_loss: 100.9752\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 87.81872\n",
      "Epoch 187/1000\n",
      "96/96 [==============================] - 0s 587us/step - loss: 40.3517 - val_loss: 96.5745\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 87.81872\n",
      "Epoch 188/1000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 39.2632 - val_loss: 94.9679\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 87.81872\n",
      "Epoch 189/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 37.9008 - val_loss: 92.6333\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 87.81872\n",
      "Epoch 190/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 552us/step - loss: 38.3020 - val_loss: 97.3359\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 87.81872\n",
      "Epoch 191/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 39.1030 - val_loss: 96.2405\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 87.81872\n",
      "Epoch 192/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 37.0734 - val_loss: 93.3019\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 87.81872\n",
      "Epoch 193/1000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 36.4817 - val_loss: 91.8143\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 87.81872\n",
      "Epoch 194/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 36.1467 - val_loss: 91.0728\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 87.81872\n",
      "Epoch 195/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 36.7420 - val_loss: 95.5575\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 87.81872\n",
      "Epoch 196/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 36.5723 - val_loss: 93.5634\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 87.81872\n",
      "Epoch 197/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 35.4199 - val_loss: 92.6653\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 87.81872\n",
      "Epoch 198/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 35.3548 - val_loss: 91.9237\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 87.81872\n",
      "Epoch 199/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 34.7987 - val_loss: 92.6832\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 87.81872\n",
      "Epoch 200/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 33.6180 - val_loss: 93.4025\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 87.81872\n",
      "Epoch 201/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 33.7544 - val_loss: 91.0646\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 87.81872\n",
      "Epoch 202/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 32.5756 - val_loss: 92.4169\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 87.81872\n",
      "Epoch 203/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 33.8808 - val_loss: 91.9737\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 87.81872\n",
      "Epoch 204/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 35.0280 - val_loss: 98.5723\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 87.81872\n",
      "Epoch 205/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 33.4449 - val_loss: 95.4632\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 87.81872\n",
      "Epoch 206/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 34.6013 - val_loss: 98.8533\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 87.81872\n",
      "Epoch 207/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 35.7465 - val_loss: 98.3227\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 87.81872\n",
      "Epoch 208/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 35.7693 - val_loss: 96.7948\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 87.81872\n",
      "Epoch 209/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 35.3983 - val_loss: 97.7384\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 87.81872\n",
      "Epoch 210/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 35.4403 - val_loss: 99.3476\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 87.81872\n",
      "Epoch 211/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 33.6427 - val_loss: 98.2751\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 87.81872\n",
      "Epoch 212/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 33.8411 - val_loss: 109.3773\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 87.81872\n",
      "Epoch 213/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 44.2779 - val_loss: 120.4183\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 87.81872\n",
      "Epoch 214/1000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 54.2771 - val_loss: 133.6509\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 87.81872\n",
      "Epoch 215/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 61.0963 - val_loss: 112.0704\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 87.81872\n",
      "Epoch 216/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 58.7759 - val_loss: 125.7981\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 87.81872\n",
      "Epoch 217/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 52.6487 - val_loss: 114.0423\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 87.81872\n",
      "Epoch 218/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 46.3754 - val_loss: 101.8888\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 87.81872\n",
      "Epoch 219/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 39.9721 - val_loss: 96.9163\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 87.81872\n",
      "Epoch 220/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 38.0073 - val_loss: 99.0837\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 87.81872\n",
      "Epoch 221/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 38.1273 - val_loss: 96.0551\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 87.81872\n",
      "Epoch 222/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 35.7026 - val_loss: 94.0755\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 87.81872\n",
      "Epoch 223/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 35.1419 - val_loss: 96.4875\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 87.81872\n",
      "Epoch 224/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 33.2871 - val_loss: 94.9193\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 87.81872\n",
      "Epoch 225/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 31.2315 - val_loss: 94.5921\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 87.81872\n",
      "Epoch 226/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 31.3292 - val_loss: 93.4520\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 87.81872\n",
      "Epoch 227/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 30.6482 - val_loss: 94.4266\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 87.81872\n",
      "Epoch 228/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 30.6777 - val_loss: 98.0342\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 87.81872\n",
      "Epoch 229/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 30.7384 - val_loss: 95.8053\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 87.81872\n",
      "Epoch 230/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 29.9496 - val_loss: 96.5514\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 87.81872\n",
      "Epoch 231/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 31.8478 - val_loss: 102.6409\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 87.81872\n",
      "Epoch 232/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 30.9848 - val_loss: 97.3445\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 87.81872\n",
      "Epoch 233/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 30.7252 - val_loss: 102.7929\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 87.81872\n",
      "Epoch 234/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 31.2676 - val_loss: 95.4579\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 87.81872\n",
      "Epoch 235/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 30.7727 - val_loss: 95.8681\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 87.81872\n",
      "Epoch 236/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 30.5568 - val_loss: 112.9306\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 87.81872\n",
      "Epoch 237/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 30.4303 - val_loss: 101.1379\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 87.81872\n",
      "Epoch 238/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 28.8517 - val_loss: 103.4017\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 87.81872\n",
      "Epoch 239/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 28.6803 - val_loss: 100.0834\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 87.81872\n",
      "Epoch 240/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 28.2419 - val_loss: 100.4153\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 87.81872\n",
      "Epoch 241/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 27.9129 - val_loss: 98.2374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00241: val_loss did not improve from 87.81872\n",
      "Epoch 242/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 27.6514 - val_loss: 102.8707\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 87.81872\n",
      "Epoch 243/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 27.8383 - val_loss: 100.4422\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 87.81872\n",
      "Epoch 244/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 27.7101 - val_loss: 105.2209\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 87.81872\n",
      "Epoch 245/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 27.7592 - val_loss: 101.8936\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 87.81872\n",
      "Epoch 246/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 27.3796 - val_loss: 105.2238\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 87.81872\n",
      "Epoch 247/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 26.4979 - val_loss: 99.6093\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 87.81872\n",
      "Epoch 248/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 26.4423 - val_loss: 104.4964\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 87.81872\n",
      "Epoch 249/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 25.8392 - val_loss: 99.5485\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 87.81872\n",
      "Epoch 250/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 26.6741 - val_loss: 100.1629\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 87.81872\n",
      "Epoch 251/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 25.7967 - val_loss: 104.2682\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 87.81872\n",
      "Epoch 252/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 27.8212 - val_loss: 111.6846\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 87.81872\n",
      "Epoch 253/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 27.9213 - val_loss: 102.2659\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 87.81872\n",
      "Epoch 254/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 27.0102 - val_loss: 105.2816\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 87.81872\n",
      "Epoch 255/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 26.7844 - val_loss: 112.4771\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 87.81872\n",
      "Epoch 256/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 32.6845 - val_loss: 125.1207\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 87.81872\n",
      "Epoch 257/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 38.2151 - val_loss: 106.9741\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 87.81872\n",
      "Epoch 258/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 36.9115 - val_loss: 125.5752\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 87.81872\n",
      "Epoch 259/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 33.4109 - val_loss: 104.2217\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 87.81872\n",
      "Epoch 260/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 29.9814 - val_loss: 103.7238\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 87.81872\n",
      "Epoch 261/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 30.4949 - val_loss: 113.7446\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 87.81872\n",
      "Epoch 262/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 33.2044 - val_loss: 114.5230\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 87.81872\n",
      "Epoch 263/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 32.8784 - val_loss: 110.4464\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 87.81872\n",
      "Epoch 264/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 34.0369 - val_loss: 105.2183\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 87.81872\n",
      "Epoch 265/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 31.4682 - val_loss: 117.8860\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 87.81872\n",
      "Epoch 266/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 30.7085 - val_loss: 107.7366\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 87.81872\n",
      "Epoch 267/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 29.8826 - val_loss: 104.3482\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 87.81872\n",
      "Epoch 268/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 28.2587 - val_loss: 115.4985\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 87.81872\n",
      "Epoch 269/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 28.5425 - val_loss: 113.1751\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 87.81872\n",
      "Epoch 270/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 28.9118 - val_loss: 105.2168\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 87.81872\n",
      "Epoch 271/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 27.5019 - val_loss: 104.5875\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 87.81872\n",
      "Epoch 272/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 26.5723 - val_loss: 109.1074\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 87.81872\n",
      "Epoch 273/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 25.9685 - val_loss: 109.5895\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 87.81872\n",
      "Epoch 274/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 24.9496 - val_loss: 100.8315\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 87.81872\n",
      "Epoch 275/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 24.0686 - val_loss: 108.5221\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 87.81872\n",
      "Epoch 276/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 22.9389 - val_loss: 103.3017\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 87.81872\n",
      "Epoch 277/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 22.3045 - val_loss: 105.7385\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 87.81872\n",
      "Epoch 278/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 22.8040 - val_loss: 101.2596\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 87.81872\n",
      "Epoch 279/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 22.1810 - val_loss: 104.0231\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 87.81872\n",
      "Epoch 280/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 21.2736 - val_loss: 103.5850\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 87.81872\n",
      "Epoch 281/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 21.3329 - val_loss: 102.4707\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 87.81872\n",
      "Epoch 282/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 22.6948 - val_loss: 109.6867\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 87.81872\n",
      "Epoch 283/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 22.3963 - val_loss: 107.8293\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 87.81872\n",
      "Epoch 284/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 23.6629 - val_loss: 106.1045\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 87.81872\n",
      "Epoch 285/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 22.9567 - val_loss: 108.2471\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 87.81872\n",
      "Epoch 286/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 23.9805 - val_loss: 114.8362\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 87.81872\n",
      "Epoch 287/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 22.8234 - val_loss: 111.9215\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 87.81872\n",
      "Epoch 288/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 24.3194 - val_loss: 106.3382\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 87.81872\n",
      "Epoch 289/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 22.6994 - val_loss: 110.5477\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 87.81872\n",
      "Epoch 290/1000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 23.5333 - val_loss: 113.0203\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 87.81872\n",
      "Epoch 291/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 22.3548 - val_loss: 111.9798\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 87.81872\n",
      "Epoch 292/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 21.4706 - val_loss: 111.8447\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 87.81872\n",
      "Epoch 293/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 554us/step - loss: 22.8085 - val_loss: 116.7095\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 87.81872\n",
      "Epoch 294/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 23.8250 - val_loss: 123.2394\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 87.81872\n",
      "Epoch 295/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 27.1115 - val_loss: 111.5031\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 87.81872\n",
      "Epoch 296/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 25.9582 - val_loss: 115.7689\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 87.81872\n",
      "Epoch 297/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 25.3958 - val_loss: 114.2510\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 87.81872\n",
      "Epoch 298/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 26.1286 - val_loss: 126.1458\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 87.81872\n",
      "Epoch 299/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 35.9623 - val_loss: 144.2175\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 87.81872\n",
      "Epoch 300/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 35.9006 - val_loss: 127.2026\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 87.81872\n",
      "Epoch 301/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 36.5133 - val_loss: 118.1398\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 87.81872\n",
      "Epoch 302/1000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 31.2486 - val_loss: 112.3709\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 87.81872\n",
      "Epoch 303/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 29.6956 - val_loss: 125.2841\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 87.81872\n",
      "Epoch 304/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 30.7187 - val_loss: 122.3803\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 87.81872\n",
      "Epoch 305/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 28.1828 - val_loss: 123.5335\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 87.81872\n",
      "Epoch 306/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 26.9674 - val_loss: 120.5696\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 87.81872\n",
      "Epoch 307/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 27.1085 - val_loss: 118.3530\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 87.81872\n",
      "Epoch 308/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 26.3431 - val_loss: 115.7862\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 87.81872\n",
      "Epoch 309/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 29.2570 - val_loss: 121.7854\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 87.81872\n",
      "Epoch 310/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 30.4492 - val_loss: 115.5072\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 87.81872\n",
      "Epoch 311/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 31.1931 - val_loss: 116.5238\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 87.81872\n",
      "Epoch 312/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 30.6564 - val_loss: 134.8341\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 87.81872\n",
      "Epoch 313/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 30.7411 - val_loss: 127.0240\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 87.81872\n",
      "Epoch 314/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 31.9894 - val_loss: 121.1768\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 87.81872\n",
      "Epoch 315/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 26.4859 - val_loss: 123.7441\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 87.81872\n",
      "Epoch 316/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 26.6860 - val_loss: 117.3396\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 87.81872\n",
      "Epoch 317/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 27.0304 - val_loss: 110.4784\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 87.81872\n",
      "Epoch 318/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 23.5391 - val_loss: 110.5935\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 87.81872\n",
      "Epoch 319/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 22.5520 - val_loss: 109.9257\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 87.81872\n",
      "Epoch 320/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 20.8409 - val_loss: 110.2650\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 87.81872\n",
      "Epoch 321/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 20.1273 - val_loss: 108.0278\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 87.81872\n",
      "Epoch 322/1000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 19.4933 - val_loss: 110.8596\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 87.81872\n",
      "Epoch 323/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 19.0685 - val_loss: 113.9022\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 87.81872\n",
      "Epoch 324/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 18.6532 - val_loss: 108.1269\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 87.81872\n",
      "Epoch 325/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 18.2460 - val_loss: 110.2238\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 87.81872\n",
      "Epoch 326/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 18.1735 - val_loss: 114.2859\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 87.81872\n",
      "Epoch 327/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 18.3101 - val_loss: 106.5538\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 87.81872\n",
      "Epoch 328/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 18.2058 - val_loss: 111.8346\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 87.81872\n",
      "Epoch 329/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 18.1136 - val_loss: 112.5420\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 87.81872\n",
      "Epoch 330/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 17.5901 - val_loss: 113.5798\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 87.81872\n",
      "Epoch 331/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 18.0331 - val_loss: 111.7211\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 87.81872\n",
      "Epoch 332/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 17.8487 - val_loss: 116.9231\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 87.81872\n",
      "Epoch 333/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 17.2667 - val_loss: 114.9418\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 87.81872\n",
      "Epoch 334/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 18.7100 - val_loss: 113.5710\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 87.81872\n",
      "Epoch 335/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 19.1416 - val_loss: 127.4983\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 87.81872\n",
      "Epoch 336/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 18.6098 - val_loss: 117.3291\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 87.81872\n",
      "Epoch 337/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 18.6603 - val_loss: 115.8900\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 87.81872\n",
      "Epoch 338/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 17.9206 - val_loss: 125.0459\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 87.81872\n",
      "Epoch 339/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 19.1407 - val_loss: 116.9370\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 87.81872\n",
      "Epoch 340/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 19.1870 - val_loss: 119.8484\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 87.81872\n",
      "Epoch 341/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 18.6595 - val_loss: 127.6967\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 87.81872\n",
      "Epoch 342/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 18.8654 - val_loss: 114.4624\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 87.81872\n",
      "Epoch 343/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 18.2255 - val_loss: 123.2122\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 87.81872\n",
      "Epoch 344/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 549us/step - loss: 19.4036 - val_loss: 122.9498\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 87.81872\n",
      "Epoch 345/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 18.8338 - val_loss: 124.2744\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 87.81872\n",
      "Epoch 346/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 18.1453 - val_loss: 114.5870\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 87.81872\n",
      "Epoch 347/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 17.4683 - val_loss: 118.6635\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 87.81872\n",
      "Epoch 348/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 18.1559 - val_loss: 114.0998\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 87.81872\n",
      "Epoch 349/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 17.4992 - val_loss: 113.0152\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 87.81872\n",
      "Epoch 350/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 17.6390 - val_loss: 120.4454\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 87.81872\n",
      "Epoch 351/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 18.7669 - val_loss: 123.6468\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 87.81872\n",
      "Epoch 352/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 19.1442 - val_loss: 126.9124\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 87.81872\n",
      "Epoch 353/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 18.0381 - val_loss: 116.3005\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 87.81872\n",
      "Epoch 354/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 17.8188 - val_loss: 121.4400\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 87.81872\n",
      "Epoch 355/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 18.0464 - val_loss: 118.9103\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 87.81872\n",
      "Epoch 356/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 19.2055 - val_loss: 131.2482\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 87.81872\n",
      "Epoch 357/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 18.7006 - val_loss: 125.3407\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 87.81872\n",
      "Epoch 358/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 18.2966 - val_loss: 127.4193\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 87.81872\n",
      "Epoch 359/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 18.8973 - val_loss: 128.1564\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 87.81872\n",
      "Epoch 360/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 17.6967 - val_loss: 128.1879\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 87.81872\n",
      "Epoch 361/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 19.5851 - val_loss: 122.2952\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 87.81872\n",
      "Epoch 362/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 18.0367 - val_loss: 122.7798\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 87.81872\n",
      "Epoch 363/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 17.3911 - val_loss: 116.8902\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 87.81872\n",
      "Epoch 364/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 17.2003 - val_loss: 131.3586\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 87.81872\n",
      "Epoch 365/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 16.6339 - val_loss: 124.9271\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 87.81872\n",
      "Epoch 366/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 17.7169 - val_loss: 123.5722\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 87.81872\n",
      "Epoch 367/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 15.9372 - val_loss: 115.1107\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 87.81872\n",
      "Epoch 368/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 16.2915 - val_loss: 117.8946\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 87.81872\n",
      "Epoch 369/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 15.6193 - val_loss: 124.8722\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 87.81872\n",
      "Epoch 370/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 15.3829 - val_loss: 125.7142\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 87.81872\n",
      "Epoch 371/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 15.9255 - val_loss: 119.8788\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 87.81872\n",
      "Epoch 372/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 15.6149 - val_loss: 134.1358\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 87.81872\n",
      "Epoch 373/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 15.7904 - val_loss: 119.3635\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 87.81872\n",
      "Epoch 374/1000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 16.2416 - val_loss: 135.4353\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 87.81872\n",
      "Epoch 375/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 16.2383 - val_loss: 123.2469\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 87.81872\n",
      "Epoch 376/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 16.3500 - val_loss: 130.5544\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 87.81872\n",
      "Epoch 377/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 17.0237 - val_loss: 123.0534\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 87.81872\n",
      "Epoch 378/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 16.8596 - val_loss: 127.6192\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 87.81872\n",
      "Epoch 379/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 16.4429 - val_loss: 122.5513\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 87.81872\n",
      "Epoch 380/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 17.3922 - val_loss: 128.0481\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 87.81872\n",
      "Epoch 381/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 17.5664 - val_loss: 127.8682\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 87.81872\n",
      "Epoch 382/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 16.5930 - val_loss: 123.3097\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 87.81872\n",
      "Epoch 383/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 17.3990 - val_loss: 129.9951\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 87.81872\n",
      "Epoch 384/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 16.5748 - val_loss: 124.4623\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 87.81872\n",
      "Epoch 385/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 18.5091 - val_loss: 135.3749\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 87.81872\n",
      "Epoch 386/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 18.2971 - val_loss: 131.9512\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 87.81872\n",
      "Epoch 387/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 17.8999 - val_loss: 145.1034\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 87.81872\n",
      "Epoch 388/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 17.6883 - val_loss: 133.6010\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 87.81872\n",
      "Epoch 389/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 17.7588 - val_loss: 126.0198\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 87.81872\n",
      "Epoch 390/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 18.0840 - val_loss: 135.5525\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 87.81872\n",
      "Epoch 391/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 18.5670 - val_loss: 129.9140\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 87.81872\n",
      "Epoch 392/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 17.2436 - val_loss: 129.2558\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 87.81872\n",
      "Epoch 393/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 17.5422 - val_loss: 135.5063\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 87.81872\n",
      "Epoch 394/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 16.5086 - val_loss: 131.6317\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 87.81872\n",
      "Epoch 395/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 539us/step - loss: 16.7447 - val_loss: 127.4945\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 87.81872\n",
      "Epoch 396/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 17.9056 - val_loss: 131.3566\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 87.81872\n",
      "Epoch 397/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 17.2245 - val_loss: 143.1051\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 87.81872\n",
      "Epoch 398/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 17.5590 - val_loss: 144.5640\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 87.81872\n",
      "Epoch 399/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 17.4338 - val_loss: 140.7586\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 87.81872\n",
      "Epoch 400/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 19.6349 - val_loss: 142.1407\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 87.81872\n",
      "Epoch 401/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 20.1564 - val_loss: 155.3765\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 87.81872\n",
      "Epoch 402/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 24.3922 - val_loss: 147.9282\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 87.81872\n",
      "Epoch 403/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 19.6074 - val_loss: 162.3916\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 87.81872\n",
      "Epoch 404/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 23.8659 - val_loss: 138.6963\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 87.81872\n",
      "Epoch 405/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 22.3028 - val_loss: 147.2962\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 87.81872\n",
      "Epoch 406/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 23.4371 - val_loss: 147.9603\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 87.81872\n",
      "Epoch 407/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 32.2545 - val_loss: 171.4341\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 87.81872\n",
      "Epoch 408/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 25.1986 - val_loss: 161.8872\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 87.81872\n",
      "Epoch 409/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 25.9191 - val_loss: 143.0879\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 87.81872\n",
      "Epoch 410/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 24.2662 - val_loss: 138.6394\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 87.81872\n",
      "Epoch 411/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 24.0218 - val_loss: 129.0926\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 87.81872\n",
      "Epoch 412/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 22.9186 - val_loss: 144.7215\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 87.81872\n",
      "Epoch 413/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 23.0073 - val_loss: 134.3492\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 87.81872\n",
      "Epoch 414/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 21.5779 - val_loss: 149.2428\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 87.81872\n",
      "Epoch 415/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 20.0933 - val_loss: 134.4902\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 87.81872\n",
      "Epoch 416/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 19.0338 - val_loss: 134.4556\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 87.81872\n",
      "Epoch 417/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 19.4072 - val_loss: 130.9137\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 87.81872\n",
      "Epoch 418/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 17.3971 - val_loss: 141.0924\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 87.81872\n",
      "Epoch 419/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 17.8274 - val_loss: 130.3945\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 87.81872\n",
      "Epoch 420/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 18.7041 - val_loss: 129.8770\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 87.81872\n",
      "Epoch 421/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 20.3336 - val_loss: 142.2184\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 87.81872\n",
      "Epoch 422/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 18.7613 - val_loss: 138.6901\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 87.81872\n",
      "Epoch 423/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 16.1065 - val_loss: 136.3290\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 87.81872\n",
      "Epoch 424/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 16.2060 - val_loss: 133.0733\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 87.81872\n",
      "Epoch 425/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 16.0799 - val_loss: 120.4308\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 87.81872\n",
      "Epoch 426/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 15.9439 - val_loss: 130.0707\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 87.81872\n",
      "Epoch 427/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 15.6328 - val_loss: 125.3605\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 87.81872\n",
      "Epoch 428/1000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 17.1053 - val_loss: 136.7072\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 87.81872\n",
      "Epoch 429/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 18.3471 - val_loss: 131.6282\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 87.81872\n",
      "Epoch 430/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 17.6743 - val_loss: 144.2042\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 87.81872\n",
      "Epoch 431/1000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 17.3294 - val_loss: 147.6064\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 87.81872\n",
      "Epoch 432/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 21.3862 - val_loss: 160.0765\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 87.81872\n",
      "Epoch 433/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 20.7741 - val_loss: 137.7965\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 87.81872\n",
      "Epoch 434/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 20.2465 - val_loss: 153.1182\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 87.81872\n",
      "Epoch 435/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 19.2798 - val_loss: 139.9006\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 87.81872\n",
      "Epoch 436/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 17.6199 - val_loss: 145.5290\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 87.81872\n",
      "Epoch 437/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 16.0047 - val_loss: 122.7653\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 87.81872\n",
      "Epoch 438/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 15.6256 - val_loss: 128.8585\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 87.81872\n",
      "Epoch 439/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 15.8030 - val_loss: 137.5418\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 87.81872\n",
      "Epoch 440/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 17.5795 - val_loss: 127.9039\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 87.81872\n",
      "Epoch 441/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 17.8826 - val_loss: 127.7050\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 87.81872\n",
      "Epoch 442/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 15.9245 - val_loss: 132.5860\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 87.81872\n",
      "Epoch 443/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 15.3761 - val_loss: 133.1748\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 87.81872\n",
      "Epoch 444/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 14.8810 - val_loss: 123.0020\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 87.81872\n",
      "Epoch 445/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 15.4479 - val_loss: 126.4363\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 87.81872\n",
      "Epoch 446/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 529us/step - loss: 15.5852 - val_loss: 135.9928\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 87.81872\n",
      "Epoch 447/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 14.9456 - val_loss: 122.5180\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 87.81872\n",
      "Epoch 448/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 15.4038 - val_loss: 129.8685\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 87.81872\n",
      "Epoch 449/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.1290 - val_loss: 127.6704\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 87.81872\n",
      "Epoch 450/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 15.7135 - val_loss: 129.7372\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 87.81872\n",
      "Epoch 451/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 14.9866 - val_loss: 136.1246\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 87.81872\n",
      "Epoch 452/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 14.1529 - val_loss: 119.4462\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 87.81872\n",
      "Epoch 453/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 13.8963 - val_loss: 141.4476\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 87.81872\n",
      "Epoch 454/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.7262 - val_loss: 126.2319\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 87.81872\n",
      "Epoch 455/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 15.1942 - val_loss: 135.5847\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 87.81872\n",
      "Epoch 456/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 14.3458 - val_loss: 134.3217\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 87.81872\n",
      "Epoch 457/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.1474 - val_loss: 122.7430\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 87.81872\n",
      "Epoch 458/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 13.7786 - val_loss: 129.9957\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 87.81872\n",
      "Epoch 459/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 13.7111 - val_loss: 132.5140\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 87.81872\n",
      "Epoch 460/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 14.1344 - val_loss: 152.4111\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 87.81872\n",
      "Epoch 461/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 13.5403 - val_loss: 141.6689\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 87.81872\n",
      "Epoch 462/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.3292 - val_loss: 130.6440\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 87.81872\n",
      "Epoch 463/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 13.0550 - val_loss: 128.8637\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 87.81872\n",
      "Epoch 464/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 12.9886 - val_loss: 126.1281\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 87.81872\n",
      "Epoch 465/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.3196 - val_loss: 139.0023\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 87.81872\n",
      "Epoch 466/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 13.6195 - val_loss: 136.3085\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 87.81872\n",
      "Epoch 467/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 14.0571 - val_loss: 132.0105\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 87.81872\n",
      "Epoch 468/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 14.3756 - val_loss: 140.4069\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 87.81872\n",
      "Epoch 469/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 13.4514 - val_loss: 134.7949\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 87.81872\n",
      "Epoch 470/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 13.3128 - val_loss: 138.6003\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 87.81872\n",
      "Epoch 471/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.1238 - val_loss: 139.4052\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 87.81872\n",
      "Epoch 472/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 14.3188 - val_loss: 128.3367\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 87.81872\n",
      "Epoch 473/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 14.4813 - val_loss: 127.6577\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 87.81872\n",
      "Epoch 474/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 13.6243 - val_loss: 138.3696\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 87.81872\n",
      "Epoch 475/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.5494 - val_loss: 128.9873\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 87.81872\n",
      "Epoch 476/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 13.9788 - val_loss: 153.2248\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 87.81872\n",
      "Epoch 477/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.9774 - val_loss: 157.3827\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 87.81872\n",
      "Epoch 478/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 12.7615 - val_loss: 128.2572\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 87.81872\n",
      "Epoch 479/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 13.2794 - val_loss: 134.1686\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 87.81872\n",
      "Epoch 480/1000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 14.4819 - val_loss: 140.2299\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 87.81872\n",
      "Epoch 481/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 13.9511 - val_loss: 138.1308\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 87.81872\n",
      "Epoch 482/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.4189 - val_loss: 127.0366\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 87.81872\n",
      "Epoch 483/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 13.8928 - val_loss: 146.9914\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 87.81872\n",
      "Epoch 484/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.9706 - val_loss: 139.5334\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 87.81872\n",
      "Epoch 485/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 13.5202 - val_loss: 133.2875\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 87.81872\n",
      "Epoch 486/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 13.5714 - val_loss: 141.3757\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 87.81872\n",
      "Epoch 487/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.1446 - val_loss: 133.8302\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 87.81872\n",
      "Epoch 488/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 13.7898 - val_loss: 128.5229\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 87.81872\n",
      "Epoch 489/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 14.2965 - val_loss: 134.2818\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 87.81872\n",
      "Epoch 490/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 14.2709 - val_loss: 134.4412\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 87.81872\n",
      "Epoch 491/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.6711 - val_loss: 143.5489\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 87.81872\n",
      "Epoch 492/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 16.1727 - val_loss: 144.7003\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 87.81872\n",
      "Epoch 493/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 14.2946 - val_loss: 139.6226\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 87.81872\n",
      "Epoch 494/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 14.7788 - val_loss: 140.0709\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 87.81872\n",
      "Epoch 495/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 15.9218 - val_loss: 131.7251\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 87.81872\n",
      "Epoch 496/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 15.7296 - val_loss: 155.2024\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 87.81872\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 537us/step - loss: 15.8683 - val_loss: 139.9252\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 87.81872\n",
      "Epoch 498/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 16.1829 - val_loss: 136.8491\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 87.81872\n",
      "Epoch 499/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 14.1028 - val_loss: 145.1787\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 87.81872\n",
      "Epoch 500/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 14.4717 - val_loss: 152.2119\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 87.81872\n",
      "Epoch 501/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 14.0088 - val_loss: 145.3086\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 87.81872\n",
      "Epoch 502/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.3052 - val_loss: 156.4610\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 87.81872\n",
      "Epoch 503/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 14.0338 - val_loss: 135.5525\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 87.81872\n",
      "Epoch 504/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 14.2527 - val_loss: 147.4433\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 87.81872\n",
      "Epoch 505/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 14.1541 - val_loss: 143.8713\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 87.81872\n",
      "Epoch 506/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 13.0843 - val_loss: 133.6374\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 87.81872\n",
      "Epoch 507/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.9734 - val_loss: 145.1937\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 87.81872\n",
      "Epoch 508/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 12.2074 - val_loss: 150.6836\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 87.81872\n",
      "Epoch 509/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.5952 - val_loss: 146.3382\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 87.81872\n",
      "Epoch 510/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.7627 - val_loss: 149.7242\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 87.81872\n",
      "Epoch 511/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 14.7748 - val_loss: 129.0955\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 87.81872\n",
      "Epoch 512/1000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 15.6065 - val_loss: 144.4871\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 87.81872\n",
      "Epoch 513/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 16.0136 - val_loss: 141.9836\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 87.81872\n",
      "Epoch 514/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 19.1633 - val_loss: 167.8528\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 87.81872\n",
      "Epoch 515/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 18.3663 - val_loss: 166.2733\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 87.81872\n",
      "Epoch 516/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 18.7612 - val_loss: 158.0230\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 87.81872\n",
      "Epoch 517/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 17.2401 - val_loss: 152.9366\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 87.81872\n",
      "Epoch 518/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 16.6535 - val_loss: 143.0448\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 87.81872\n",
      "Epoch 519/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 14.7701 - val_loss: 142.2062\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 87.81872\n",
      "Epoch 520/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 14.7936 - val_loss: 137.1958\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 87.81872\n",
      "Epoch 521/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 18.1926 - val_loss: 143.1152\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 87.81872\n",
      "Epoch 522/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 21.9173 - val_loss: 155.9866\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 87.81872\n",
      "Epoch 523/1000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 39.5648 - val_loss: 194.1665\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 87.81872\n",
      "Epoch 524/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 46.1214 - val_loss: 172.5704\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 87.81872\n",
      "Epoch 525/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 39.1587 - val_loss: 198.0849\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 87.81872\n",
      "Epoch 526/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 43.0620 - val_loss: 170.3761\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 87.81872\n",
      "Epoch 527/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 40.1649 - val_loss: 155.0998\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 87.81872\n",
      "Epoch 528/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 30.9070 - val_loss: 157.9130\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 87.81872\n",
      "Epoch 529/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 38.2853 - val_loss: 157.3178\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 87.81872\n",
      "Epoch 530/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 38.9848 - val_loss: 159.8972\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 87.81872\n",
      "Epoch 531/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 39.6576 - val_loss: 171.3585\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 87.81872\n",
      "Epoch 532/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 30.0855 - val_loss: 132.7866\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 87.81872\n",
      "Epoch 533/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 24.3093 - val_loss: 139.5704\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 87.81872\n",
      "Epoch 534/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 26.6614 - val_loss: 139.3147\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 87.81872\n",
      "Epoch 535/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 23.6286 - val_loss: 128.4412\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 87.81872\n",
      "Epoch 536/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 19.4395 - val_loss: 130.8997\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 87.81872\n",
      "Epoch 537/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 20.7497 - val_loss: 123.7846\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 87.81872\n",
      "Epoch 538/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 19.0273 - val_loss: 139.0626\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 87.81872\n",
      "Epoch 539/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 17.5226 - val_loss: 135.9437\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 87.81872\n",
      "Epoch 540/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 16.9612 - val_loss: 138.9737\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 87.81872\n",
      "Epoch 541/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 16.2363 - val_loss: 132.0591\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 87.81872\n",
      "Epoch 542/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 16.7427 - val_loss: 147.2585\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 87.81872\n",
      "Epoch 543/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 15.3003 - val_loss: 138.7378\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 87.81872\n",
      "Epoch 544/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 18.8670 - val_loss: 150.5081\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 87.81872\n",
      "Epoch 545/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 16.5158 - val_loss: 133.5287\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 87.81872\n",
      "Epoch 546/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 19.5479 - val_loss: 141.4443\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 87.81872\n",
      "Epoch 547/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 17.9541 - val_loss: 140.8076\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 87.81872\n",
      "Epoch 548/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 532us/step - loss: 17.9421 - val_loss: 134.8406\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 87.81872\n",
      "Epoch 549/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 16.3842 - val_loss: 122.8278\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 87.81872\n",
      "Epoch 550/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 16.4278 - val_loss: 131.9617\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 87.81872\n",
      "Epoch 551/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 15.0772 - val_loss: 131.4810\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 87.81872\n",
      "Epoch 552/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 14.6209 - val_loss: 121.4891\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 87.81872\n",
      "Epoch 553/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 13.2458 - val_loss: 130.4143\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 87.81872\n",
      "Epoch 554/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.6176 - val_loss: 152.3539\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 87.81872\n",
      "Epoch 555/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 13.6520 - val_loss: 144.0556\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 87.81872\n",
      "Epoch 556/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.3976 - val_loss: 137.3488\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 87.81872\n",
      "Epoch 557/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 14.5333 - val_loss: 130.5635\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 87.81872\n",
      "Epoch 558/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 13.9706 - val_loss: 141.5608\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 87.81872\n",
      "Epoch 559/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 13.9616 - val_loss: 137.2746\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 87.81872\n",
      "Epoch 560/1000\n",
      "96/96 [==============================] - 0s 588us/step - loss: 13.5812 - val_loss: 132.1269\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 87.81872\n",
      "Epoch 561/1000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 14.9201 - val_loss: 141.3846\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 87.81872\n",
      "Epoch 562/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 12.6554 - val_loss: 138.5678\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 87.81872\n",
      "Epoch 563/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 13.3654 - val_loss: 137.7369\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 87.81872\n",
      "Epoch 564/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 12.9657 - val_loss: 134.9381\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 87.81872\n",
      "Epoch 565/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 13.1777 - val_loss: 141.5566\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 87.81872\n",
      "Epoch 566/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 12.6381 - val_loss: 133.2776\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 87.81872\n",
      "Epoch 567/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 12.7936 - val_loss: 129.7735\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 87.81872\n",
      "Epoch 568/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.7138 - val_loss: 138.5823\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 87.81872\n",
      "Epoch 569/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 12.6294 - val_loss: 129.6978\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 87.81872\n",
      "Epoch 570/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.9468 - val_loss: 137.1318\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 87.81872\n",
      "Epoch 571/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 13.6795 - val_loss: 134.4811\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 87.81872\n",
      "Epoch 572/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 13.3964 - val_loss: 143.4390\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 87.81872\n",
      "Epoch 573/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.8258 - val_loss: 142.8801\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 87.81872\n",
      "Epoch 574/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 12.7388 - val_loss: 131.9541\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 87.81872\n",
      "Epoch 575/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.4011 - val_loss: 141.7590\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 87.81872\n",
      "Epoch 576/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 13.4514 - val_loss: 148.0384\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 87.81872\n",
      "Epoch 577/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.4155 - val_loss: 139.1710\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 87.81872\n",
      "Epoch 578/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 14.2323 - val_loss: 130.3272\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 87.81872\n",
      "Epoch 579/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 13.6158 - val_loss: 146.2405\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 87.81872\n",
      "Epoch 580/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 13.6607 - val_loss: 139.6259\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 87.81872\n",
      "Epoch 581/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 15.2086 - val_loss: 149.2725\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 87.81872\n",
      "Epoch 582/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 15.4457 - val_loss: 146.4563\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 87.81872\n",
      "Epoch 583/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 13.3261 - val_loss: 164.2822\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 87.81872\n",
      "Epoch 584/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 14.9377 - val_loss: 145.6835\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 87.81872\n",
      "Epoch 585/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 15.3636 - val_loss: 151.6711\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 87.81872\n",
      "Epoch 586/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 16.4029 - val_loss: 150.5646\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 87.81872\n",
      "Epoch 587/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.6901 - val_loss: 157.3502\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 87.81872\n",
      "Epoch 588/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 15.5717 - val_loss: 154.1078\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 87.81872\n",
      "Epoch 589/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 15.0814 - val_loss: 143.3725\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 87.81872\n",
      "Epoch 590/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 15.1595 - val_loss: 134.9878\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 87.81872\n",
      "Epoch 591/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 14.3808 - val_loss: 157.7953\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 87.81872\n",
      "Epoch 592/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 14.4527 - val_loss: 141.0065\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 87.81872\n",
      "Epoch 593/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 15.2639 - val_loss: 155.4004\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 87.81872\n",
      "Epoch 594/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 14.5056 - val_loss: 140.8883\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 87.81872\n",
      "Epoch 595/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 15.6751 - val_loss: 151.4437\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 87.81872\n",
      "Epoch 596/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 17.3078 - val_loss: 136.4619\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 87.81872\n",
      "Epoch 597/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 15.3317 - val_loss: 168.5800\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 87.81872\n",
      "Epoch 598/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 14.0632 - val_loss: 141.5849\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 87.81872\n",
      "Epoch 599/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 540us/step - loss: 13.3533 - val_loss: 140.4437\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 87.81872\n",
      "Epoch 600/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 13.1785 - val_loss: 125.4422\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 87.81872\n",
      "Epoch 601/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 12.0711 - val_loss: 147.7184\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 87.81872\n",
      "Epoch 602/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 12.3835 - val_loss: 131.1894\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 87.81872\n",
      "Epoch 603/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 12.1331 - val_loss: 145.1339\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 87.81872\n",
      "Epoch 604/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.5452 - val_loss: 156.3557\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 87.81872\n",
      "Epoch 605/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.4212 - val_loss: 128.7759\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 87.81872\n",
      "Epoch 606/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 12.6620 - val_loss: 126.6256\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 87.81872\n",
      "Epoch 607/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 12.4397 - val_loss: 157.2224\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 87.81872\n",
      "Epoch 608/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 12.6576 - val_loss: 133.8847\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 87.81872\n",
      "Epoch 609/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 13.1067 - val_loss: 153.1340\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 87.81872\n",
      "Epoch 610/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 13.9484 - val_loss: 150.7254\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 87.81872\n",
      "Epoch 611/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 15.4412 - val_loss: 144.5380\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 87.81872\n",
      "Epoch 612/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.3461 - val_loss: 153.7387\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 87.81872\n",
      "Epoch 613/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 12.6089 - val_loss: 147.2914\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 87.81872\n",
      "Epoch 614/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 12.8305 - val_loss: 147.1377\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 87.81872\n",
      "Epoch 615/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.0839 - val_loss: 133.4507\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 87.81872\n",
      "Epoch 616/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.7444 - val_loss: 141.4015\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 87.81872\n",
      "Epoch 617/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.3467 - val_loss: 178.8438\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 87.81872\n",
      "Epoch 618/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 12.4457 - val_loss: 132.0279\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 87.81872\n",
      "Epoch 619/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 12.0863 - val_loss: 138.7234\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 87.81872\n",
      "Epoch 620/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.1737 - val_loss: 146.6401\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 87.81872\n",
      "Epoch 621/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 11.3226 - val_loss: 149.4731\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 87.81872\n",
      "Epoch 622/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 12.2202 - val_loss: 175.0017\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 87.81872\n",
      "Epoch 623/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.7282 - val_loss: 150.5367\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 87.81872\n",
      "Epoch 624/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.4048 - val_loss: 149.7181\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 87.81872\n",
      "Epoch 625/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.6350 - val_loss: 158.9271\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 87.81872\n",
      "Epoch 626/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 11.9411 - val_loss: 150.4346\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 87.81872\n",
      "Epoch 627/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.6206 - val_loss: 141.6493\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 87.81872\n",
      "Epoch 628/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.5861 - val_loss: 158.9896\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 87.81872\n",
      "Epoch 629/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.9534 - val_loss: 143.2274\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 87.81872\n",
      "Epoch 630/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.9174 - val_loss: 139.3475\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 87.81872\n",
      "Epoch 631/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 12.1221 - val_loss: 165.5646\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 87.81872\n",
      "Epoch 632/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.0677 - val_loss: 139.9810\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 87.81872\n",
      "Epoch 633/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.9217 - val_loss: 146.6408\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 87.81872\n",
      "Epoch 634/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.7436 - val_loss: 162.0843\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 87.81872\n",
      "Epoch 635/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.5100 - val_loss: 141.3327\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 87.81872\n",
      "Epoch 636/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 14.7225 - val_loss: 151.6911\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 87.81872\n",
      "Epoch 637/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.5531 - val_loss: 154.9868\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 87.81872\n",
      "Epoch 638/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.9389 - val_loss: 147.0795\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 87.81872\n",
      "Epoch 639/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.8814 - val_loss: 167.9546\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 87.81872\n",
      "Epoch 640/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 12.5305 - val_loss: 162.0533\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 87.81872\n",
      "Epoch 641/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 13.2047 - val_loss: 163.5379\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 87.81872\n",
      "Epoch 642/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 12.3310 - val_loss: 150.3114\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 87.81872\n",
      "Epoch 643/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.0317 - val_loss: 174.5313\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 87.81872\n",
      "Epoch 644/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.5523 - val_loss: 180.6098\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 87.81872\n",
      "Epoch 645/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.2652 - val_loss: 139.2021\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 87.81872\n",
      "Epoch 646/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 11.0044 - val_loss: 168.3729\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 87.81872\n",
      "Epoch 647/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.0694 - val_loss: 168.1682\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 87.81872\n",
      "Epoch 648/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.5308 - val_loss: 173.5149\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 87.81872\n",
      "Epoch 649/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 12.8008 - val_loss: 183.5694\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 87.81872\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 527us/step - loss: 11.9785 - val_loss: 150.1714\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 87.81872\n",
      "Epoch 651/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.9335 - val_loss: 145.3133\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 87.81872\n",
      "Epoch 652/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 11.8460 - val_loss: 160.0594\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 87.81872\n",
      "Epoch 653/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.3708 - val_loss: 154.3112\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 87.81872\n",
      "Epoch 654/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 11.4636 - val_loss: 154.9037\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 87.81872\n",
      "Epoch 655/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.8657 - val_loss: 177.3727\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 87.81872\n",
      "Epoch 656/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.3726 - val_loss: 174.2173\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 87.81872\n",
      "Epoch 657/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 11.4738 - val_loss: 158.8878\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 87.81872\n",
      "Epoch 658/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.1111 - val_loss: 156.8974\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 87.81872\n",
      "Epoch 659/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 12.8725 - val_loss: 173.5949\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 87.81872\n",
      "Epoch 660/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 14.4513 - val_loss: 170.4328\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 87.81872\n",
      "Epoch 661/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 13.0187 - val_loss: 204.6510\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 87.81872\n",
      "Epoch 662/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 14.1304 - val_loss: 164.7721\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 87.81872\n",
      "Epoch 663/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 14.7338 - val_loss: 161.7377\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 87.81872\n",
      "Epoch 664/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 14.3409 - val_loss: 170.2720\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 87.81872\n",
      "Epoch 665/1000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 13.5450 - val_loss: 158.2714\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 87.81872\n",
      "Epoch 666/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 13.4431 - val_loss: 148.6946\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 87.81872\n",
      "Epoch 667/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 14.2170 - val_loss: 163.2509\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 87.81872\n",
      "Epoch 668/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 17.0721 - val_loss: 164.5471\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 87.81872\n",
      "Epoch 669/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 15.5541 - val_loss: 185.4018\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 87.81872\n",
      "Epoch 670/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 18.4958 - val_loss: 168.8855\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 87.81872\n",
      "Epoch 671/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 14.7455 - val_loss: 151.2781\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 87.81872\n",
      "Epoch 672/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 15.2961 - val_loss: 148.7789\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 87.81872\n",
      "Epoch 673/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 15.8711 - val_loss: 197.0832\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 87.81872\n",
      "Epoch 674/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 17.4056 - val_loss: 178.6218\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 87.81872\n",
      "Epoch 675/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 19.7878 - val_loss: 166.2067\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 87.81872\n",
      "Epoch 676/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 18.6959 - val_loss: 134.4068\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 87.81872\n",
      "Epoch 677/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 16.1127 - val_loss: 152.1866\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 87.81872\n",
      "Epoch 678/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 16.4205 - val_loss: 188.8346\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 87.81872\n",
      "Epoch 679/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 15.9335 - val_loss: 142.7133\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 87.81872\n",
      "Epoch 680/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 15.5940 - val_loss: 162.2404\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 87.81872\n",
      "Epoch 681/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 14.7415 - val_loss: 188.7059\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 87.81872\n",
      "Epoch 682/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 12.3995 - val_loss: 151.4507\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 87.81872\n",
      "Epoch 683/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 12.4372 - val_loss: 175.3071\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 87.81872\n",
      "Epoch 684/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.5555 - val_loss: 150.3561\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 87.81872\n",
      "Epoch 685/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 13.5838 - val_loss: 175.0049\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 87.81872\n",
      "Epoch 686/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 19.9379 - val_loss: 180.5454\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 87.81872\n",
      "Epoch 687/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 15.4829 - val_loss: 175.2295\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 87.81872\n",
      "Epoch 688/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 14.8952 - val_loss: 183.8599\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 87.81872\n",
      "Epoch 689/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 14.6142 - val_loss: 177.8687\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 87.81872\n",
      "Epoch 690/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 14.2920 - val_loss: 195.0928\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 87.81872\n",
      "Epoch 691/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 14.2065 - val_loss: 140.0108\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 87.81872\n",
      "Epoch 692/1000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 14.9307 - val_loss: 158.1970\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 87.81872\n",
      "Epoch 693/1000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 13.0527 - val_loss: 160.3843\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 87.81872\n",
      "Epoch 694/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.6513 - val_loss: 141.1083\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 87.81872\n",
      "Epoch 695/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 13.2315 - val_loss: 159.9571\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 87.81872\n",
      "Epoch 696/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 13.2081 - val_loss: 159.8054\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 87.81872\n",
      "Epoch 697/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.4626 - val_loss: 150.7864\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 87.81872\n",
      "Epoch 698/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.2218 - val_loss: 172.0118\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 87.81872\n",
      "Epoch 699/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.9248 - val_loss: 145.8299\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 87.81872\n",
      "Epoch 700/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 11.4236 - val_loss: 141.3991\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 87.81872\n",
      "Epoch 701/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 561us/step - loss: 14.8484 - val_loss: 168.7511\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 87.81872\n",
      "Epoch 702/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 15.0944 - val_loss: 157.0925\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 87.81872\n",
      "Epoch 703/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 14.4519 - val_loss: 160.7304\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 87.81872\n",
      "Epoch 704/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 13.0303 - val_loss: 163.5156\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 87.81872\n",
      "Epoch 705/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 13.7301 - val_loss: 153.4360\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 87.81872\n",
      "Epoch 706/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 14.5434 - val_loss: 156.2065\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 87.81872\n",
      "Epoch 707/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 16.3964 - val_loss: 179.9330\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 87.81872\n",
      "Epoch 708/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 17.2214 - val_loss: 160.7003\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 87.81872\n",
      "Epoch 709/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 15.4811 - val_loss: 153.3207\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 87.81872\n",
      "Epoch 710/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 15.3726 - val_loss: 157.3688\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 87.81872\n",
      "Epoch 711/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 15.8655 - val_loss: 144.9310\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 87.81872\n",
      "Epoch 712/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 15.5343 - val_loss: 137.5672\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 87.81872\n",
      "Epoch 713/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 13.6533 - val_loss: 161.5626\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 87.81872\n",
      "Epoch 714/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 13.3159 - val_loss: 144.7125\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 87.81872\n",
      "Epoch 715/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 13.3468 - val_loss: 152.6026\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 87.81872\n",
      "Epoch 716/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 12.7523 - val_loss: 140.1882\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 87.81872\n",
      "Epoch 717/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 12.1796 - val_loss: 167.3501\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 87.81872\n",
      "Epoch 718/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 13.1664 - val_loss: 163.6454\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 87.81872\n",
      "Epoch 719/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.9413 - val_loss: 156.3687\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 87.81872\n",
      "Epoch 720/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.9608 - val_loss: 159.5492\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 87.81872\n",
      "Epoch 721/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 12.7489 - val_loss: 161.6256\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 87.81872\n",
      "Epoch 722/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.8764 - val_loss: 164.6090\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 87.81872\n",
      "Epoch 723/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.5026 - val_loss: 169.3638\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 87.81872\n",
      "Epoch 724/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 12.3893 - val_loss: 158.4796\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 87.81872\n",
      "Epoch 725/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 13.0795 - val_loss: 156.5566\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 87.81872\n",
      "Epoch 726/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 12.5347 - val_loss: 153.4907\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 87.81872\n",
      "Epoch 727/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.5373 - val_loss: 141.3314\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 87.81872\n",
      "Epoch 728/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 13.9571 - val_loss: 197.6258\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 87.81872\n",
      "Epoch 729/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 13.1232 - val_loss: 160.1379\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 87.81872\n",
      "Epoch 730/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 12.1229 - val_loss: 155.7838\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 87.81872\n",
      "Epoch 731/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 13.7010 - val_loss: 165.5861\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 87.81872\n",
      "Epoch 732/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 12.2963 - val_loss: 142.9791\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 87.81872\n",
      "Epoch 733/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 13.0798 - val_loss: 173.5119\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 87.81872\n",
      "Epoch 734/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 14.3521 - val_loss: 146.3046\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 87.81872\n",
      "Epoch 735/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 13.9391 - val_loss: 188.8118\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 87.81872\n",
      "Epoch 736/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 14.0881 - val_loss: 199.8165\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 87.81872\n",
      "Epoch 737/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 14.5214 - val_loss: 169.2207\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 87.81872\n",
      "Epoch 738/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.9945 - val_loss: 168.9544\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 87.81872\n",
      "Epoch 739/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 14.3592 - val_loss: 170.1490\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 87.81872\n",
      "Epoch 740/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 15.5646 - val_loss: 203.4913\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 87.81872\n",
      "Epoch 741/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 15.0238 - val_loss: 162.9212\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 87.81872\n",
      "Epoch 742/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 17.5904 - val_loss: 203.4581\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 87.81872\n",
      "Epoch 743/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.4146 - val_loss: 157.5221\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 87.81872\n",
      "Epoch 744/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 19.0008 - val_loss: 194.3856\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 87.81872\n",
      "Epoch 745/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 17.8496 - val_loss: 172.1705\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 87.81872\n",
      "Epoch 746/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 17.1695 - val_loss: 220.4972\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 87.81872\n",
      "Epoch 747/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 22.0221 - val_loss: 156.1384\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 87.81872\n",
      "Epoch 748/1000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 21.5647 - val_loss: 177.8246\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 87.81872\n",
      "Epoch 749/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 20.7098 - val_loss: 155.9385\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 87.81872\n",
      "Epoch 750/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 21.4611 - val_loss: 164.7604\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 87.81872\n",
      "Epoch 751/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 19.1914 - val_loss: 151.1019\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 87.81872\n",
      "Epoch 752/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 575us/step - loss: 20.6733 - val_loss: 161.8547\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 87.81872\n",
      "Epoch 753/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 35.4006 - val_loss: 195.5609\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 87.81872\n",
      "Epoch 754/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 46.8210 - val_loss: 260.2275\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 87.81872\n",
      "Epoch 755/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 54.9085 - val_loss: 233.1690\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 87.81872\n",
      "Epoch 756/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 48.3628 - val_loss: 219.2035\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 87.81872\n",
      "Epoch 757/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 61.8446 - val_loss: 170.0514\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 87.81872\n",
      "Epoch 758/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 69.1216 - val_loss: 307.1637\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 87.81872\n",
      "Epoch 759/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 99.4668 - val_loss: 157.2684\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 87.81872\n",
      "Epoch 760/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 51.1106 - val_loss: 184.6362\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 87.81872\n",
      "Epoch 761/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 89.8190 - val_loss: 193.3363\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 87.81872\n",
      "Epoch 762/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 55.1355 - val_loss: 144.3738\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 87.81872\n",
      "Epoch 763/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 36.3088 - val_loss: 130.2657\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 87.81872\n",
      "Epoch 764/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 34.2369 - val_loss: 125.3220\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 87.81872\n",
      "Epoch 765/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 26.6632 - val_loss: 121.7894\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 87.81872\n",
      "Epoch 766/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 20.4670 - val_loss: 121.2374\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 87.81872\n",
      "Epoch 767/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 16.9804 - val_loss: 125.3403\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 87.81872\n",
      "Epoch 768/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 15.7978 - val_loss: 139.8117\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 87.81872\n",
      "Epoch 769/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 15.2961 - val_loss: 116.5480\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 87.81872\n",
      "Epoch 770/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 14.9903 - val_loss: 123.3839\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 87.81872\n",
      "Epoch 771/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 14.5928 - val_loss: 131.2150\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 87.81872\n",
      "Epoch 772/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 14.1379 - val_loss: 143.0688\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 87.81872\n",
      "Epoch 773/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 13.2301 - val_loss: 127.2733\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 87.81872\n",
      "Epoch 774/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 13.5136 - val_loss: 128.0149\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 87.81872\n",
      "Epoch 775/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.3091 - val_loss: 128.8534\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 87.81872\n",
      "Epoch 776/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 12.2551 - val_loss: 122.7712\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 87.81872\n",
      "Epoch 777/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.3425 - val_loss: 124.9016\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 87.81872\n",
      "Epoch 778/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 12.4494 - val_loss: 137.5252\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 87.81872\n",
      "Epoch 779/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 12.0253 - val_loss: 130.1407\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 87.81872\n",
      "Epoch 780/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.3937 - val_loss: 137.8156\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 87.81872\n",
      "Epoch 781/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.4911 - val_loss: 145.2545\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 87.81872\n",
      "Epoch 782/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 12.0688 - val_loss: 130.5166\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 87.81872\n",
      "Epoch 783/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.8158 - val_loss: 135.1054\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 87.81872\n",
      "Epoch 784/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 11.8681 - val_loss: 136.6194\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 87.81872\n",
      "Epoch 785/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 11.8350 - val_loss: 143.4989\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 87.81872\n",
      "Epoch 786/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.4901 - val_loss: 144.2620\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 87.81872\n",
      "Epoch 787/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.9424 - val_loss: 123.6128\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 87.81872\n",
      "Epoch 788/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.7111 - val_loss: 145.9789\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 87.81872\n",
      "Epoch 789/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 11.8491 - val_loss: 150.0788\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 87.81872\n",
      "Epoch 790/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.1925 - val_loss: 131.1561\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 87.81872\n",
      "Epoch 791/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.2752 - val_loss: 127.9559\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 87.81872\n",
      "Epoch 792/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.7721 - val_loss: 158.0056\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 87.81872\n",
      "Epoch 793/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 11.6180 - val_loss: 154.9092\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 87.81872\n",
      "Epoch 794/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.0599 - val_loss: 131.1289\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 87.81872\n",
      "Epoch 795/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 11.1460 - val_loss: 151.4202\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 87.81872\n",
      "Epoch 796/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 11.6068 - val_loss: 152.5096\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 87.81872\n",
      "Epoch 797/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 10.9783 - val_loss: 150.9928\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 87.81872\n",
      "Epoch 798/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 10.8602 - val_loss: 157.6135\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 87.81872\n",
      "Epoch 799/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 10.9291 - val_loss: 141.5794\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 87.81872\n",
      "Epoch 800/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.3596 - val_loss: 146.9906\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 87.81872\n",
      "Epoch 801/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.5008 - val_loss: 156.6729\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 87.81872\n",
      "Epoch 802/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.2449 - val_loss: 117.6143\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 87.81872\n",
      "Epoch 803/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 551us/step - loss: 11.0955 - val_loss: 132.0826\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 87.81872\n",
      "Epoch 804/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.2770 - val_loss: 159.6700\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 87.81872\n",
      "Epoch 805/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 10.7077 - val_loss: 157.8790\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 87.81872\n",
      "Epoch 806/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 11.0643 - val_loss: 136.3095\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 87.81872\n",
      "Epoch 807/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.9614 - val_loss: 157.8309\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 87.81872\n",
      "Epoch 808/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 11.2108 - val_loss: 148.4944\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 87.81872\n",
      "Epoch 809/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.3747 - val_loss: 147.4773\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 87.81872\n",
      "Epoch 810/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 10.8543 - val_loss: 156.3023\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 87.81872\n",
      "Epoch 811/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 10.8789 - val_loss: 132.7133\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 87.81872\n",
      "Epoch 812/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.9552 - val_loss: 164.9775\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 87.81872\n",
      "Epoch 813/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.2908 - val_loss: 152.5980\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 87.81872\n",
      "Epoch 814/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.6494 - val_loss: 179.0006\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 87.81872\n",
      "Epoch 815/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 13.0449 - val_loss: 142.1655\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 87.81872\n",
      "Epoch 816/1000\n",
      "96/96 [==============================] - 0s 578us/step - loss: 13.5248 - val_loss: 137.7981\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 87.81872\n",
      "Epoch 817/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.2595 - val_loss: 172.7684\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 87.81872\n",
      "Epoch 818/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.7396 - val_loss: 131.6168\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 87.81872\n",
      "Epoch 819/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.8282 - val_loss: 166.0161\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 87.81872\n",
      "Epoch 820/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 12.4730 - val_loss: 132.7246\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 87.81872\n",
      "Epoch 821/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 12.7939 - val_loss: 143.1982\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 87.81872\n",
      "Epoch 822/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.8352 - val_loss: 135.2657\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 87.81872\n",
      "Epoch 823/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 12.5344 - val_loss: 139.7388\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 87.81872\n",
      "Epoch 824/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.7519 - val_loss: 147.4383\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 87.81872\n",
      "Epoch 825/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 11.4929 - val_loss: 169.2168\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 87.81872\n",
      "Epoch 826/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.6348 - val_loss: 126.4883\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 87.81872\n",
      "Epoch 827/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 11.1008 - val_loss: 176.3915\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 87.81872\n",
      "Epoch 828/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 10.6960 - val_loss: 139.9329\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 87.81872\n",
      "Epoch 829/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 10.8569 - val_loss: 171.4384\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 87.81872\n",
      "Epoch 830/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 10.7875 - val_loss: 151.3774\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 87.81872\n",
      "Epoch 831/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.0137 - val_loss: 158.8586\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 87.81872\n",
      "Epoch 832/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 11.4380 - val_loss: 156.7298\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 87.81872\n",
      "Epoch 833/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.7296 - val_loss: 130.5241\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 87.81872\n",
      "Epoch 834/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 13.1890 - val_loss: 152.0746\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 87.81872\n",
      "Epoch 835/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.4570 - val_loss: 154.9005\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 87.81872\n",
      "Epoch 836/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.1835 - val_loss: 174.1840\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 87.81872\n",
      "Epoch 837/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.5491 - val_loss: 155.5319\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 87.81872\n",
      "Epoch 838/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.3075 - val_loss: 155.4659\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 87.81872\n",
      "Epoch 839/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.2653 - val_loss: 141.4683\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 87.81872\n",
      "Epoch 840/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.3569 - val_loss: 157.6311\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 87.81872\n",
      "Epoch 841/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 10.6750 - val_loss: 147.2718\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 87.81872\n",
      "Epoch 842/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 10.1364 - val_loss: 134.0069\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 87.81872\n",
      "Epoch 843/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.7749 - val_loss: 134.8271\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 87.81872\n",
      "Epoch 844/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.6911 - val_loss: 132.6987\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 87.81872\n",
      "Epoch 845/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 10.4986 - val_loss: 158.3644\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 87.81872\n",
      "Epoch 846/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.1270 - val_loss: 148.4404\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 87.81872\n",
      "Epoch 847/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 11.6967 - val_loss: 141.4875\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 87.81872\n",
      "Epoch 848/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.1983 - val_loss: 182.6288\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 87.81872\n",
      "Epoch 849/1000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 12.4266 - val_loss: 181.2639\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 87.81872\n",
      "Epoch 850/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.1001 - val_loss: 141.0606\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 87.81872\n",
      "Epoch 851/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 10.9576 - val_loss: 134.9728\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 87.81872\n",
      "Epoch 852/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 12.2857 - val_loss: 138.0884\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 87.81872\n",
      "Epoch 853/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 11.0326 - val_loss: 147.2157\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 87.81872\n",
      "Epoch 854/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 558us/step - loss: 11.2722 - val_loss: 149.5862\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 87.81872\n",
      "Epoch 855/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 11.1985 - val_loss: 166.8099\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 87.81872\n",
      "Epoch 856/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.8043 - val_loss: 160.5762\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 87.81872\n",
      "Epoch 857/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 11.5275 - val_loss: 166.3970\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 87.81872\n",
      "Epoch 858/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 11.6503 - val_loss: 155.3446\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 87.81872\n",
      "Epoch 859/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 11.5776 - val_loss: 133.8733\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 87.81872\n",
      "Epoch 860/1000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 11.0556 - val_loss: 171.5617\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 87.81872\n",
      "Epoch 861/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 10.6691 - val_loss: 172.3613\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 87.81872\n",
      "Epoch 862/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.0289 - val_loss: 164.4049\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 87.81872\n",
      "Epoch 863/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 10.6205 - val_loss: 190.4898\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 87.81872\n",
      "Epoch 864/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.4686 - val_loss: 187.2320\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 87.81872\n",
      "Epoch 865/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.5940 - val_loss: 148.2641\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 87.81872\n",
      "Epoch 866/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.5976 - val_loss: 168.8697\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 87.81872\n",
      "Epoch 867/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.7427 - val_loss: 171.9173\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 87.81872\n",
      "Epoch 868/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 15.1281 - val_loss: 178.0791\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 87.81872\n",
      "Epoch 869/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 13.1012 - val_loss: 163.0815\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 87.81872\n",
      "Epoch 870/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.5471 - val_loss: 135.8697\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 87.81872\n",
      "Epoch 871/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.6162 - val_loss: 155.7466\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 87.81872\n",
      "Epoch 872/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.0373 - val_loss: 154.6207\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 87.81872\n",
      "Epoch 873/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 10.5812 - val_loss: 159.2740\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 87.81872\n",
      "Epoch 874/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 11.5165 - val_loss: 159.4220\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 87.81872\n",
      "Epoch 875/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 11.7384 - val_loss: 149.8715\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 87.81872\n",
      "Epoch 876/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.8673 - val_loss: 130.1143\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 87.81872\n",
      "Epoch 877/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.5115 - val_loss: 180.0658\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 87.81872\n",
      "Epoch 878/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.5875 - val_loss: 180.1887\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 87.81872\n",
      "Epoch 879/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.7774 - val_loss: 159.8028\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 87.81872\n",
      "Epoch 880/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 10.5587 - val_loss: 162.5740\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 87.81872\n",
      "Epoch 881/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 10.4997 - val_loss: 148.5128\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 87.81872\n",
      "Epoch 882/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.1586 - val_loss: 164.6986\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 87.81872\n",
      "Epoch 883/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.6229 - val_loss: 158.5640\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 87.81872\n",
      "Epoch 884/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.1280 - val_loss: 149.3216\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 87.81872\n",
      "Epoch 885/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 13.5702 - val_loss: 179.1822\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 87.81872\n",
      "Epoch 886/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.6496 - val_loss: 153.1782\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 87.81872\n",
      "Epoch 887/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 14.2543 - val_loss: 169.1124\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 87.81872\n",
      "Epoch 888/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 15.1435 - val_loss: 173.0078\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 87.81872\n",
      "Epoch 889/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 15.1833 - val_loss: 148.3022\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 87.81872\n",
      "Epoch 890/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 15.0263 - val_loss: 157.4600\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 87.81872\n",
      "Epoch 891/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.2254 - val_loss: 157.3918\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 87.81872\n",
      "Epoch 892/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 13.6821 - val_loss: 169.7317\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 87.81872\n",
      "Epoch 893/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 14.0659 - val_loss: 151.6774\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 87.81872\n",
      "Epoch 894/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 12.3250 - val_loss: 147.3604\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 87.81872\n",
      "Epoch 895/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 11.9474 - val_loss: 143.2496\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 87.81872\n",
      "Epoch 896/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 11.3483 - val_loss: 149.2865\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 87.81872\n",
      "Epoch 897/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.3738 - val_loss: 167.8390\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 87.81872\n",
      "Epoch 898/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 11.2257 - val_loss: 169.4789\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 87.81872\n",
      "Epoch 899/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 10.2578 - val_loss: 163.5299\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 87.81872\n",
      "Epoch 900/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 11.2357 - val_loss: 149.8194\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 87.81872\n",
      "Epoch 901/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 10.3158 - val_loss: 153.8359\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 87.81872\n",
      "Epoch 902/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.0607 - val_loss: 163.6231\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 87.81872\n",
      "Epoch 903/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 10.3875 - val_loss: 167.8252\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 87.81872\n",
      "Epoch 904/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 11.2263 - val_loss: 166.8185\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 87.81872\n",
      "Epoch 905/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 555us/step - loss: 11.1869 - val_loss: 154.8429\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 87.81872\n",
      "Epoch 906/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 11.3231 - val_loss: 174.7449\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 87.81872\n",
      "Epoch 907/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.3165 - val_loss: 186.8538\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 87.81872\n",
      "Epoch 908/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 11.8704 - val_loss: 165.9483\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 87.81872\n",
      "Epoch 909/1000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 11.0055 - val_loss: 166.4275\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 87.81872\n",
      "Epoch 910/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.0555 - val_loss: 148.8279\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 87.81872\n",
      "Epoch 911/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.6948 - val_loss: 138.9741\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 87.81872\n",
      "Epoch 912/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 10.8108 - val_loss: 155.7525\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 87.81872\n",
      "Epoch 913/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 10.8792 - val_loss: 150.2025\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 87.81872\n",
      "Epoch 914/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 10.2494 - val_loss: 158.5599\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 87.81872\n",
      "Epoch 915/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 10.6400 - val_loss: 164.9632\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 87.81872\n",
      "Epoch 916/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.1410 - val_loss: 160.2582\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 87.81872\n",
      "Epoch 917/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 10.6877 - val_loss: 172.7857\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 87.81872\n",
      "Epoch 918/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 11.0739 - val_loss: 155.7248\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 87.81872\n",
      "Epoch 919/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 10.6033 - val_loss: 145.6512\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 87.81872\n",
      "Epoch 920/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 11.2268 - val_loss: 151.4039\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 87.81872\n",
      "Epoch 921/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.3709 - val_loss: 164.7707\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 87.81872\n",
      "Epoch 922/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 14.3919 - val_loss: 183.0376\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 87.81872\n",
      "Epoch 923/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.7052 - val_loss: 162.5930\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 87.81872\n",
      "Epoch 924/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 13.9015 - val_loss: 170.2421\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 87.81872\n",
      "Epoch 925/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.7280 - val_loss: 173.6172\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 87.81872\n",
      "Epoch 926/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.8766 - val_loss: 153.6653\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 87.81872\n",
      "Epoch 927/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 12.1619 - val_loss: 155.3687\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 87.81872\n",
      "Epoch 928/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 12.4992 - val_loss: 163.4366\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 87.81872\n",
      "Epoch 929/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.1195 - val_loss: 166.2337\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 87.81872\n",
      "Epoch 930/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.1238 - val_loss: 151.5660\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 87.81872\n",
      "Epoch 931/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 11.0945 - val_loss: 169.8820\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 87.81872\n",
      "Epoch 932/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.1678 - val_loss: 164.5005\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 87.81872\n",
      "Epoch 933/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.9228 - val_loss: 143.5955\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 87.81872\n",
      "Epoch 934/1000\n",
      "96/96 [==============================] - 0s 578us/step - loss: 10.9691 - val_loss: 165.4063\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 87.81872\n",
      "Epoch 935/1000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 11.0721 - val_loss: 184.4479\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 87.81872\n",
      "Epoch 936/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.4798 - val_loss: 168.1893\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 87.81872\n",
      "Epoch 937/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 10.4393 - val_loss: 160.7802\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 87.81872\n",
      "Epoch 938/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.1124 - val_loss: 165.9436\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 87.81872\n",
      "Epoch 939/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.6432 - val_loss: 176.5947\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 87.81872\n",
      "Epoch 940/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 10.3724 - val_loss: 162.0174\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 87.81872\n",
      "Epoch 941/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 10.6563 - val_loss: 176.0089\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 87.81872\n",
      "Epoch 942/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 10.9655 - val_loss: 162.5565\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 87.81872\n",
      "Epoch 943/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.0256 - val_loss: 146.7124\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 87.81872\n",
      "Epoch 944/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.7128 - val_loss: 193.8674\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 87.81872\n",
      "Epoch 945/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.0097 - val_loss: 168.3162\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 87.81872\n",
      "Epoch 946/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 11.1688 - val_loss: 179.3406\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 87.81872\n",
      "Epoch 947/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 12.0150 - val_loss: 150.5991\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 87.81872\n",
      "Epoch 948/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.5644 - val_loss: 167.0640\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 87.81872\n",
      "Epoch 949/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.8302 - val_loss: 170.8020\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 87.81872\n",
      "Epoch 950/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 10.9239 - val_loss: 169.2526\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 87.81872\n",
      "Epoch 951/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.7769 - val_loss: 139.9788\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 87.81872\n",
      "Epoch 952/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 12.3224 - val_loss: 198.9939\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 87.81872\n",
      "Epoch 953/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 11.6904 - val_loss: 154.6961\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 87.81872\n",
      "Epoch 954/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.6399 - val_loss: 176.9427\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 87.81872\n",
      "Epoch 955/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.1906 - val_loss: 166.5107\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 87.81872\n",
      "Epoch 956/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 532us/step - loss: 10.6107 - val_loss: 162.3293\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 87.81872\n",
      "Epoch 957/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 11.0309 - val_loss: 163.6185\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 87.81872\n",
      "Epoch 958/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 10.9823 - val_loss: 151.8517\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 87.81872\n",
      "Epoch 959/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.5709 - val_loss: 168.4185\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 87.81872\n",
      "Epoch 960/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 10.9874 - val_loss: 195.7079\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 87.81872\n",
      "Epoch 961/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 12.7816 - val_loss: 177.4341\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 87.81872\n",
      "Epoch 962/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 15.9872 - val_loss: 173.9039\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 87.81872\n",
      "Epoch 963/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 13.8532 - val_loss: 168.1131\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 87.81872\n",
      "Epoch 964/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 14.3731 - val_loss: 155.4696\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 87.81872\n",
      "Epoch 965/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.8861 - val_loss: 180.5036\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 87.81872\n",
      "Epoch 966/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 12.2103 - val_loss: 187.8173\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 87.81872\n",
      "Epoch 967/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 12.2912 - val_loss: 166.5642\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 87.81872\n",
      "Epoch 968/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 11.7805 - val_loss: 176.4651\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 87.81872\n",
      "Epoch 969/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 10.9727 - val_loss: 152.3117\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 87.81872\n",
      "Epoch 970/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.1335 - val_loss: 167.5574\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 87.81872\n",
      "Epoch 971/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.2943 - val_loss: 154.8160\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 87.81872\n",
      "Epoch 972/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 9.6440 - val_loss: 149.1822\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 87.81872\n",
      "Epoch 973/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 10.4900 - val_loss: 154.7318\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 87.81872\n",
      "Epoch 974/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.2210 - val_loss: 191.7327\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 87.81872\n",
      "Epoch 975/1000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 12.0941 - val_loss: 168.3202\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 87.81872\n",
      "Epoch 976/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 12.1939 - val_loss: 159.9213\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 87.81872\n",
      "Epoch 977/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.6559 - val_loss: 154.8651\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 87.81872\n",
      "Epoch 978/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.1504 - val_loss: 162.3693\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 87.81872\n",
      "Epoch 979/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.7314 - val_loss: 156.1450\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 87.81872\n",
      "Epoch 980/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 11.7403 - val_loss: 164.1030\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 87.81872\n",
      "Epoch 981/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 11.4256 - val_loss: 154.6846\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 87.81872\n",
      "Epoch 982/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 11.6721 - val_loss: 145.1982\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 87.81872\n",
      "Epoch 983/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.9397 - val_loss: 181.0849\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 87.81872\n",
      "Epoch 984/1000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 16.3498 - val_loss: 186.8528\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 87.81872\n",
      "Epoch 985/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.3308 - val_loss: 152.1455\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 87.81872\n",
      "Epoch 986/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.7144 - val_loss: 165.4265\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 87.81872\n",
      "Epoch 987/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 12.6290 - val_loss: 168.4019\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 87.81872\n",
      "Epoch 988/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.7702 - val_loss: 184.6219\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 87.81872\n",
      "Epoch 989/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.3185 - val_loss: 172.9146\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 87.81872\n",
      "Epoch 990/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 12.7808 - val_loss: 154.5841\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 87.81872\n",
      "Epoch 991/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 13.3154 - val_loss: 185.2503\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 87.81872\n",
      "Epoch 992/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 14.2059 - val_loss: 165.6479\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 87.81872\n",
      "Epoch 993/1000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 18.2638 - val_loss: 182.7731\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 87.81872\n",
      "Epoch 994/1000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 16.2095 - val_loss: 221.2591\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 87.81872\n",
      "Epoch 995/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 17.0732 - val_loss: 169.2773\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 87.81872\n",
      "Epoch 996/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 20.6327 - val_loss: 156.9848\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 87.81872\n",
      "Epoch 997/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 16.4634 - val_loss: 159.7049\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 87.81872\n",
      "Epoch 998/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 15.7121 - val_loss: 208.1706\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 87.81872\n",
      "Epoch 999/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 14.5151 - val_loss: 172.7648\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 87.81872\n",
      "Epoch 1000/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.5690 - val_loss: 149.5115\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 87.81872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4045400f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "vae.fit(x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None), \n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_output = sess.run( z_mean, feed_dict={'encoder_input: 0':x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01929458,  0.06869002, -0.00900903], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m = np.mean(z_output,axis=0)\n",
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.819688  , 0.15810818, 0.05383771],\n",
       "       [0.15810818, 1.77326946, 0.20359517],\n",
       "       [0.05383771, 0.20359517, 1.84243953]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(z_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Pearson correlation among Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff407a05828>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(np.corrcoef(z_output.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01929458,  0.06869002, -0.00900903], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJAAAABsCAYAAAA1z0hYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHItJREFUeJzt3XtwVPX5x/HnnN1skk02Fy4BASECNWlKJSpeKtTijHak2EFn/EOcztjOtDrUC9hWHdtptTjTC/9o9afYUosz2EpHBgHt1JEKUoSqQ1FIFSgQAsglQELYkGSzt+f3R367PyLkJDnnLOfs+n7NfMclm7M+n5y9nH3O95xjqKoAAAAAAAAAAzG9LgAAAAAAAAD+RgMJAAAAAAAAlmggAQAAAAAAwBINJAAAAAAAAFiigQQAAAAAAABLNJAAAAAAAABgiQYSAAAAAAAALNFAAgAAAAAAgCUaSAAAAAAAALBEAwkAAAAAAACWaCABAAAAAADAEg0kAAAAAAAAWAp6XYAdhmGo1zUAAAAAAAAUElU1BrqPGUgAAAAAAACwRAMJAAAAAAAAlmggAQAAAAAAwBINJAAAAAAAAFiigQQAAAAAAABLNJAAAAAAAABgiQYSAAAAAAAALNFAAgAAAAAAgCUaSAAAAAAAALBEAwkAAAAAAACWaCABAAAAAADAEg0kAAAAAAAAWKKBBAAAAAAAAEs0kAAAAAAAAGCJBhIAAAAAAAAs0UACAAAAAACAJRpIAABAREQMw/C6hJyZN2+e3HHHHfLkk0/K1KlTJRgMel0SXPLUU0/JmjVrvC4jZw4fPix79+71uoyc6enp8bqEnEmlUtLe3u51GTmjqtLZ2el1GTmjqgX9/FRV6e3t9bqMnFFVr0vIGVWVZDLpyf+bBhIAABZM05RIJCKRSETC4bAUFRV5XZKrqqurZcOGDbJt2zZ54YUX5MYbb5TJkyeLaRbGJkIgEJDdu3fLiy++KP/zP/8j9957r6xbt04WLFgg4XC4oJpmhmFIKBSSn/3sZ7J27VppamqSQ4cOiWEYUlJSIiNHjvS6xCG79tpr5b777pPx48fLxIkT+923Zs0a+f73vy/V1dVy6NAhmTNnjqxdu9ajSu0JBoOyYsUKaWhokLvuuiv782QyKZ2dnbJu3TopLS2Vnp4eGTlypHzve9/zsNrhqaysFMMwZPPmzXL11VfL7373u+x9qirpdFpaWlokGAxKMpmUUCjkYbXDl6l3z549MmvWrH73qWq2qWKappimed7z1+8CgYCIiLS2tsrhw4f73ZfJl0gksl/Oq6urL3qNThiGIdFoVObPn39e80tVJZVKZW+LiITD4Yteo1OxWEwWLFgg8Xg8+7PMusvkyvy3uLjYkxqdSCQS8sQTT2TXlcjA+fJROp2WP/zhD/1+9vl8IuLZjjAjH/+4hmHkX9EAgLwUiUSkp6dHUqlUtqkSCASkqqpKTp48mdcbKaZpysaNG+XEiRPy8ccfy9ixY6WiokL27t0rlZWV8tRTT0k0GvW6TNtCoZC0trZKIBCQkpIS6e3tlWg0mt2j/Morr8jixYslnU57XOnQhUIhMQxDDMMQVZVQKCTxeFyqq6tlwoQJctlll8m0adPka1/7mnz22Wdy5513SnFxscydO1fq6upk8uTJ8uMf/9jrGAMyTVNUVWbOnClr1qw5r+n19NNPy8KFC2X79u1y5ZVXypYtW+T666+Xs2fP5lWD7L777pPf/OY3UlVVdcH729vbZcSIEfLBBx/INddcI+l0Oq+a19OnT5f33ntPysvLsz+Lx+PZ5ksikZCioiLZsWOHXHHFFXnXsB4xYoR89tlnUlpamv2ZqmYb0ufeFsm/2Z0lJSVy5swZy+be3r175Utf+pKI5F++ZDKZbZQNZP/+/TJlyhQRya98hmFIMpkc9DV14MABueyyy7LL5JN0Oj1ozc3NzTJ58uS8yyYyvOZXrvKp6sAPfG43K1+GiOjFHj09PRf9/8lwZ3R3d3teA/nIN9CIxWKe15CLYRiGmqapgUDA81qc5shkOffnkUhEOzs7ta6uTv9vp0ZejpEjR+rtt9+uixYt0pkzZ2p9fb3OmTNHFyxYoB0dHbphwwatrKz0vE47IxgM6p133qlHjx7V06dPaywW03379umHH36oe/bs0b179+qBAwf0scce87zWoY6qqipdunSpvvTSS3r48GFta2vTnp4e7ejo0Hg8rtu3b9elS5fq73//e33uuef0448/1h07dmg8Htfu7m6tra3VSZMmeZ7DaqxZs0aHKhqN6vr167W7u1sTiYTOmjXL8/oHG+Xl5YPm+uijj1RVNZlM6vLlyzUWi2kqlfK89qGMoTh+/Hj29pIlSzQej5/3HuvXMRRdXV3Z28lkUtPptOd1u5kvHo+rquo999yjyWRSVdXzut3MlzFnzhxNpVIFlS+dTmdvX3/99dl/e1232+tOVXXq1KkF99q70LrMYT0D92Ks7vTr8GKFnjlzxvMnVi7zeV1DLrN1dHR4Xgf57OeLRqOe15HLfIXcIFPVvG6uiPQ1kILBoAaDwezPTNNUVdXe3l4dNWqU5zU6GSNHjtTnnntOH3roIW1oaNDLL79cJ02apKqqiURCV6xY0S97Po1QKKS/+MUvtKurS1tbW7Wzs1NPnjypBw4c0GQyqc3Nzbpu3TpduHChhkIhz+sdytizZ48mEol+XwIy75OZn2X+u3//fl21apVu3bpVt23bprFYTJ999lktKSnxPMeFxttvv225sXwh6XRaU6mU7ty5U3t6evSBBx7wPMdAo7m5edj5Mnbt2qXJZNLzDFbj3KbQcDU3N/v2eZkZ7e3ttrIdPnxYVdXXDbJoNGp73R07dkxV/f09oru723a+EydO+D5fLBazne/UqVO+z9fb22s7X1tbm+f1DzYSiYTtbLlcd0oDyfYf7jxeP8lymc/reshHvoGykS+/RiFlM02z34Z/UVHRefmKioo8r9POMAxDq6qqtKGhQevr63XMmDE6c+bM8/L99re/9bxWO2PUqFG6aNEibW5u1mg0qt3d3dkN7UyTpaenR9944w3Pax3K+OSTT3Tfvn0XfP/o7u7Wzs7OC943evRove666/S1117TnTt3+nZG2bJlyy5Y/2ACgYDOnz9fm5qa9ODBg57nGGg8+OCDtvIZhqG33HKL7tixw/MMVuOb3/ymrXwiorNmzdLOzk7PM1iNr3zlK7byzZgxQ3fv3q29vb2eZxhojBkzxva6q6+v1//+97++bnAOZdbfQPkmTZqk+/bt8/VMllAoZDvf2LFjtbm52fMMViOz085Ovqqqquxtvw7DMGzlC4fDunv37pzlU4teTH4dcHyR6P83qgoS+fLbFyFfofoirLsL5cvH488zVDV7fhxV7XdCyozx48df7LJcodp3BY/m5mbZv3+/HD9+XN57773zfm/+/PkeVOdMeXm5jBo1Sg4ePChVVVXS2toqpaWl2ZOFZp6TJSUl8o1vfMPLUocsHA4P+FwrLS3Nnmvm86/BEydOyLFjx6SpqUkWL14ssVgs57Xa8de//tXWcslkUt555x2555575NNPP3W5Kvds3brV1nLpdFq2b98us2fPdrcglx06dMjWcqoqn3zyibz66qsuV+Sus2fP2lpu37590tDQIC+//LK7BfmAqsrRo0fl8ssvl6VLl3pdzoDsnmhYVeXUqVMydepUefbZZ12uyj12z42mqnLmzBmZPHmyyxW5y+5J9lX7rqC3ePFilytyl93nZzwel/r6elm4cKHLFQ2Ok2ifY6h/i3z9MlTI+YbzPCaf/xTyc1OEfBn5mK+rq2vIV2DJt3yGYcgrr7wid99995B/Px+UlZXJE088Iddee63U1NTIl7/85UGXyYdsP/nJT+RXv/qV7S8L8XhcNmzYIIsWLZI9e/a4XJ074vG47Xx333237Ny5U1paWqSrq8vlytwxlBO/DqS2tlaOHDni2WWbh8LJd4qKigrp6ury9Qnt7eYrLi4W0zSlt7fXtzuRnNQVCAQkEAhIIpFwsSJ3OcmXuZreuVf88hsn+fLh88/p+vPr6y7Dbn25XHdqcRJtZiBJ3x7I4ay4wc7a7zfFxcUF3YAYbmc6n/IVFRX5/k3PiWAwSL48FggECjafYRgSDAbz8vK9QxEKhaStrW3IzaN88tprr8l1110nN9xwg9TV1Q1pmXy5ApTT19vYsWPlhRde8G3e48eP2874l7/8Rb761a/KJZdc4nJV7mlvb7e9bEtLS78rmvmR3Vk6IiITJ07My8uJD0Vvb6+MHz/e11fRc9L8SaVSMmbMGF9/P3LS/Emn0zJq1Cjfvm+KiKPGq6r6/ruRk8++SCTiYiX+oqr9rgR5sTADSew9Kc99oQUCgYLrSvv9jeRcTvJlLoPsV0470oWY74vy3MwHhZzPMAxJpVLDqjeRSNiean2xDeUSxp/3wx/+0NeHKYj0rbdoNCqlpaVimuaQ19+ECRPkyJEjOa7OmYqKCnn//felrq7O9heZjo4O2bt3r8ydO1dOnjzpcoXOHT161HEDaPr06bJz506XKnJXZ2enoyZQVVWVnDlzxsWK3OVkBplI3w7d3t5eFytyl9PtqaKiIt/OIHMyOy4jEAj4dgaZG9vCfp7J4rQuv2+bkc9aLvIxA8mCG28EhdY8yidO8/H38Y6fa3MD+fKbnY1pP+9dPld9fb2tPcXPP/98Dqpx12WXXSbl5eUSCASGtf5uu+22HFbljs7OTlm5cqXtL2ixWEwqKipEVeXee+91uTp3jBs3Tnbv3u3oMWbOnOlSNe6LRCLy4Ycf2l5+KIdjeslpA93v55N79913HS1fVVXlTiE5YJqm/OMf/3D0GH6esWsYhqxfv97RY/h5hpzTfH7ffnHaIPHz7DERkVWrVjla/mI3yL7QM5CcZm9sbJQdO3a4UUpOOM3n9z2yfuzWuslJvnA4LD09PS5W4z4n+UKhUMEeay/i7714Is7z+XkvnoizfGfPnvX9dGkne5oNw/D1zMaxY8fKkSNHsnUOVTKZlEgkIrFYTEKhkCSTSV++BkOhkLz55psya9asYU9bT6fT0tzcLCUlJbJu3Tq5//77c1SlM9OnT5ePP/7Y0WP4+fO9pqZGWltbbS8/btw4OXbsmIsVucvpe8Po0aPl1KlTLlXjruLiYscnoa+urpaOjg6XKnJXMBh0vG1VUVEhnZ2dLlXkLjfOY+Tn9xbDMBx9bhX6DMBC/+7g9gxHZiDlyDXXXON1CTl17bXXel0CbLrqqqu8LiGnpk+f7nUJOXXFFVd4XUJOXX311V6XkDN+P0fJcGfmfN68efN82zwS6ZuhMZxD1zICgYDMnz9fxo4dK4lEwpfNI5G+Q4Qef/xx+eSTT4a1XCqVkpaWFpkyZYoEAgG56aabclShc8eOHZMVK1Z4XUbOJBIJR1cc279/v4vVuO/FF190tLyfm2PBYFCWLFni6DGcnAcr14LBoPz0pz919BjRaNSlatwXDAblwQcf9LqMnAkEAvLd737X9vJ+vUJnxrx58xwtf6Gr6PqFYRgya9YsR49xMZtjzEByyM+daPJZ83M2EfINxs/5ONZ+cH6eZeU0XzAYLNhDmz/66COZMWOGb9ediL18O3fulB/96EeyceNGX2cT6Xvvmz17tqxatUpGjBgxpGXeeustmTlzppSXl4uqSnt7u8yZM0e2bduW42qHzzRNmTZtmtx4443y3HPP2XoMP38+GIYhdXV1smvXLlvLq6qvD8eYPHmyXHrppY4O9/Lz+rv00kulsrJSmpqabD+Gn/PV1NRIaWmptLS02H4MP+errq6WYDAoJ06csLW8n7OJ9B0m66SJ5+d8mVm33d3dth/Dz/kyhxE6aXS5mY8ZSDkUDAa9LiGn/H5MrBP5crJbu/x8rLYbCn39+flqJk4aEJll/fwhbreBkEqlJJVK+bbxlzHcvVRtbW1y6tQpOX36tJw+fdr3+aLRqMTj8SFP5X7xxRflwQcflEgk4uvXXYaqypYtW+SSSy6RUCg06KH0ixYtkptuuknKyspEVSWRSEhZWZkvm0cifa+/999/39E5t+bOnetiRe5SVdm1a5eUlZXZWt4wDF/PEN+/f79s2rRJxowZY/sx6uvrXazIXYcOHZL//Oc/cuWVV9p+jIkTJ7pYkbtaW1vl4MGDcvvtt3tdSk60t7c7uoBARUWFi9W4LxqNyrJly2wv78UVvYaqu7tbenp65M0337T9GH7+7hCPxyWRSMjmzZttP8bF6kt8oRtIbnyB8evVFETcyefnY0Wd5vPzVEYR5/n8fByzG/y8/gr9vWW4e7/PvbRz5m/j5xk6AzURzm0spdNpUVVRVdm4caOoqgQCAV/PrMoYaAPq3PNW3H333fLYY4/JypUrZd68ebJ161ZJpVKyY8cO3zeQnnzySRHp+6J+oXMZJZNJqa2tlUmTJklNTY08+uijsm3bNvnb3/7m+2wifa+hyy+/XEzTFNM0pbGxURobG+Wdd945L6uqyuzZs6WzszP7nhkKheTQoUNelD5kLS0t8uGHH0plZaWt5W+99VaXK3JXT0+PfPDBB3LppZfaWt5J8yLX0um0pNNp2bdvnzQ2Ntp6DKdX4ss1VZVt27bJt771LVvL+/lLukhfvtdff923J9t3SlXlqaeesrWsn7fNMn7wgx/I8uXLbS3r93yqKrfddpu88cYbtpbPh3xf//rXZcOGDbaWv1j5vtCHsGVc6G9gmqaMHj160BMd+nkvesZA+SKRyKCXg83XfIZhSGlpqeU0x3zNJiJDOoltvuYzDGNIJ3LM53yBQGDQN/l8yJdKpbLNpHQ6LalUSoqLiyUQCEg8HrfMkA/5Dhw4ILW1tZJKpSQajUpHR4dMmTJFQqGQxGKxbANJRLINJJH8yNbY2CgfffSRiIjs27dPNm/eLM3NzfLrX/9a/vznP8vMmTNl27ZtMmbMmOylxxsaGuSPf/yjPPLIIx5XP7iysjLZuHGjrF27Vr797W/LyZMn5ec//7mEQiGpra2V5ubm7J7oYDAo8XhcSktLpb293fcNwNLSUqmpqZGGhgbZu3evHD16VLq7u6W2tlZeffVVMQxDZsyYIW1tbVJTUyNnz56VsrKybC5VlUceeUSeeeYZj5MM7Prrr5eXXnpJRo8eLVVVVZJOp4c1q3bNmjVyxx135LBCZ+rq6uT111+X+vp6W+8X+/fvl6lTp+agMufGjBkjx48fl2g0anu2xokTJxzNYMqlzCFCyWTS9t7+zs5O385kyZwoXFUdXWzBr5yeKDwej/t6hr/TE4Unk0nfHn3i9CThIn3bqn6eaey0L+NmPqtD2LIbwPk0RERzMVRVDcPQ6upqFRE1TVOt5KqOXI+ysjIVETUMo2DynVtvcXFxQecrKirK/qwQsn0+X6Gtu3PzmaZZkPn+r6mfzWeapqbTaU2lUudlS6fTntc7nNHb23tevkAgoB0dHZpMJjWVSmkqldJ0Oq2qqqdPn/a85uGMBQsWqGmaahiGBgIBFRFdvXq1Ll68WHfu3KnLly/XlStX6p49e/TkyZOe1zucYRiGjhs3Tq+66iqtq6vT+vp6veqqq/T+++/XmpoaHTdunIbDYQ2Hw1pUVJRdz34f48aN03HjxunDDz+s8+bN65fXMAy94oor9Oabb9bHH39cT506pS0tLdrS0qIHDx7UJUuW6MKFC7PbOX4dM2bM0CNHjmgwGNRLLrlEV65cqePHj7d838yYMWOG5/UPNsrKynTatGkaCAT0jjvu0FQqpbNnzx40m9d1D2WYpqmpVEqnTZumpmnqsmXLVFX1gQceKIh8In2f6Y2NjWoYhu7evVv/9Kc/Dem56XXdw8k3bdq07LbKli1bCi6fiGg8HldV1ebm5oLK19DQoCKS3S5pb28vmGyqqlOnTs3eVlXt7u4uqHwTJ07M3k4kEp49N9WqF2N1p1+HFyszX5+IX/R8F/OFRr6Lk8/rmshHPhHJfqh/vknmdV1OxrmNsqamJj1y5IiuX79ejx07pm1tbdrT0+N5jXZGOBzWuro6HT9+vBYXF2t5ebk2NDTohAkTNBKJ6MiRIzUcDnte51DH888/r6+++qrecMMNunr1an3ooYf6rUPTNNU0TQ2FQjphwgQNhUJqmqYWFxdrKBTSUCjkeYahjPb2dt20aZOuXLlSJ0+erMeOHdPVq1drfX199nW3du1a/de//qWrV6/WG264QUUk2wj1+0ilUnrmzBlta2vT73znO6qqmkwm+72fLF++XDdt2qR33XVX3qw3kf9/708mk5pIJPT1119XVdUlS5b0y/fyyy/rli1bdMqUKZ7XbCff57311lvnrb/MTtt8Gufq6enJ3m5qaup337///W8NBoOe1+sk37mf4YcPH87efu2117SpqSlvdiwMlG8ga9eu1d27d3teq1vZurq6+v377bff1gMHDnher1v5ent7+/1769atF6MWGkhurVCvn1jks5+NfPk5vij5vK4jFyOz9zKdTmdn7xTKCAaDGovFNJlMajKZzKsGxGAjFAppJBLRzZs36+HDh7W1tVWnTZvmeV1ORzgc1hEjRmh1dbXW1NRoY2OjTpw4USsrKz2vbbijrKxMw+FwtllUUVGh5eXlntfl5nj44Yf11ltv1XA4rE8//bTecsst/b7MZWaXzZkzR8vLy7WxsdHzmoczYrGYrl+/XisqKvT06dP6wQcfaElJSfb+6dOn6y9/+UtdvHixhkIhra2t9bzmoY7a2lpNp9N66tQprays1FQqpZ2dnf1+5+abb9Z//vOfumzZsrz7fKioqFDVvtkB1dXVmkqldP78+f1+Z/78+Xr06FEVkbxq/on0zXTPfHZnrFixot/vPProo/ruu++qSP40bTMjc4RJJBLJ5vv000+z9z/zzDOaTCZ1+/btKiJ510TKKCsry97u6OjI3r9y5UpNp9O6Z88ez2t1kq+kpOSC29CbNm1S1b6GoNe1OskXCoWyr8Nz79+1a9dF+d6gFr0YzoEEAAC+EILBoKRSKamoqJBkMildXV1el4QB/P3vf5c5c+Z4XUbOtLa2+vY8P061trZKdXW1r6945MSZM2ckEokM+4IS+aKrq0vC4bCvz2XkRCwWk5KSEq/LyBlV++ev8rtkMimBQKBg82XOLeqHfGpxDiQaSAAAAAAAALBsIBVm6xwAAAAAAACuoYEEAAAAAAAASzSQAAAAAAAAYIkGEgAAAAAAACzRQAIAAAAAAIAlGkgAAAAAAACwRAMJAAAAAAAAlmggAQAAAAAAwBINJAAAAAAAAFiigQQAAAAAAABLNJAAAAAAAABgiQYSAAAAAAAALNFAAgAAAAAAgCUaSAAAAAAAALBEAwkAAAAAAACWaCABAAAAAADAEg0kAAAAAAAAWDJU1esaAAAAAAAA4GPMQAIAAAAAAIAlGkgAAAAAAACwRAMJAAAAAAAAlmggAQAAAAAAwBINJAAAAAAAAFiigQQAAAAAAABLNJAAAAAAAABgiQYSAAAAAAAALNFAAgAAAAAAgCUaSAAAAAAAALBEAwkAAAAAAACWaCABAAAAAADAEg0kAAAAAAAAWKKBBAAAAAAAAEs0kAAAAAAAAGCJBhIAAAAAAAAs0UACAAAAAACAJRpIAAAAAAAAsEQDCQAAAAAAAJZoIAEAAAAAAMASDSQAAAAAAABYooEEAAAAAAAASzSQAAAAAAAAYIkGEgAAAAAAACzRQAIAAAAAAIAlGkgAAAAAAACwRAMJAAAAAAAAlmggAQAAAAAAwBINJAAAAAAAAFiigQQAAAAAAABLNJAAAAAAAABgiQYSAAAAAAAALNFAAgAAAAAAgCUaSAAAAAAAALBEAwkAAAAAAACWaCABAAAAAADAEg0kAAAAAAAAWKKBBAAAAAAAAEv/C7+Wlo0EMD2UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, latent_dim):\n",
    "    plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num = i,\n",
    "                 z_m_m = z_m_m ,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
