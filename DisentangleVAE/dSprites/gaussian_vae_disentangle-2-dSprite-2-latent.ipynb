{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7f6a913d1d68>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2376x2376 with 121 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz'\n",
    "                      , encoding='bytes')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]\n",
    "\n",
    "# Define number of values per latents and functions to convert to indices\n",
    "latents_sizes =  np.array([ 1,  3,  6, 40, 32, 32])\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "  return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "# Helper function to show images\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "  ncols = int(np.ceil(num_images**0.5))\n",
    "  nrows = int(np.ceil(num_images / ncols))\n",
    "  _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for ax_i, ax in enumerate(axes):\n",
    "    if ax_i < num_images:\n",
    "      ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "    else:\n",
    "      ax.axis('off')\n",
    "\n",
    "def show_density(imgs):\n",
    "  _, ax = plt.subplots()\n",
    "  ax.imshow(imgs.mean(axis=0), interpolation='nearest', cmap='Greys_r')\n",
    "  ax.grid('off')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "from copy import deepcopy\n",
    "## Fix posX latent to left\n",
    "#latents_sampled = sample_latent(size=5000)\n",
    "latents_sampled = deepcopy(latents_classes)\n",
    "latents_sampled[:, [4,5]] = 15\n",
    "latents_sampled[:,2]= 5\n",
    "\n",
    "\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[np.unique(indices_sampled)]\n",
    "\n",
    "#np.unique(indices_sampled)\n",
    "\n",
    "# Samples\n",
    "show_images_grid(imgs_sampled,len(np.unique(indices_sampled)))\n",
    "\n",
    "# Show the density too to check\n",
    "show_density(imgs_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(69)\n",
    "ff = imgs_sampled\n",
    "n_data =  ff.shape[0]\n",
    "n_train = int(np.ceil(n_data*0.8))\n",
    "\n",
    "print(n_train)\n",
    "idx_train = random.sample(range(n_data), n_train)\n",
    "idx_test = np.delete(range(n_data),idx_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   6,  14,  15,  20,  22,  23,  24,  28,  32,  43,  46,\n",
       "        51,  55,  67,  85,  92,  93,  96, 104, 105, 110, 118])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape[0]*0.8\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train / validation folds\n",
    "#np.random.seed(42)\n",
    "\n",
    "img_rows = ff.shape[1]\n",
    "img_cols = ff.shape[2]\n",
    "\n",
    "n_pixels = img_rows * img_cols\n",
    "x_train = ff[idx_train]\n",
    "x_test = ff[idx_test]\n",
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train.reshape((len(x_train), n_pixels))\n",
    "x_test = x_test.reshape((len(x_test), n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4608x4608 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_examples(data, n=None, n_cols=20, thumbnail_cb=None):\n",
    "    if n is None:\n",
    "        n = len(data)    \n",
    "    n_rows = int(np.ceil(n / float(n_cols)))\n",
    "    figure = np.zeros((img_rows * n_rows, img_cols * n_cols))\n",
    "    for k, x in enumerate(data[:n]):\n",
    "        r = k // n_cols\n",
    "        c = k % n_cols\n",
    "        figure[r * img_rows: (r + 1) * img_rows,\n",
    "               c * img_cols: (c + 1) * img_cols] = x\n",
    "        if thumbnail_cb is not None:\n",
    "            thumbnail_cb(locals())\n",
    "        \n",
    "    plt.figure(figsize=(64, 64))\n",
    "    plt.imshow(figure)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "show_examples(ff, n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "#     return z_mean + K.exp(0.5 ) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num,\n",
    "                 z_m_m, \n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_face\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    #x_test = data\n",
    "    latent_dim = latent_dim\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(model_name, \"face_over_latent.png\")\n",
    "    n = 20\n",
    "    #digit_size = 28\n",
    "    img_rows, img_cols = 64, 64\n",
    "    figure = np.zeros((img_rows , img_cols * n))\n",
    "    grid_x = np.linspace(-5, 5, n)\n",
    "    #grid_y = np.linspace(-5, 5, n)[::-1]\n",
    "    z_sample = np.zeros((1,latent_dim))\n",
    "    z_sample[0,:] = z_m_m \n",
    "    \n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample[0,latent_num] = xi\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_rows, img_cols)\n",
    "        figure[0: img_rows,j * img_cols: (j + 1) * img_cols] = digit\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #start_range = digit_size // 2\n",
    "    #end_range = n * digit_size + start_range + 1\n",
    "    #pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    #sample_range_x = np.round(grid_x, 1)\n",
    "    #sample_range_y = np.round(grid_y, 1)\n",
    "    #plt.xticks(pixel_range, sample_range_x)\n",
    "    #plt.yticks(pixel_range, sample_range_y)\n",
    "    #plt.xlabel(\"z[0]\")\n",
    "    #plt.ylabel(\"z[1]\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "original_dim = n_pixels\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim1 = 512\n",
    "intermediate_dim2 = 256\n",
    "intermediate_dim3 = 64\n",
    "\n",
    "batch_size = 20\n",
    "latent_dim = 2\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x1)\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(x2)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x3)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the mean of z, so that mean(m_z)=0 and cov(m_z)=I\n",
    "def standardize(z_mean):\n",
    "    z_m_m = K.mean(z_mean,axis=0, keepdims=True)\n",
    "    z1 = z_mean - z_m_m\n",
    "    n = tf.cast(K.shape(z_mean)[0], tf.float32)\n",
    "    cov = K.transpose(z1) @ z1 /n\n",
    "    \n",
    "    D = tf.diag(tf.diag_part(cov)) ** 0.5\n",
    "    \n",
    "    L = tf.linalg.inv(tf.transpose(tf.cholesky(cov)))\n",
    "       \n",
    "#     z2 = z1 @ L @ D +  z_m_m\n",
    "    z2 = z1 @ L  +  z_m_m\n",
    "\n",
    "    return( z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_std = Lambda(standardize, output_shape=(latent_dim,), name='z_mean_std')(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            130         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            130         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean_std (Lambda)             (None, 2)            0           z_mean[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean_std[0][0]                 \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,245,700\n",
      "Trainable params: 2,245,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "#z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_std, z_log_var])\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 2,249,664\n",
      "Trainable params: 2,249,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z')\n",
    "\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(latent_inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x3)\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(x2)\n",
    "\n",
    "# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x1)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 2245700   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 4096)              2249664   \n",
      "=================================================================\n",
      "Total params: 4,495,364\n",
      "Trainable params: 4,495,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test )\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "\n",
    "#     reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(1E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/1000\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 2734.7971 - val_loss: 2154.1434\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2154.14335, saving model to weights.hdf5\n",
      "Epoch 2/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 1591.5355 - val_loss: 550.0900\n",
      "\n",
      "Epoch 00002: val_loss improved from 2154.14335 to 550.09000, saving model to weights.hdf5\n",
      "Epoch 3/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 449.6288 - val_loss: 357.4591\n",
      "\n",
      "Epoch 00003: val_loss improved from 550.09000 to 357.45908, saving model to weights.hdf5\n",
      "Epoch 4/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 314.0213 - val_loss: 282.4471\n",
      "\n",
      "Epoch 00004: val_loss improved from 357.45908 to 282.44713, saving model to weights.hdf5\n",
      "Epoch 5/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 257.0769 - val_loss: 248.0404\n",
      "\n",
      "Epoch 00005: val_loss improved from 282.44713 to 248.04038, saving model to weights.hdf5\n",
      "Epoch 6/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 232.6778 - val_loss: 239.5937\n",
      "\n",
      "Epoch 00006: val_loss improved from 248.04038 to 239.59365, saving model to weights.hdf5\n",
      "Epoch 7/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 223.8422 - val_loss: 226.3130\n",
      "\n",
      "Epoch 00007: val_loss improved from 239.59365 to 226.31299, saving model to weights.hdf5\n",
      "Epoch 8/1000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 218.0868 - val_loss: 225.5100\n",
      "\n",
      "Epoch 00008: val_loss improved from 226.31299 to 225.50999, saving model to weights.hdf5\n",
      "Epoch 9/1000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 217.9090 - val_loss: 230.4843\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 225.50999\n",
      "Epoch 10/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 216.7403 - val_loss: 225.1925\n",
      "\n",
      "Epoch 00010: val_loss improved from 225.50999 to 225.19255, saving model to weights.hdf5\n",
      "Epoch 11/1000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 214.0247 - val_loss: 222.8765\n",
      "\n",
      "Epoch 00011: val_loss improved from 225.19255 to 222.87651, saving model to weights.hdf5\n",
      "Epoch 12/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 214.2348 - val_loss: 221.4688\n",
      "\n",
      "Epoch 00012: val_loss improved from 222.87651 to 221.46881, saving model to weights.hdf5\n",
      "Epoch 13/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 211.0299 - val_loss: 220.5630\n",
      "\n",
      "Epoch 00013: val_loss improved from 221.46881 to 220.56304, saving model to weights.hdf5\n",
      "Epoch 14/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 209.1497 - val_loss: 223.1178\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 220.56304\n",
      "Epoch 15/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 207.0872 - val_loss: 222.0718\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 220.56304\n",
      "Epoch 16/1000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 205.8063 - val_loss: 216.3568\n",
      "\n",
      "Epoch 00016: val_loss improved from 220.56304 to 216.35680, saving model to weights.hdf5\n",
      "Epoch 17/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 203.7419 - val_loss: 214.1953\n",
      "\n",
      "Epoch 00017: val_loss improved from 216.35680 to 214.19535, saving model to weights.hdf5\n",
      "Epoch 18/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 201.2397 - val_loss: 209.7091\n",
      "\n",
      "Epoch 00018: val_loss improved from 214.19535 to 209.70912, saving model to weights.hdf5\n",
      "Epoch 19/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 197.8938 - val_loss: 205.9258\n",
      "\n",
      "Epoch 00019: val_loss improved from 209.70912 to 205.92576, saving model to weights.hdf5\n",
      "Epoch 20/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 195.3687 - val_loss: 204.3628\n",
      "\n",
      "Epoch 00020: val_loss improved from 205.92576 to 204.36275, saving model to weights.hdf5\n",
      "Epoch 21/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 192.9425 - val_loss: 200.6446\n",
      "\n",
      "Epoch 00021: val_loss improved from 204.36275 to 200.64458, saving model to weights.hdf5\n",
      "Epoch 22/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 190.5362 - val_loss: 198.2016\n",
      "\n",
      "Epoch 00022: val_loss improved from 200.64458 to 198.20158, saving model to weights.hdf5\n",
      "Epoch 23/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 188.4504 - val_loss: 199.6976\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 198.20158\n",
      "Epoch 24/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 186.3663 - val_loss: 199.1708\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 198.20158\n",
      "Epoch 25/1000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 185.8893 - val_loss: 198.6022\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 198.20158\n",
      "Epoch 26/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 184.9234 - val_loss: 201.7056\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 198.20158\n",
      "Epoch 27/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 182.0423 - val_loss: 198.6526\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 198.20158\n",
      "Epoch 28/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 178.3316 - val_loss: 199.5084\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 198.20158\n",
      "Epoch 29/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 177.5756 - val_loss: 197.1967\n",
      "\n",
      "Epoch 00029: val_loss improved from 198.20158 to 197.19666, saving model to weights.hdf5\n",
      "Epoch 30/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 172.8325 - val_loss: 196.6463\n",
      "\n",
      "Epoch 00030: val_loss improved from 197.19666 to 196.64634, saving model to weights.hdf5\n",
      "Epoch 31/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 172.4126 - val_loss: 194.3367\n",
      "\n",
      "Epoch 00031: val_loss improved from 196.64634 to 194.33674, saving model to weights.hdf5\n",
      "Epoch 32/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 171.9640 - val_loss: 194.4688\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 194.33674\n",
      "Epoch 33/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 170.2328 - val_loss: 197.0191\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 194.33674\n",
      "Epoch 34/1000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 172.7246 - val_loss: 192.7734\n",
      "\n",
      "Epoch 00034: val_loss improved from 194.33674 to 192.77338, saving model to weights.hdf5\n",
      "Epoch 35/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 170.6209 - val_loss: 187.8808\n",
      "\n",
      "Epoch 00035: val_loss improved from 192.77338 to 187.88076, saving model to weights.hdf5\n",
      "Epoch 36/1000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 173.4777 - val_loss: 188.7488\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 187.88076\n",
      "Epoch 37/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 164.2196 - val_loss: 198.4716\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 187.88076\n",
      "Epoch 38/1000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 167.7735 - val_loss: 186.2178\n",
      "\n",
      "Epoch 00038: val_loss improved from 187.88076 to 186.21781, saving model to weights.hdf5\n",
      "Epoch 39/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 161.2497 - val_loss: 185.4642\n",
      "\n",
      "Epoch 00039: val_loss improved from 186.21781 to 185.46419, saving model to weights.hdf5\n",
      "Epoch 40/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 162.2399 - val_loss: 192.9434\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 185.46419\n",
      "Epoch 41/1000\n",
      "96/96 [==============================] - 0s 584us/step - loss: 161.0784 - val_loss: 193.7502\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 185.46419\n",
      "Epoch 42/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 162.3917 - val_loss: 194.0052\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 185.46419\n",
      "Epoch 43/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 159.6485 - val_loss: 185.8787\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 185.46419\n",
      "Epoch 44/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 157.4333 - val_loss: 191.8397\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 185.46419\n",
      "Epoch 45/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 157.0025 - val_loss: 190.5708\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 185.46419\n",
      "Epoch 46/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 626us/step - loss: 157.1230 - val_loss: 182.6076\n",
      "\n",
      "Epoch 00046: val_loss improved from 185.46419 to 182.60764, saving model to weights.hdf5\n",
      "Epoch 47/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 157.2992 - val_loss: 186.0230\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 182.60764\n",
      "Epoch 48/1000\n",
      "96/96 [==============================] - 0s 591us/step - loss: 155.4708 - val_loss: 186.6545\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 182.60764\n",
      "Epoch 49/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 159.3916 - val_loss: 186.0630\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 182.60764\n",
      "Epoch 50/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 156.2470 - val_loss: 190.3699\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 182.60764\n",
      "Epoch 51/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 156.7370 - val_loss: 191.0281\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 182.60764\n",
      "Epoch 52/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 156.4625 - val_loss: 191.1460\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 182.60764\n",
      "Epoch 53/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 154.0347 - val_loss: 191.8984\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 182.60764\n",
      "Epoch 54/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 155.6442 - val_loss: 193.1504\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 182.60764\n",
      "Epoch 55/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 157.9595 - val_loss: 183.5888\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 182.60764\n",
      "Epoch 56/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 155.2129 - val_loss: 185.8515\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 182.60764\n",
      "Epoch 57/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 154.0049 - val_loss: 190.2532\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 182.60764\n",
      "Epoch 58/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 154.3250 - val_loss: 183.8854\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 182.60764\n",
      "Epoch 59/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 156.5751 - val_loss: 186.4684\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 182.60764\n",
      "Epoch 60/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 155.1515 - val_loss: 182.3454\n",
      "\n",
      "Epoch 00060: val_loss improved from 182.60764 to 182.34543, saving model to weights.hdf5\n",
      "Epoch 61/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 153.3804 - val_loss: 189.6920\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 182.34543\n",
      "Epoch 62/1000\n",
      "96/96 [==============================] - 0s 587us/step - loss: 154.9097 - val_loss: 186.8416\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 182.34543\n",
      "Epoch 63/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 154.6589 - val_loss: 185.0238\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 182.34543\n",
      "Epoch 64/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 156.1618 - val_loss: 192.0700\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 182.34543\n",
      "Epoch 65/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 151.9449 - val_loss: 188.2337\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 182.34543\n",
      "Epoch 66/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 152.2117 - val_loss: 186.3245\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 182.34543\n",
      "Epoch 67/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 154.2260 - val_loss: 183.9330\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 182.34543\n",
      "Epoch 68/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 151.4991 - val_loss: 179.0587\n",
      "\n",
      "Epoch 00068: val_loss improved from 182.34543 to 179.05874, saving model to weights.hdf5\n",
      "Epoch 69/1000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 153.4110 - val_loss: 182.3493\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 179.05874\n",
      "Epoch 70/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 150.6823 - val_loss: 181.0829\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 179.05874\n",
      "Epoch 71/1000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 150.0618 - val_loss: 192.1350\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 179.05874\n",
      "Epoch 72/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 156.5264 - val_loss: 181.7933\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 179.05874\n",
      "Epoch 73/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 150.8873 - val_loss: 181.8632\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 179.05874\n",
      "Epoch 74/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 150.4114 - val_loss: 195.4223\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 179.05874\n",
      "Epoch 75/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 152.6550 - val_loss: 177.0709\n",
      "\n",
      "Epoch 00075: val_loss improved from 179.05874 to 177.07085, saving model to weights.hdf5\n",
      "Epoch 76/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 149.1712 - val_loss: 183.1019\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 177.07085\n",
      "Epoch 77/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 150.5889 - val_loss: 183.4070\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 177.07085\n",
      "Epoch 78/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 150.5535 - val_loss: 184.8307\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 177.07085\n",
      "Epoch 79/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 149.8289 - val_loss: 187.9525\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 177.07085\n",
      "Epoch 80/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 150.2388 - val_loss: 186.3758\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 177.07085\n",
      "Epoch 81/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 146.7423 - val_loss: 182.0129\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 177.07085\n",
      "Epoch 82/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 148.1563 - val_loss: 183.6457\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 177.07085\n",
      "Epoch 83/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 146.6023 - val_loss: 188.0641\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 177.07085\n",
      "Epoch 84/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 146.5081 - val_loss: 184.0957\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 177.07085\n",
      "Epoch 85/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 149.2761 - val_loss: 181.3508\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 177.07085\n",
      "Epoch 86/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 146.0862 - val_loss: 182.3857\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 177.07085\n",
      "Epoch 87/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 148.9042 - val_loss: 178.9113\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 177.07085\n",
      "Epoch 88/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 149.1358 - val_loss: 192.6505\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 177.07085\n",
      "Epoch 89/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 146.1136 - val_loss: 189.4461\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 177.07085\n",
      "Epoch 90/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 150.9468 - val_loss: 183.7635\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 177.07085\n",
      "Epoch 91/1000\n",
      "96/96 [==============================] - 0s 597us/step - loss: 151.1874 - val_loss: 190.8693\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 177.07085\n",
      "Epoch 92/1000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 145.2722 - val_loss: 179.2385\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 177.07085\n",
      "Epoch 93/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 146.5634 - val_loss: 177.5323\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 177.07085\n",
      "Epoch 94/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 150.5142 - val_loss: 184.6056\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 177.07085\n",
      "Epoch 95/1000\n",
      "96/96 [==============================] - 0s 650us/step - loss: 151.0975 - val_loss: 174.3369\n",
      "\n",
      "Epoch 00095: val_loss improved from 177.07085 to 174.33687, saving model to weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 143.7767 - val_loss: 181.9436\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 174.33687\n",
      "Epoch 97/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 147.2806 - val_loss: 178.6864\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 174.33687\n",
      "Epoch 98/1000\n",
      "96/96 [==============================] - 0s 599us/step - loss: 145.3598 - val_loss: 176.7509\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 174.33687\n",
      "Epoch 99/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 142.6564 - val_loss: 181.0430\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 174.33687\n",
      "Epoch 100/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 144.3429 - val_loss: 176.7386\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 174.33687\n",
      "Epoch 101/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 141.7639 - val_loss: 179.8559\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 174.33687\n",
      "Epoch 102/1000\n",
      "96/96 [==============================] - 0s 662us/step - loss: 141.3243 - val_loss: 177.3490\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 174.33687\n",
      "Epoch 103/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 141.8324 - val_loss: 181.5658\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 174.33687\n",
      "Epoch 104/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 144.0928 - val_loss: 176.3930\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 174.33687\n",
      "Epoch 105/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 140.8160 - val_loss: 177.3511\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 174.33687\n",
      "Epoch 106/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 138.7444 - val_loss: 172.3584\n",
      "\n",
      "Epoch 00106: val_loss improved from 174.33687 to 172.35839, saving model to weights.hdf5\n",
      "Epoch 107/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 139.1839 - val_loss: 168.7532\n",
      "\n",
      "Epoch 00107: val_loss improved from 172.35839 to 168.75323, saving model to weights.hdf5\n",
      "Epoch 108/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 140.5362 - val_loss: 164.1613\n",
      "\n",
      "Epoch 00108: val_loss improved from 168.75323 to 164.16128, saving model to weights.hdf5\n",
      "Epoch 109/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 137.0394 - val_loss: 174.3187\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 164.16128\n",
      "Epoch 110/1000\n",
      "96/96 [==============================] - 0s 580us/step - loss: 137.8847 - val_loss: 166.1919\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 164.16128\n",
      "Epoch 111/1000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 141.9209 - val_loss: 159.1443\n",
      "\n",
      "Epoch 00111: val_loss improved from 164.16128 to 159.14431, saving model to weights.hdf5\n",
      "Epoch 112/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 143.1850 - val_loss: 166.3012\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 159.14431\n",
      "Epoch 113/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 137.8071 - val_loss: 167.9324\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 159.14431\n",
      "Epoch 114/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 139.5315 - val_loss: 174.1494\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 159.14431\n",
      "Epoch 115/1000\n",
      "96/96 [==============================] - 0s 651us/step - loss: 135.9062 - val_loss: 168.1037\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 159.14431\n",
      "Epoch 116/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 135.3318 - val_loss: 167.6894\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 159.14431\n",
      "Epoch 117/1000\n",
      "96/96 [==============================] - 0s 599us/step - loss: 141.1403 - val_loss: 156.4015\n",
      "\n",
      "Epoch 00117: val_loss improved from 159.14431 to 156.40149, saving model to weights.hdf5\n",
      "Epoch 118/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 135.5460 - val_loss: 165.5144\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 156.40149\n",
      "Epoch 119/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 136.6685 - val_loss: 162.9875\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 156.40149\n",
      "Epoch 120/1000\n",
      "96/96 [==============================] - 0s 595us/step - loss: 134.7010 - val_loss: 154.2789\n",
      "\n",
      "Epoch 00120: val_loss improved from 156.40149 to 154.27887, saving model to weights.hdf5\n",
      "Epoch 121/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 130.8879 - val_loss: 155.3656\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 154.27887\n",
      "Epoch 122/1000\n",
      "96/96 [==============================] - 0s 584us/step - loss: 130.3604 - val_loss: 148.9891\n",
      "\n",
      "Epoch 00122: val_loss improved from 154.27887 to 148.98910, saving model to weights.hdf5\n",
      "Epoch 123/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 131.8427 - val_loss: 154.2230\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 148.98910\n",
      "Epoch 124/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 129.7808 - val_loss: 152.2461\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 148.98910\n",
      "Epoch 125/1000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 127.6528 - val_loss: 156.3378\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 148.98910\n",
      "Epoch 126/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 128.8877 - val_loss: 159.6849\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 148.98910\n",
      "Epoch 127/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 130.8438 - val_loss: 148.9173\n",
      "\n",
      "Epoch 00127: val_loss improved from 148.98910 to 148.91728, saving model to weights.hdf5\n",
      "Epoch 128/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 126.5103 - val_loss: 147.6085\n",
      "\n",
      "Epoch 00128: val_loss improved from 148.91728 to 147.60846, saving model to weights.hdf5\n",
      "Epoch 129/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 126.4322 - val_loss: 146.0381\n",
      "\n",
      "Epoch 00129: val_loss improved from 147.60846 to 146.03814, saving model to weights.hdf5\n",
      "Epoch 130/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 127.0007 - val_loss: 151.6751\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 146.03814\n",
      "Epoch 131/1000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 128.1157 - val_loss: 151.3959\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 146.03814\n",
      "Epoch 132/1000\n",
      "96/96 [==============================] - 0s 598us/step - loss: 122.0460 - val_loss: 147.5330\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 146.03814\n",
      "Epoch 133/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 127.5877 - val_loss: 149.3978\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 146.03814\n",
      "Epoch 134/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 120.9531 - val_loss: 160.4371\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 146.03814\n",
      "Epoch 135/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 125.0033 - val_loss: 148.6017\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 146.03814\n",
      "Epoch 136/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 123.9972 - val_loss: 154.5454\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 146.03814\n",
      "Epoch 137/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 125.6168 - val_loss: 144.8611\n",
      "\n",
      "Epoch 00137: val_loss improved from 146.03814 to 144.86111, saving model to weights.hdf5\n",
      "Epoch 138/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 120.3837 - val_loss: 148.4046\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 144.86111\n",
      "Epoch 139/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 135.1479 - val_loss: 154.1110\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 144.86111\n",
      "Epoch 140/1000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 122.7888 - val_loss: 147.0415\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 144.86111\n",
      "Epoch 141/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 123.6491 - val_loss: 152.7797\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 144.86111\n",
      "Epoch 142/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 123.1155 - val_loss: 154.5860\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 144.86111\n",
      "Epoch 143/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 120.3566 - val_loss: 141.6769\n",
      "\n",
      "Epoch 00143: val_loss improved from 144.86111 to 141.67689, saving model to weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 121.2668 - val_loss: 163.2228\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 141.67689\n",
      "Epoch 145/1000\n",
      "96/96 [==============================] - 0s 584us/step - loss: 122.6563 - val_loss: 150.4047\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 141.67689\n",
      "Epoch 146/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 121.1132 - val_loss: 144.4247\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 141.67689\n",
      "Epoch 147/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 128.0379 - val_loss: 155.4973\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 141.67689\n",
      "Epoch 148/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 119.4545 - val_loss: 141.9253\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 141.67689\n",
      "Epoch 149/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 120.8083 - val_loss: 154.2718\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 141.67689\n",
      "Epoch 150/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 123.3884 - val_loss: 137.6877\n",
      "\n",
      "Epoch 00150: val_loss improved from 141.67689 to 137.68772, saving model to weights.hdf5\n",
      "Epoch 151/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 118.0026 - val_loss: 143.1019\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 137.68772\n",
      "Epoch 152/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 121.3076 - val_loss: 143.4801\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 137.68772\n",
      "Epoch 153/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 115.1924 - val_loss: 145.3787\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 137.68772\n",
      "Epoch 154/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 112.4227 - val_loss: 143.7750\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 137.68772\n",
      "Epoch 155/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 112.6409 - val_loss: 153.3803\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 137.68772\n",
      "Epoch 156/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 112.8168 - val_loss: 141.5794\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 137.68772\n",
      "Epoch 157/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 113.2869 - val_loss: 140.3992\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 137.68772\n",
      "Epoch 158/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 112.4341 - val_loss: 147.4534\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 137.68772\n",
      "Epoch 159/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 109.2582 - val_loss: 136.0888\n",
      "\n",
      "Epoch 00159: val_loss improved from 137.68772 to 136.08881, saving model to weights.hdf5\n",
      "Epoch 160/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 109.9866 - val_loss: 145.2788\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 136.08881\n",
      "Epoch 161/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 104.0804 - val_loss: 143.4068\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 136.08881\n",
      "Epoch 162/1000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 108.6258 - val_loss: 143.6097\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 136.08881\n",
      "Epoch 163/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 107.0909 - val_loss: 154.2921\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 136.08881\n",
      "Epoch 164/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 113.9967 - val_loss: 145.2092\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 136.08881\n",
      "Epoch 165/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 106.5499 - val_loss: 148.9315\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 136.08881\n",
      "Epoch 166/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 125.2702 - val_loss: 155.0362\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 136.08881\n",
      "Epoch 167/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 126.5888 - val_loss: 150.8159\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 136.08881\n",
      "Epoch 168/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 115.0106 - val_loss: 153.9371\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 136.08881\n",
      "Epoch 169/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 108.3958 - val_loss: 145.9679\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 136.08881\n",
      "Epoch 170/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 106.4516 - val_loss: 145.9472\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 136.08881\n",
      "Epoch 171/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 105.1684 - val_loss: 143.6750\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 136.08881\n",
      "Epoch 172/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 106.8384 - val_loss: 141.6042\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 136.08881\n",
      "Epoch 173/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 100.4016 - val_loss: 140.1032\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 136.08881\n",
      "Epoch 174/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 102.7726 - val_loss: 141.0106\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 136.08881\n",
      "Epoch 175/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 103.0622 - val_loss: 146.6856\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 136.08881\n",
      "Epoch 176/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 103.2668 - val_loss: 144.4815\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 136.08881\n",
      "Epoch 177/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 96.8885 - val_loss: 140.4353\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 136.08881\n",
      "Epoch 178/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 99.0214 - val_loss: 153.0537\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 136.08881\n",
      "Epoch 179/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 113.3114 - val_loss: 154.1322\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 136.08881\n",
      "Epoch 180/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 103.9702 - val_loss: 146.9930\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 136.08881\n",
      "Epoch 181/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 110.2186 - val_loss: 155.2205\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 136.08881\n",
      "Epoch 182/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 106.8950 - val_loss: 136.2946\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 136.08881\n",
      "Epoch 183/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 103.3583 - val_loss: 139.6820\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 136.08881\n",
      "Epoch 184/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 102.4264 - val_loss: 148.9549\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 136.08881\n",
      "Epoch 185/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 96.8583 - val_loss: 138.9997\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 136.08881\n",
      "Epoch 186/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 102.5749 - val_loss: 146.6780\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 136.08881\n",
      "Epoch 187/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 103.0222 - val_loss: 153.9347\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 136.08881\n",
      "Epoch 188/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 103.5262 - val_loss: 145.2702\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 136.08881\n",
      "Epoch 189/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 108.3599 - val_loss: 147.4219\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 136.08881\n",
      "Epoch 190/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 98.5431 - val_loss: 152.6524\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 136.08881\n",
      "Epoch 191/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 102.3772 - val_loss: 148.8615\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 136.08881\n",
      "Epoch 192/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 97.7230 - val_loss: 158.8245\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 136.08881\n",
      "Epoch 193/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 99.2537 - val_loss: 147.3660\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 136.08881\n",
      "Epoch 194/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 643us/step - loss: 94.3685 - val_loss: 151.1672\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 136.08881\n",
      "Epoch 195/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 96.9153 - val_loss: 154.3106\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 136.08881\n",
      "Epoch 196/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 100.7534 - val_loss: 150.7843\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 136.08881\n",
      "Epoch 197/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 100.5405 - val_loss: 159.9276\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 136.08881\n",
      "Epoch 198/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 96.1933 - val_loss: 136.1549\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 136.08881\n",
      "Epoch 199/1000\n",
      "96/96 [==============================] - 0s 650us/step - loss: 93.4581 - val_loss: 157.6005\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 136.08881\n",
      "Epoch 200/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 96.0784 - val_loss: 148.9301\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 136.08881\n",
      "Epoch 201/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 88.2538 - val_loss: 157.0918\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 136.08881\n",
      "Epoch 202/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 92.0035 - val_loss: 151.3057\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 136.08881\n",
      "Epoch 203/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 93.5484 - val_loss: 151.9293\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 136.08881\n",
      "Epoch 204/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 93.9848 - val_loss: 151.5618\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 136.08881\n",
      "Epoch 205/1000\n",
      "96/96 [==============================] - 0s 648us/step - loss: 96.1970 - val_loss: 142.1480\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 136.08881\n",
      "Epoch 206/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 85.7094 - val_loss: 144.8017\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 136.08881\n",
      "Epoch 207/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 94.9497 - val_loss: 153.5256\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 136.08881\n",
      "Epoch 208/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 89.9232 - val_loss: 151.6386\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 136.08881\n",
      "Epoch 209/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 83.4454 - val_loss: 155.8927\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 136.08881\n",
      "Epoch 210/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 82.5305 - val_loss: 157.1168\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 136.08881\n",
      "Epoch 211/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 84.1045 - val_loss: 150.7370\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 136.08881\n",
      "Epoch 212/1000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 87.1852 - val_loss: 153.7753\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 136.08881\n",
      "Epoch 213/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 86.3530 - val_loss: 154.9989\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 136.08881\n",
      "Epoch 214/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 95.8758 - val_loss: 158.7519\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 136.08881\n",
      "Epoch 215/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 88.5347 - val_loss: 157.3427\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 136.08881\n",
      "Epoch 216/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 91.0050 - val_loss: 150.2333\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 136.08881\n",
      "Epoch 217/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 85.4689 - val_loss: 149.6840\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 136.08881\n",
      "Epoch 218/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 85.8199 - val_loss: 152.2495\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 136.08881\n",
      "Epoch 219/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 91.2888 - val_loss: 154.2320\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 136.08881\n",
      "Epoch 220/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 105.3348 - val_loss: 195.1969\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 136.08881\n",
      "Epoch 221/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 96.0845 - val_loss: 156.4202\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 136.08881\n",
      "Epoch 222/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 90.7271 - val_loss: 152.2283\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 136.08881\n",
      "Epoch 223/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 87.2588 - val_loss: 155.6572\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 136.08881\n",
      "Epoch 224/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 105.5415 - val_loss: 150.2054\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 136.08881\n",
      "Epoch 225/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 95.0496 - val_loss: 157.9165\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 136.08881\n",
      "Epoch 226/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 91.7530 - val_loss: 144.3720\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 136.08881\n",
      "Epoch 227/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 90.6181 - val_loss: 144.7157\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 136.08881\n",
      "Epoch 228/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 87.0276 - val_loss: 141.0365\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 136.08881\n",
      "Epoch 229/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 100.3764 - val_loss: 143.9548\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 136.08881\n",
      "Epoch 230/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 101.0694 - val_loss: 142.9446\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 136.08881\n",
      "Epoch 231/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 124.6599 - val_loss: 132.9294\n",
      "\n",
      "Epoch 00231: val_loss improved from 136.08881 to 132.92938, saving model to weights.hdf5\n",
      "Epoch 232/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 120.6374 - val_loss: 190.6394\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 132.92938\n",
      "Epoch 233/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 115.7052 - val_loss: 158.6404\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 132.92938\n",
      "Epoch 234/1000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 107.7328 - val_loss: 148.6166\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 132.92938\n",
      "Epoch 235/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 100.8188 - val_loss: 153.9804\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 132.92938\n",
      "Epoch 236/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 101.7761 - val_loss: 150.4962\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 132.92938\n",
      "Epoch 237/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 104.8694 - val_loss: 148.2165\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 132.92938\n",
      "Epoch 238/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 91.3957 - val_loss: 154.6055\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 132.92938\n",
      "Epoch 239/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 83.7917 - val_loss: 140.4767\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 132.92938\n",
      "Epoch 240/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 84.5655 - val_loss: 157.4453\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 132.92938\n",
      "Epoch 241/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 92.2738 - val_loss: 150.8113\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 132.92938\n",
      "Epoch 242/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 87.7598 - val_loss: 154.5929\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 132.92938\n",
      "Epoch 243/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 82.3596 - val_loss: 148.1223\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 132.92938\n",
      "Epoch 244/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 83.4041 - val_loss: 152.0241\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 132.92938\n",
      "Epoch 245/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 615us/step - loss: 81.0800 - val_loss: 147.6965\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 132.92938\n",
      "Epoch 246/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 81.2828 - val_loss: 152.2398\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 132.92938\n",
      "Epoch 247/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 77.8033 - val_loss: 146.2869\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 132.92938\n",
      "Epoch 248/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 83.0897 - val_loss: 154.0516\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 132.92938\n",
      "Epoch 249/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 88.5698 - val_loss: 157.8582\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 132.92938\n",
      "Epoch 250/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 79.3559 - val_loss: 154.5384\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 132.92938\n",
      "Epoch 251/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 76.1850 - val_loss: 151.9411\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 132.92938\n",
      "Epoch 252/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 78.9544 - val_loss: 151.5409\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 132.92938\n",
      "Epoch 253/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 77.9628 - val_loss: 158.2964\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 132.92938\n",
      "Epoch 254/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 76.5361 - val_loss: 157.2209\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 132.92938\n",
      "Epoch 255/1000\n",
      "96/96 [==============================] - 0s 654us/step - loss: 79.7067 - val_loss: 153.9238\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 132.92938\n",
      "Epoch 256/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 94.1537 - val_loss: 161.7614\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 132.92938\n",
      "Epoch 257/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 88.2079 - val_loss: 167.5488\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 132.92938\n",
      "Epoch 258/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 88.0068 - val_loss: 159.9635\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 132.92938\n",
      "Epoch 259/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 93.8965 - val_loss: 163.5953\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 132.92938\n",
      "Epoch 260/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 89.9764 - val_loss: 146.1412\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 132.92938\n",
      "Epoch 261/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 89.1058 - val_loss: 141.6484\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 132.92938\n",
      "Epoch 262/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 99.3553 - val_loss: 149.2641\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 132.92938\n",
      "Epoch 263/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 83.0045 - val_loss: 144.9252\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 132.92938\n",
      "Epoch 264/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 83.6058 - val_loss: 149.5055\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 132.92938\n",
      "Epoch 265/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 79.1563 - val_loss: 149.6326\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 132.92938\n",
      "Epoch 266/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 76.8400 - val_loss: 161.8344\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 132.92938\n",
      "Epoch 267/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 79.7492 - val_loss: 158.2794\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 132.92938\n",
      "Epoch 268/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 74.5660 - val_loss: 152.7565\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 132.92938\n",
      "Epoch 269/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 71.1007 - val_loss: 154.1175\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 132.92938\n",
      "Epoch 270/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 73.1040 - val_loss: 154.2332\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 132.92938\n",
      "Epoch 271/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 70.4936 - val_loss: 172.1088\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 132.92938\n",
      "Epoch 272/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 71.1475 - val_loss: 161.7717\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 132.92938\n",
      "Epoch 273/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 79.1177 - val_loss: 147.3213\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 132.92938\n",
      "Epoch 274/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 90.7174 - val_loss: 152.4719\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 132.92938\n",
      "Epoch 275/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 79.4464 - val_loss: 154.3558\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 132.92938\n",
      "Epoch 276/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 72.9666 - val_loss: 159.5042\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 132.92938\n",
      "Epoch 277/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 80.6090 - val_loss: 163.9455\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 132.92938\n",
      "Epoch 278/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 77.9534 - val_loss: 167.7473\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 132.92938\n",
      "Epoch 279/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 86.9792 - val_loss: 166.5041\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 132.92938\n",
      "Epoch 280/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 90.2730 - val_loss: 187.4763\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 132.92938\n",
      "Epoch 281/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 84.8793 - val_loss: 172.4136\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 132.92938\n",
      "Epoch 282/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 89.1435 - val_loss: 165.3771\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 132.92938\n",
      "Epoch 283/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 82.2291 - val_loss: 155.9490\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 132.92938\n",
      "Epoch 284/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 68.8636 - val_loss: 160.5109\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 132.92938\n",
      "Epoch 285/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 80.4316 - val_loss: 154.0479\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 132.92938\n",
      "Epoch 286/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 77.4596 - val_loss: 156.8404\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 132.92938\n",
      "Epoch 287/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 67.0739 - val_loss: 161.7100\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 132.92938\n",
      "Epoch 288/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 73.4499 - val_loss: 163.2886\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 132.92938\n",
      "Epoch 289/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 81.1718 - val_loss: 165.6375\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 132.92938\n",
      "Epoch 290/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 80.0097 - val_loss: 169.9241\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 132.92938\n",
      "Epoch 291/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 73.7743 - val_loss: 153.4376\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 132.92938\n",
      "Epoch 292/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 68.8668 - val_loss: 151.4850\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 132.92938\n",
      "Epoch 293/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 72.5003 - val_loss: 156.8305\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 132.92938\n",
      "Epoch 294/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 68.1609 - val_loss: 149.4476\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 132.92938\n",
      "Epoch 295/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 79.1195 - val_loss: 159.0919\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 132.92938\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 619us/step - loss: 64.9736 - val_loss: 161.5289\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 132.92938\n",
      "Epoch 297/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 82.5958 - val_loss: 168.0100\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 132.92938\n",
      "Epoch 298/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 70.5301 - val_loss: 164.9264\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 132.92938\n",
      "Epoch 299/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 71.6345 - val_loss: 153.3011\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 132.92938\n",
      "Epoch 300/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 76.8589 - val_loss: 168.1870\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 132.92938\n",
      "Epoch 301/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 67.9205 - val_loss: 157.7085\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 132.92938\n",
      "Epoch 302/1000\n",
      "96/96 [==============================] - 0s 648us/step - loss: 75.6168 - val_loss: 166.7843\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 132.92938\n",
      "Epoch 303/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 74.6604 - val_loss: 153.9191\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 132.92938\n",
      "Epoch 304/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 75.9069 - val_loss: 147.8629\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 132.92938\n",
      "Epoch 305/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 67.2043 - val_loss: 147.6162\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 132.92938\n",
      "Epoch 306/1000\n",
      "96/96 [==============================] - 0s 666us/step - loss: 73.3026 - val_loss: 151.4123\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 132.92938\n",
      "Epoch 307/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 68.8900 - val_loss: 164.4850\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 132.92938\n",
      "Epoch 308/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 62.7582 - val_loss: 156.0404\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 132.92938\n",
      "Epoch 309/1000\n",
      "96/96 [==============================] - 0s 583us/step - loss: 66.0996 - val_loss: 152.1504\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 132.92938\n",
      "Epoch 310/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 66.1086 - val_loss: 160.1591\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 132.92938\n",
      "Epoch 311/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 65.8539 - val_loss: 164.3188\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 132.92938\n",
      "Epoch 312/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 73.6307 - val_loss: 142.7016\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 132.92938\n",
      "Epoch 313/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 73.9233 - val_loss: 158.3706\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 132.92938\n",
      "Epoch 314/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 76.1041 - val_loss: 149.2675\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 132.92938\n",
      "Epoch 315/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 68.9656 - val_loss: 158.6801\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 132.92938\n",
      "Epoch 316/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 70.6685 - val_loss: 151.5517\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 132.92938\n",
      "Epoch 317/1000\n",
      "96/96 [==============================] - 0s 650us/step - loss: 73.3281 - val_loss: 155.3972\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 132.92938\n",
      "Epoch 318/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 69.9275 - val_loss: 151.9849\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 132.92938\n",
      "Epoch 319/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 72.7077 - val_loss: 152.2519\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 132.92938\n",
      "Epoch 320/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 76.7066 - val_loss: 151.8116\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 132.92938\n",
      "Epoch 321/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 72.7564 - val_loss: 149.7476\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 132.92938\n",
      "Epoch 322/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 79.8383 - val_loss: 136.8568\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 132.92938\n",
      "Epoch 323/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 72.0940 - val_loss: 151.3614\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 132.92938\n",
      "Epoch 324/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 73.4829 - val_loss: 147.6022\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 132.92938\n",
      "Epoch 325/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 65.4835 - val_loss: 154.3271\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 132.92938\n",
      "Epoch 326/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 68.8777 - val_loss: 136.5365\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 132.92938\n",
      "Epoch 327/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 64.4992 - val_loss: 157.8329\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 132.92938\n",
      "Epoch 328/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 71.4430 - val_loss: 141.9146\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 132.92938\n",
      "Epoch 329/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 73.0143 - val_loss: 152.3512\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 132.92938\n",
      "Epoch 330/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 82.9779 - val_loss: 148.4994\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 132.92938\n",
      "Epoch 331/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 64.6841 - val_loss: 157.5755\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 132.92938\n",
      "Epoch 332/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 75.5109 - val_loss: 154.9065\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 132.92938\n",
      "Epoch 333/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 68.4072 - val_loss: 155.5434\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 132.92938\n",
      "Epoch 334/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 70.1275 - val_loss: 151.2281\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 132.92938\n",
      "Epoch 335/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 62.5323 - val_loss: 146.9520\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 132.92938\n",
      "Epoch 336/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 71.9970 - val_loss: 141.5707\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 132.92938\n",
      "Epoch 337/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 68.6937 - val_loss: 162.7309\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 132.92938\n",
      "Epoch 338/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 78.6232 - val_loss: 163.2951\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 132.92938\n",
      "Epoch 339/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 80.3928 - val_loss: 153.9237\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 132.92938\n",
      "Epoch 340/1000\n",
      "96/96 [==============================] - 0s 661us/step - loss: 79.1239 - val_loss: 160.4362\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 132.92938\n",
      "Epoch 341/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 89.9996 - val_loss: 152.3637\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 132.92938\n",
      "Epoch 342/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 66.6767 - val_loss: 145.9494\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 132.92938\n",
      "Epoch 343/1000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 74.8020 - val_loss: 146.3118\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 132.92938\n",
      "Epoch 344/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 67.1361 - val_loss: 139.2793\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 132.92938\n",
      "Epoch 345/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 64.3196 - val_loss: 141.5582\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 132.92938\n",
      "Epoch 346/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 74.2867 - val_loss: 140.6677\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 132.92938\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 623us/step - loss: 79.2686 - val_loss: 158.7386\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 132.92938\n",
      "Epoch 348/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 74.3816 - val_loss: 153.8971\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 132.92938\n",
      "Epoch 349/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 65.3787 - val_loss: 161.1012\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 132.92938\n",
      "Epoch 350/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 96.1918 - val_loss: 152.0412\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 132.92938\n",
      "Epoch 351/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 79.6343 - val_loss: 159.2888\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 132.92938\n",
      "Epoch 352/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 71.6549 - val_loss: 147.9195\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 132.92938\n",
      "Epoch 353/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 72.5420 - val_loss: 142.0213\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 132.92938\n",
      "Epoch 354/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 66.4917 - val_loss: 144.1473\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 132.92938\n",
      "Epoch 355/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 84.9427 - val_loss: 142.4326\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 132.92938\n",
      "Epoch 356/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 64.2700 - val_loss: 151.1550\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 132.92938\n",
      "Epoch 357/1000\n",
      "96/96 [==============================] - 0s 677us/step - loss: 65.6475 - val_loss: 140.3399\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 132.92938\n",
      "Epoch 358/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 74.1968 - val_loss: 142.3265\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 132.92938\n",
      "Epoch 359/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 69.8009 - val_loss: 152.7012\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 132.92938\n",
      "Epoch 360/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 73.5748 - val_loss: 137.3068\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 132.92938\n",
      "Epoch 361/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 76.9886 - val_loss: 147.2678\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 132.92938\n",
      "Epoch 362/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 81.0324 - val_loss: 149.2109\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 132.92938\n",
      "Epoch 363/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 63.3810 - val_loss: 142.3049\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 132.92938\n",
      "Epoch 364/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 61.7895 - val_loss: 134.5167\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 132.92938\n",
      "Epoch 365/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 62.0590 - val_loss: 151.0602\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 132.92938\n",
      "Epoch 366/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 59.6829 - val_loss: 140.4805\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 132.92938\n",
      "Epoch 367/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 61.7238 - val_loss: 148.5736\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 132.92938\n",
      "Epoch 368/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 69.6706 - val_loss: 150.9316\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 132.92938\n",
      "Epoch 369/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 70.4507 - val_loss: 148.8367\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 132.92938\n",
      "Epoch 370/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 67.3022 - val_loss: 153.3603\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 132.92938\n",
      "Epoch 371/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 66.2468 - val_loss: 140.3492\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 132.92938\n",
      "Epoch 372/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 65.2241 - val_loss: 151.4215\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 132.92938\n",
      "Epoch 373/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 77.7876 - val_loss: 159.2764\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 132.92938\n",
      "Epoch 374/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 59.9592 - val_loss: 163.3670\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 132.92938\n",
      "Epoch 375/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 71.4744 - val_loss: 151.3443\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 132.92938\n",
      "Epoch 376/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 67.5850 - val_loss: 168.2917\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 132.92938\n",
      "Epoch 377/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 67.6454 - val_loss: 150.7231\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 132.92938\n",
      "Epoch 378/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 58.9927 - val_loss: 153.1210\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 132.92938\n",
      "Epoch 379/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 64.5704 - val_loss: 153.0085\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 132.92938\n",
      "Epoch 380/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 60.5987 - val_loss: 153.7787\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 132.92938\n",
      "Epoch 381/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 63.9455 - val_loss: 152.7521\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 132.92938\n",
      "Epoch 382/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 70.8908 - val_loss: 148.4603\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 132.92938\n",
      "Epoch 383/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 57.1303 - val_loss: 144.3890\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 132.92938\n",
      "Epoch 384/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 59.2991 - val_loss: 142.1476\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 132.92938\n",
      "Epoch 385/1000\n",
      "96/96 [==============================] - 0s 659us/step - loss: 69.7386 - val_loss: 147.0974\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 132.92938\n",
      "Epoch 386/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 82.9692 - val_loss: 156.5627\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 132.92938\n",
      "Epoch 387/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 59.9103 - val_loss: 149.7484\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 132.92938\n",
      "Epoch 388/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 65.6214 - val_loss: 167.1305\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 132.92938\n",
      "Epoch 389/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 66.5654 - val_loss: 141.4825\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 132.92938\n",
      "Epoch 390/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 59.2427 - val_loss: 150.6202\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 132.92938\n",
      "Epoch 391/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 71.7604 - val_loss: 165.5805\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 132.92938\n",
      "Epoch 392/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 63.8618 - val_loss: 154.4761\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 132.92938\n",
      "Epoch 393/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 64.9626 - val_loss: 171.2300\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 132.92938\n",
      "Epoch 394/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 73.9098 - val_loss: 146.0594\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 132.92938\n",
      "Epoch 395/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 61.7118 - val_loss: 158.9168\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 132.92938\n",
      "Epoch 396/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 62.7924 - val_loss: 142.8470\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 132.92938\n",
      "Epoch 397/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 75.2999 - val_loss: 148.1519\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 132.92938\n",
      "Epoch 398/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 633us/step - loss: 63.2069 - val_loss: 154.9718\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 132.92938\n",
      "Epoch 399/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 60.4652 - val_loss: 156.9376\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 132.92938\n",
      "Epoch 400/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 64.2982 - val_loss: 151.1987\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 132.92938\n",
      "Epoch 401/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 54.0143 - val_loss: 148.6910\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 132.92938\n",
      "Epoch 402/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 70.8207 - val_loss: 148.9721\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 132.92938\n",
      "Epoch 403/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 59.3630 - val_loss: 152.6273\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 132.92938\n",
      "Epoch 404/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 68.1120 - val_loss: 145.1295\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 132.92938\n",
      "Epoch 405/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 60.4146 - val_loss: 149.3128\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 132.92938\n",
      "Epoch 406/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 78.6159 - val_loss: 147.3349\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 132.92938\n",
      "Epoch 407/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 62.3204 - val_loss: 169.1948\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 132.92938\n",
      "Epoch 408/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 73.0695 - val_loss: 149.2063\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 132.92938\n",
      "Epoch 409/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 57.8341 - val_loss: 160.2251\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 132.92938\n",
      "Epoch 410/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 64.3735 - val_loss: 144.0122\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 132.92938\n",
      "Epoch 411/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 70.4557 - val_loss: 156.5492\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 132.92938\n",
      "Epoch 412/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 71.0349 - val_loss: 159.2221\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 132.92938\n",
      "Epoch 413/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 71.0116 - val_loss: 151.9976\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 132.92938\n",
      "Epoch 414/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 72.7430 - val_loss: 148.7337\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 132.92938\n",
      "Epoch 415/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 64.5454 - val_loss: 150.5644\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 132.92938\n",
      "Epoch 416/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 56.4717 - val_loss: 152.9150\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 132.92938\n",
      "Epoch 417/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 56.9931 - val_loss: 150.3474\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 132.92938\n",
      "Epoch 418/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 61.7773 - val_loss: 142.5830\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 132.92938\n",
      "Epoch 419/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 52.2971 - val_loss: 163.2536\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 132.92938\n",
      "Epoch 420/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 53.8705 - val_loss: 147.6764\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 132.92938\n",
      "Epoch 421/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 56.9940 - val_loss: 168.0518\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 132.92938\n",
      "Epoch 422/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 56.0229 - val_loss: 140.8901\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 132.92938\n",
      "Epoch 423/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 61.3817 - val_loss: 163.3556\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 132.92938\n",
      "Epoch 424/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 78.1408 - val_loss: 171.0046\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 132.92938\n",
      "Epoch 425/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 65.5465 - val_loss: 175.2965\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 132.92938\n",
      "Epoch 426/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 76.3943 - val_loss: 158.3845\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 132.92938\n",
      "Epoch 427/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 66.9113 - val_loss: 163.2199\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 132.92938\n",
      "Epoch 428/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 70.5278 - val_loss: 163.8867\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 132.92938\n",
      "Epoch 429/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 88.2306 - val_loss: 165.2122\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 132.92938\n",
      "Epoch 430/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 70.8482 - val_loss: 170.3507\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 132.92938\n",
      "Epoch 431/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 75.6987 - val_loss: 208.8439\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 132.92938\n",
      "Epoch 432/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 68.0402 - val_loss: 158.3165\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 132.92938\n",
      "Epoch 433/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 74.3373 - val_loss: 166.6103\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 132.92938\n",
      "Epoch 434/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 68.2134 - val_loss: 163.3124\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 132.92938\n",
      "Epoch 435/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 67.3865 - val_loss: 154.8695\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 132.92938\n",
      "Epoch 436/1000\n",
      "96/96 [==============================] - 0s 597us/step - loss: 60.9004 - val_loss: 154.9961\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 132.92938\n",
      "Epoch 437/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 61.5030 - val_loss: 153.4497\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 132.92938\n",
      "Epoch 438/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 67.0325 - val_loss: 150.3153\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 132.92938\n",
      "Epoch 439/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 55.0510 - val_loss: 147.1118\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 132.92938\n",
      "Epoch 440/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 57.3226 - val_loss: 146.2181\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 132.92938\n",
      "Epoch 441/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 65.3495 - val_loss: 153.3462\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 132.92938\n",
      "Epoch 442/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 53.0701 - val_loss: 150.6997\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 132.92938\n",
      "Epoch 443/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 63.1351 - val_loss: 158.5590\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 132.92938\n",
      "Epoch 444/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 62.5668 - val_loss: 154.9104\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 132.92938\n",
      "Epoch 445/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 59.2416 - val_loss: 157.4232\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 132.92938\n",
      "Epoch 446/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 58.4473 - val_loss: 156.7460\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 132.92938\n",
      "Epoch 447/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 63.6924 - val_loss: 156.8485\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 132.92938\n",
      "Epoch 448/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 52.9080 - val_loss: 156.7251\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 132.92938\n",
      "Epoch 449/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 618us/step - loss: 59.7752 - val_loss: 156.8671\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 132.92938\n",
      "Epoch 450/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 54.7327 - val_loss: 151.9531\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 132.92938\n",
      "Epoch 451/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 66.9437 - val_loss: 151.0052\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 132.92938\n",
      "Epoch 452/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 65.6265 - val_loss: 146.5829\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 132.92938\n",
      "Epoch 453/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 56.1000 - val_loss: 153.5081\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 132.92938\n",
      "Epoch 454/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 65.1410 - val_loss: 152.9508\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 132.92938\n",
      "Epoch 455/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 59.1151 - val_loss: 156.8923\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 132.92938\n",
      "Epoch 456/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 60.4861 - val_loss: 153.1426\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 132.92938\n",
      "Epoch 457/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 63.0727 - val_loss: 149.1460\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 132.92938\n",
      "Epoch 458/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 83.9375 - val_loss: 155.6800\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 132.92938\n",
      "Epoch 459/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 59.6534 - val_loss: 155.9159\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 132.92938\n",
      "Epoch 460/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 78.0787 - val_loss: 175.1009\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 132.92938\n",
      "Epoch 461/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 57.4211 - val_loss: 156.2945\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 132.92938\n",
      "Epoch 462/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 59.9254 - val_loss: 154.7935\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 132.92938\n",
      "Epoch 463/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 67.0001 - val_loss: 155.7595\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 132.92938\n",
      "Epoch 464/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 53.0213 - val_loss: 158.2120\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 132.92938\n",
      "Epoch 465/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 54.8012 - val_loss: 157.5555\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 132.92938\n",
      "Epoch 466/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 55.5206 - val_loss: 154.1289\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 132.92938\n",
      "Epoch 467/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 61.7756 - val_loss: 152.9838\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 132.92938\n",
      "Epoch 468/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 61.2249 - val_loss: 149.0096\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 132.92938\n",
      "Epoch 469/1000\n",
      "96/96 [==============================] - 0s 595us/step - loss: 75.2264 - val_loss: 153.7512\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 132.92938\n",
      "Epoch 470/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 70.4307 - val_loss: 149.3619\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 132.92938\n",
      "Epoch 471/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 71.1401 - val_loss: 149.5500\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 132.92938\n",
      "Epoch 472/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 52.5694 - val_loss: 150.3717\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 132.92938\n",
      "Epoch 473/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 50.4304 - val_loss: 159.1188\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 132.92938\n",
      "Epoch 474/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 68.3113 - val_loss: 156.5574\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 132.92938\n",
      "Epoch 475/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 71.2061 - val_loss: 152.6340\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 132.92938\n",
      "Epoch 476/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 53.8600 - val_loss: 150.9197\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 132.92938\n",
      "Epoch 477/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 58.7944 - val_loss: 154.1714\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 132.92938\n",
      "Epoch 478/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 56.1973 - val_loss: 157.7577\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 132.92938\n",
      "Epoch 479/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 73.1849 - val_loss: 156.1872\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 132.92938\n",
      "Epoch 480/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 55.4891 - val_loss: 157.1693\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 132.92938\n",
      "Epoch 481/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 58.8511 - val_loss: 153.9496\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 132.92938\n",
      "Epoch 482/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 59.5429 - val_loss: 167.5776\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 132.92938\n",
      "Epoch 483/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 57.0849 - val_loss: 153.0074\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 132.92938\n",
      "Epoch 484/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 67.0965 - val_loss: 154.7255\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 132.92938\n",
      "Epoch 485/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 53.7335 - val_loss: 158.9249\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 132.92938\n",
      "Epoch 486/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 69.8626 - val_loss: 152.6361\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 132.92938\n",
      "Epoch 487/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 63.6342 - val_loss: 168.4730\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 132.92938\n",
      "Epoch 488/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 81.9988 - val_loss: 172.3947\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 132.92938\n",
      "Epoch 489/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 72.3302 - val_loss: 154.7661\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 132.92938\n",
      "Epoch 490/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 62.2199 - val_loss: 159.4395\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 132.92938\n",
      "Epoch 491/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 53.4388 - val_loss: 148.2800\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 132.92938\n",
      "Epoch 492/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 58.3393 - val_loss: 141.9536\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 132.92938\n",
      "Epoch 493/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 54.8201 - val_loss: 151.8485\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 132.92938\n",
      "Epoch 494/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 56.6545 - val_loss: 157.5547\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 132.92938\n",
      "Epoch 495/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 47.7048 - val_loss: 161.8867\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 132.92938\n",
      "Epoch 496/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 54.5029 - val_loss: 161.7046\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 132.92938\n",
      "Epoch 497/1000\n",
      "96/96 [==============================] - 0s 595us/step - loss: 53.2445 - val_loss: 156.9192\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 132.92938\n",
      "Epoch 498/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 48.1663 - val_loss: 151.6528\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 132.92938\n",
      "Epoch 499/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 58.8608 - val_loss: 163.7199\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 132.92938\n",
      "Epoch 500/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 631us/step - loss: 53.5482 - val_loss: 168.5904\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 132.92938\n",
      "Epoch 501/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 57.4969 - val_loss: 163.4366\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 132.92938\n",
      "Epoch 502/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 54.7050 - val_loss: 154.3233\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 132.92938\n",
      "Epoch 503/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 46.2534 - val_loss: 156.2976\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 132.92938\n",
      "Epoch 504/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 61.3401 - val_loss: 161.9968\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 132.92938\n",
      "Epoch 505/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 67.6709 - val_loss: 158.7798\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 132.92938\n",
      "Epoch 506/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 50.5938 - val_loss: 161.7258\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 132.92938\n",
      "Epoch 507/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 46.1961 - val_loss: 162.6764\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 132.92938\n",
      "Epoch 508/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 53.8601 - val_loss: 156.4181\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 132.92938\n",
      "Epoch 509/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 49.5397 - val_loss: 157.3641\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 132.92938\n",
      "Epoch 510/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 67.7111 - val_loss: 161.4066\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 132.92938\n",
      "Epoch 511/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 65.1347 - val_loss: 160.2639\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 132.92938\n",
      "Epoch 512/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 69.8344 - val_loss: 152.4346\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 132.92938\n",
      "Epoch 513/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 64.0724 - val_loss: 155.3170\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 132.92938\n",
      "Epoch 514/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 51.1458 - val_loss: 162.9603\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 132.92938\n",
      "Epoch 515/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 51.0452 - val_loss: 163.9307\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 132.92938\n",
      "Epoch 516/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 74.9516 - val_loss: 159.3293\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 132.92938\n",
      "Epoch 517/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 58.9760 - val_loss: 156.7693\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 132.92938\n",
      "Epoch 518/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 72.1569 - val_loss: 153.4907\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 132.92938\n",
      "Epoch 519/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 47.0874 - val_loss: 167.4435\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 132.92938\n",
      "Epoch 520/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 54.3655 - val_loss: 159.1426\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 132.92938\n",
      "Epoch 521/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 59.5379 - val_loss: 154.1623\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 132.92938\n",
      "Epoch 522/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 50.0247 - val_loss: 156.6964\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 132.92938\n",
      "Epoch 523/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 69.6186 - val_loss: 166.2479\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 132.92938\n",
      "Epoch 524/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 61.7809 - val_loss: 167.9500\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 132.92938\n",
      "Epoch 525/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 64.2149 - val_loss: 165.2463\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 132.92938\n",
      "Epoch 526/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 52.9358 - val_loss: 166.1197\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 132.92938\n",
      "Epoch 527/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 48.8442 - val_loss: 155.2954\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 132.92938\n",
      "Epoch 528/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 57.2742 - val_loss: 159.0641\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 132.92938\n",
      "Epoch 529/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 51.9768 - val_loss: 161.4332\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 132.92938\n",
      "Epoch 530/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 59.4212 - val_loss: 160.2755\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 132.92938\n",
      "Epoch 531/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 48.3818 - val_loss: 165.7011\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 132.92938\n",
      "Epoch 532/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 49.8909 - val_loss: 158.9252\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 132.92938\n",
      "Epoch 533/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 66.8523 - val_loss: 158.8977\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 132.92938\n",
      "Epoch 534/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 49.4291 - val_loss: 153.6634\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 132.92938\n",
      "Epoch 535/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 102.1398 - val_loss: 152.8439\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 132.92938\n",
      "Epoch 536/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 63.8814 - val_loss: 156.8536\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 132.92938\n",
      "Epoch 537/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 58.7006 - val_loss: 153.6511\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 132.92938\n",
      "Epoch 538/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 64.9647 - val_loss: 159.7546\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 132.92938\n",
      "Epoch 539/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 60.4234 - val_loss: 157.9571\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 132.92938\n",
      "Epoch 540/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 64.5814 - val_loss: 160.7313\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 132.92938\n",
      "Epoch 541/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 65.7875 - val_loss: 158.2107\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 132.92938\n",
      "Epoch 542/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 63.5846 - val_loss: 161.0775\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 132.92938\n",
      "Epoch 543/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 52.5461 - val_loss: 167.8548\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 132.92938\n",
      "Epoch 544/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 52.6208 - val_loss: 163.7879\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 132.92938\n",
      "Epoch 545/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 59.3720 - val_loss: 166.0802\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 132.92938\n",
      "Epoch 546/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 43.4374 - val_loss: 178.9322\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 132.92938\n",
      "Epoch 547/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 63.4867 - val_loss: 164.2273\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 132.92938\n",
      "Epoch 548/1000\n",
      "96/96 [==============================] - 0s 659us/step - loss: 59.0768 - val_loss: 163.6654\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 132.92938\n",
      "Epoch 549/1000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 55.3959 - val_loss: 169.5569\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 132.92938\n",
      "Epoch 550/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 55.7444 - val_loss: 165.6337\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 132.92938\n",
      "Epoch 551/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 632us/step - loss: 61.7082 - val_loss: 156.5027\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 132.92938\n",
      "Epoch 552/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 98.0384 - val_loss: 156.6182\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 132.92938\n",
      "Epoch 553/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 84.2754 - val_loss: 154.9107\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 132.92938\n",
      "Epoch 554/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 85.3045 - val_loss: 179.9828\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 132.92938\n",
      "Epoch 555/1000\n",
      "96/96 [==============================] - 0s 654us/step - loss: 82.5747 - val_loss: 178.5894\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 132.92938\n",
      "Epoch 556/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 77.7184 - val_loss: 176.9520\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 132.92938\n",
      "Epoch 557/1000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 59.8915 - val_loss: 187.5277\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 132.92938\n",
      "Epoch 558/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 58.6365 - val_loss: 173.7846\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 132.92938\n",
      "Epoch 559/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 55.1323 - val_loss: 167.4634\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 132.92938\n",
      "Epoch 560/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 45.7965 - val_loss: 180.8188\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 132.92938\n",
      "Epoch 561/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 49.7815 - val_loss: 183.6737\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 132.92938\n",
      "Epoch 562/1000\n",
      "96/96 [==============================] - 0s 654us/step - loss: 60.6018 - val_loss: 180.0576\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 132.92938\n",
      "Epoch 563/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 64.5974 - val_loss: 167.1613\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 132.92938\n",
      "Epoch 564/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 57.5326 - val_loss: 157.7085\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 132.92938\n",
      "Epoch 565/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 49.6574 - val_loss: 159.7766\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 132.92938\n",
      "Epoch 566/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 58.9749 - val_loss: 175.7276\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 132.92938\n",
      "Epoch 567/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 49.9931 - val_loss: 169.3611\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 132.92938\n",
      "Epoch 568/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 59.0058 - val_loss: 169.8527\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 132.92938\n",
      "Epoch 569/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 49.6763 - val_loss: 166.5736\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 132.92938\n",
      "Epoch 570/1000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 59.9817 - val_loss: 167.9631\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 132.92938\n",
      "Epoch 571/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.3896 - val_loss: 158.5152\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 132.92938\n",
      "Epoch 572/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 44.4831 - val_loss: 156.8429\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 132.92938\n",
      "Epoch 573/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 47.4066 - val_loss: 160.0960\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 132.92938\n",
      "Epoch 574/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 57.4409 - val_loss: 155.7429\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 132.92938\n",
      "Epoch 575/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 47.8632 - val_loss: 161.0025\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 132.92938\n",
      "Epoch 576/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 54.6414 - val_loss: 163.3623\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 132.92938\n",
      "Epoch 577/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 64.5831 - val_loss: 159.9714\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 132.92938\n",
      "Epoch 578/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 54.4439 - val_loss: 168.3067\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 132.92938\n",
      "Epoch 579/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 54.0910 - val_loss: 158.1045\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 132.92938\n",
      "Epoch 580/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 56.1136 - val_loss: 164.3116\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 132.92938\n",
      "Epoch 581/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 48.1513 - val_loss: 162.4225\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 132.92938\n",
      "Epoch 582/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 50.7755 - val_loss: 163.5936\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 132.92938\n",
      "Epoch 583/1000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 50.8182 - val_loss: 164.8531\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 132.92938\n",
      "Epoch 584/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 50.3555 - val_loss: 161.7095\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 132.92938\n",
      "Epoch 585/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 53.1497 - val_loss: 162.7461\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 132.92938\n",
      "Epoch 586/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 61.0783 - val_loss: 161.5108\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 132.92938\n",
      "Epoch 587/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 48.4984 - val_loss: 163.1822\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 132.92938\n",
      "Epoch 588/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 54.4798 - val_loss: 161.7279\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 132.92938\n",
      "Epoch 589/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 59.0157 - val_loss: 156.8767\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 132.92938\n",
      "Epoch 590/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 49.8967 - val_loss: 167.1634\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 132.92938\n",
      "Epoch 591/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 49.7613 - val_loss: 160.2384\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 132.92938\n",
      "Epoch 592/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 57.8567 - val_loss: 163.0863\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 132.92938\n",
      "Epoch 593/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 49.4956 - val_loss: 165.6430\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 132.92938\n",
      "Epoch 594/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 44.2633 - val_loss: 163.9931\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 132.92938\n",
      "Epoch 595/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 68.2676 - val_loss: 175.1456\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 132.92938\n",
      "Epoch 596/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 59.6759 - val_loss: 158.4661\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 132.92938\n",
      "Epoch 597/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 50.4040 - val_loss: 166.3182\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 132.92938\n",
      "Epoch 598/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 55.7127 - val_loss: 164.0083\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 132.92938\n",
      "Epoch 599/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 60.4896 - val_loss: 170.6996\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 132.92938\n",
      "Epoch 600/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 49.8113 - val_loss: 164.7512\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 132.92938\n",
      "Epoch 601/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 50.4113 - val_loss: 163.2634\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 132.92938\n",
      "Epoch 602/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 618us/step - loss: 77.7266 - val_loss: 161.2518\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 132.92938\n",
      "Epoch 603/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 53.3614 - val_loss: 153.9298\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 132.92938\n",
      "Epoch 604/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 54.0609 - val_loss: 173.9436\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 132.92938\n",
      "Epoch 605/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 51.8281 - val_loss: 158.2046\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 132.92938\n",
      "Epoch 606/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 47.2107 - val_loss: 155.1305\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 132.92938\n",
      "Epoch 607/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 51.7965 - val_loss: 156.9471\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 132.92938\n",
      "Epoch 608/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 48.5867 - val_loss: 160.3035\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 132.92938\n",
      "Epoch 609/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 64.9096 - val_loss: 166.4082\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 132.92938\n",
      "Epoch 610/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 54.5732 - val_loss: 171.0726\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 132.92938\n",
      "Epoch 611/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 42.1159 - val_loss: 168.2432\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 132.92938\n",
      "Epoch 612/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 56.0015 - val_loss: 169.1037\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 132.92938\n",
      "Epoch 613/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 53.2577 - val_loss: 169.1935\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 132.92938\n",
      "Epoch 614/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 45.4045 - val_loss: 169.5130\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 132.92938\n",
      "Epoch 615/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.7460 - val_loss: 163.6881\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 132.92938\n",
      "Epoch 616/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 54.4298 - val_loss: 168.0610\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 132.92938\n",
      "Epoch 617/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 54.8460 - val_loss: 163.1327\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 132.92938\n",
      "Epoch 618/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 70.0442 - val_loss: 177.9880\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 132.92938\n",
      "Epoch 619/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 71.6196 - val_loss: 175.6915\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 132.92938\n",
      "Epoch 620/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 62.4890 - val_loss: 187.9797\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 132.92938\n",
      "Epoch 621/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 69.0049 - val_loss: 175.3215\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 132.92938\n",
      "Epoch 622/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 62.2162 - val_loss: 172.9476\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 132.92938\n",
      "Epoch 623/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 44.7645 - val_loss: 179.6115\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 132.92938\n",
      "Epoch 624/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 49.8117 - val_loss: 172.1924\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 132.92938\n",
      "Epoch 625/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 45.8175 - val_loss: 171.5043\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 132.92938\n",
      "Epoch 626/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 48.1212 - val_loss: 176.6347\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 132.92938\n",
      "Epoch 627/1000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 49.6974 - val_loss: 175.9253\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 132.92938\n",
      "Epoch 628/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 57.2390 - val_loss: 176.8185\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 132.92938\n",
      "Epoch 629/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 87.2364 - val_loss: 168.5512\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 132.92938\n",
      "Epoch 630/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 58.9529 - val_loss: 165.1892\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 132.92938\n",
      "Epoch 631/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 58.1554 - val_loss: 171.3072\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 132.92938\n",
      "Epoch 632/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 59.3288 - val_loss: 172.0466\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 132.92938\n",
      "Epoch 633/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.8381 - val_loss: 175.1161\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 132.92938\n",
      "Epoch 634/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 62.9621 - val_loss: 168.0299\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 132.92938\n",
      "Epoch 635/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 47.0559 - val_loss: 163.6684\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 132.92938\n",
      "Epoch 636/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 44.1300 - val_loss: 165.7077\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 132.92938\n",
      "Epoch 637/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 52.0630 - val_loss: 165.1553\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 132.92938\n",
      "Epoch 638/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 44.5205 - val_loss: 172.6756\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 132.92938\n",
      "Epoch 639/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 51.1455 - val_loss: 171.9203\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 132.92938\n",
      "Epoch 640/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 55.1842 - val_loss: 176.4550\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 132.92938\n",
      "Epoch 641/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 71.2142 - val_loss: 183.7916\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 132.92938\n",
      "Epoch 642/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 54.0364 - val_loss: 187.8747\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 132.92938\n",
      "Epoch 643/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 58.1642 - val_loss: 195.4710\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 132.92938\n",
      "Epoch 644/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 69.6444 - val_loss: 205.4423\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 132.92938\n",
      "Epoch 645/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 63.2826 - val_loss: 187.6091\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 132.92938\n",
      "Epoch 646/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 74.5434 - val_loss: 186.8962\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 132.92938\n",
      "Epoch 647/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 71.2660 - val_loss: 165.3771\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 132.92938\n",
      "Epoch 648/1000\n",
      "96/96 [==============================] - 0s 656us/step - loss: 72.2899 - val_loss: 166.1884\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 132.92938\n",
      "Epoch 649/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 57.4155 - val_loss: 190.1026\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 132.92938\n",
      "Epoch 650/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 47.2143 - val_loss: 191.1387\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 132.92938\n",
      "Epoch 651/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 70.8702 - val_loss: 185.4839\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 132.92938\n",
      "Epoch 652/1000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 46.1812 - val_loss: 170.9254\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 132.92938\n",
      "Epoch 653/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 630us/step - loss: 47.6195 - val_loss: 171.5453\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 132.92938\n",
      "Epoch 654/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 40.3121 - val_loss: 176.6921\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 132.92938\n",
      "Epoch 655/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 55.0018 - val_loss: 172.5236\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 132.92938\n",
      "Epoch 656/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 54.5825 - val_loss: 178.5382\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 132.92938\n",
      "Epoch 657/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 55.0558 - val_loss: 178.4217\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 132.92938\n",
      "Epoch 658/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 50.7588 - val_loss: 187.8812\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 132.92938\n",
      "Epoch 659/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 66.2240 - val_loss: 199.8814\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 132.92938\n",
      "Epoch 660/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 58.2621 - val_loss: 201.2249\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 132.92938\n",
      "Epoch 661/1000\n",
      "96/96 [==============================] - 0s 651us/step - loss: 81.9699 - val_loss: 184.4963\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 132.92938\n",
      "Epoch 662/1000\n",
      "96/96 [==============================] - 0s 588us/step - loss: 81.7052 - val_loss: 166.2191\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 132.92938\n",
      "Epoch 663/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 49.9415 - val_loss: 162.0020\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 132.92938\n",
      "Epoch 664/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 63.4318 - val_loss: 167.3591\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 132.92938\n",
      "Epoch 665/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 82.0979 - val_loss: 167.0166\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 132.92938\n",
      "Epoch 666/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 106.0237 - val_loss: 168.3074\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 132.92938\n",
      "Epoch 667/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 63.0631 - val_loss: 169.5635\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 132.92938\n",
      "Epoch 668/1000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 67.6473 - val_loss: 171.2556\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 132.92938\n",
      "Epoch 669/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 80.5262 - val_loss: 165.2348\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 132.92938\n",
      "Epoch 670/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 72.0112 - val_loss: 148.8327\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 132.92938\n",
      "Epoch 671/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 55.4684 - val_loss: 162.8518\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 132.92938\n",
      "Epoch 672/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 59.4136 - val_loss: 153.3983\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 132.92938\n",
      "Epoch 673/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 58.7026 - val_loss: 166.6278\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 132.92938\n",
      "Epoch 674/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 71.4622 - val_loss: 175.7701\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 132.92938\n",
      "Epoch 675/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 52.2081 - val_loss: 166.4655\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 132.92938\n",
      "Epoch 676/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 68.2835 - val_loss: 165.2289\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 132.92938\n",
      "Epoch 677/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 76.1785 - val_loss: 172.2751\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 132.92938\n",
      "Epoch 678/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 57.0898 - val_loss: 169.6526\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 132.92938\n",
      "Epoch 679/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 66.1048 - val_loss: 169.7968\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 132.92938\n",
      "Epoch 680/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 63.3390 - val_loss: 173.6744\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 132.92938\n",
      "Epoch 681/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 59.0392 - val_loss: 166.7059\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 132.92938\n",
      "Epoch 682/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 54.1540 - val_loss: 166.4996\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 132.92938\n",
      "Epoch 683/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 43.9984 - val_loss: 174.4003\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 132.92938\n",
      "Epoch 684/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 48.8490 - val_loss: 178.9828\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 132.92938\n",
      "Epoch 685/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 55.4395 - val_loss: 170.5508\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 132.92938\n",
      "Epoch 686/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 72.3326 - val_loss: 195.9080\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 132.92938\n",
      "Epoch 687/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 54.4898 - val_loss: 182.7112\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 132.92938\n",
      "Epoch 688/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 50.7237 - val_loss: 164.8740\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 132.92938\n",
      "Epoch 689/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 47.6660 - val_loss: 168.3514\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 132.92938\n",
      "Epoch 690/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 47.3500 - val_loss: 165.7280\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 132.92938\n",
      "Epoch 691/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 61.8991 - val_loss: 171.8572\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 132.92938\n",
      "Epoch 692/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 45.0630 - val_loss: 169.7327\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 132.92938\n",
      "Epoch 693/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 57.3989 - val_loss: 168.2256\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 132.92938\n",
      "Epoch 694/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 47.0727 - val_loss: 165.8571\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 132.92938\n",
      "Epoch 695/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 56.8790 - val_loss: 168.7015\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 132.92938\n",
      "Epoch 696/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 48.6580 - val_loss: 167.4626\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 132.92938\n",
      "Epoch 697/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 55.9543 - val_loss: 175.9062\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 132.92938\n",
      "Epoch 698/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 43.6160 - val_loss: 177.5882\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 132.92938\n",
      "Epoch 699/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 66.7693 - val_loss: 174.6683\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 132.92938\n",
      "Epoch 700/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 41.1787 - val_loss: 174.8575\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 132.92938\n",
      "Epoch 701/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 51.1674 - val_loss: 175.4945\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 132.92938\n",
      "Epoch 702/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 59.5547 - val_loss: 180.3825\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 132.92938\n",
      "Epoch 703/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 47.0484 - val_loss: 189.1987\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 132.92938\n",
      "Epoch 704/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 624us/step - loss: 51.0369 - val_loss: 191.7340\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 132.92938\n",
      "Epoch 705/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 65.5964 - val_loss: 173.4060\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 132.92938\n",
      "Epoch 706/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 58.2084 - val_loss: 171.3313\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 132.92938\n",
      "Epoch 707/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.1647 - val_loss: 178.4361\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 132.92938\n",
      "Epoch 708/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.4145 - val_loss: 180.0155\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 132.92938\n",
      "Epoch 709/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 66.0020 - val_loss: 210.2857\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 132.92938\n",
      "Epoch 710/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 60.6807 - val_loss: 183.4273\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 132.92938\n",
      "Epoch 711/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 62.6933 - val_loss: 171.8196\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 132.92938\n",
      "Epoch 712/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 55.1977 - val_loss: 168.3213\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 132.92938\n",
      "Epoch 713/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 49.5275 - val_loss: 176.9281\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 132.92938\n",
      "Epoch 714/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 54.8468 - val_loss: 173.0116\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 132.92938\n",
      "Epoch 715/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 47.2058 - val_loss: 172.3844\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 132.92938\n",
      "Epoch 716/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 42.9556 - val_loss: 170.4217\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 132.92938\n",
      "Epoch 717/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 59.2047 - val_loss: 171.2195\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 132.92938\n",
      "Epoch 718/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 89.1909 - val_loss: 199.0458\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 132.92938\n",
      "Epoch 719/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 73.7730 - val_loss: 197.4152\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 132.92938\n",
      "Epoch 720/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 94.1749 - val_loss: 207.4254\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 132.92938\n",
      "Epoch 721/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 58.8234 - val_loss: 199.6518\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 132.92938\n",
      "Epoch 722/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 52.3965 - val_loss: 201.5349\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 132.92938\n",
      "Epoch 723/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 58.7692 - val_loss: 191.1470\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 132.92938\n",
      "Epoch 724/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 53.0753 - val_loss: 200.1772\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 132.92938\n",
      "Epoch 725/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 48.5699 - val_loss: 196.6956\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 132.92938\n",
      "Epoch 726/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 48.7546 - val_loss: 194.1587\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 132.92938\n",
      "Epoch 727/1000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 51.0113 - val_loss: 191.4417\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 132.92938\n",
      "Epoch 728/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 48.2960 - val_loss: 192.6907\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 132.92938\n",
      "Epoch 729/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 42.0859 - val_loss: 194.6292\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 132.92938\n",
      "Epoch 730/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 50.0180 - val_loss: 196.2517\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 132.92938\n",
      "Epoch 731/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 44.0915 - val_loss: 194.8342\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 132.92938\n",
      "Epoch 732/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 55.2421 - val_loss: 168.5283\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 132.92938\n",
      "Epoch 733/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 42.5537 - val_loss: 165.4908\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 132.92938\n",
      "Epoch 734/1000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 44.3441 - val_loss: 167.7868\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 132.92938\n",
      "Epoch 735/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 40.3495 - val_loss: 180.5552\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 132.92938\n",
      "Epoch 736/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 53.1088 - val_loss: 176.0503\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 132.92938\n",
      "Epoch 737/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.8102 - val_loss: 172.5820\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 132.92938\n",
      "Epoch 738/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 64.5901 - val_loss: 163.7907\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 132.92938\n",
      "Epoch 739/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 50.3637 - val_loss: 163.1243\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 132.92938\n",
      "Epoch 740/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 69.5699 - val_loss: 168.8034\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 132.92938\n",
      "Epoch 741/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 43.0595 - val_loss: 176.3376\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 132.92938\n",
      "Epoch 742/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 51.8110 - val_loss: 182.5515\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 132.92938\n",
      "Epoch 743/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 43.1445 - val_loss: 171.9667\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 132.92938\n",
      "Epoch 744/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 42.0742 - val_loss: 172.9069\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 132.92938\n",
      "Epoch 745/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 38.7754 - val_loss: 168.1673\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 132.92938\n",
      "Epoch 746/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 45.0586 - val_loss: 171.9433\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 132.92938\n",
      "Epoch 747/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 49.4149 - val_loss: 174.5487\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 132.92938\n",
      "Epoch 748/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 43.1068 - val_loss: 176.5316\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 132.92938\n",
      "Epoch 749/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 63.9478 - val_loss: 187.6043\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 132.92938\n",
      "Epoch 750/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 50.2270 - val_loss: 210.6276\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 132.92938\n",
      "Epoch 751/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 55.3195 - val_loss: 217.5799\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 132.92938\n",
      "Epoch 752/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 49.7301 - val_loss: 202.4588\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 132.92938\n",
      "Epoch 753/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 49.9668 - val_loss: 182.3366\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 132.92938\n",
      "Epoch 754/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 41.8060 - val_loss: 181.3127\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 132.92938\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 624us/step - loss: 60.6330 - val_loss: 190.2359\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 132.92938\n",
      "Epoch 756/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 47.7622 - val_loss: 190.6828\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 132.92938\n",
      "Epoch 757/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 45.7001 - val_loss: 192.9324\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 132.92938\n",
      "Epoch 758/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 46.8039 - val_loss: 189.9337\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 132.92938\n",
      "Epoch 759/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 38.8618 - val_loss: 185.0134\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 132.92938\n",
      "Epoch 760/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 47.1108 - val_loss: 189.0333\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 132.92938\n",
      "Epoch 761/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 45.8622 - val_loss: 190.4420\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 132.92938\n",
      "Epoch 762/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 51.4932 - val_loss: 192.9703\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 132.92938\n",
      "Epoch 763/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 52.6718 - val_loss: 185.0780\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 132.92938\n",
      "Epoch 764/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 68.1192 - val_loss: 194.7135\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 132.92938\n",
      "Epoch 765/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 69.3107 - val_loss: 214.3431\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 132.92938\n",
      "Epoch 766/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 83.8794 - val_loss: 205.7623\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 132.92938\n",
      "Epoch 767/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 62.4076 - val_loss: 205.2894\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 132.92938\n",
      "Epoch 768/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 49.4554 - val_loss: 191.8099\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 132.92938\n",
      "Epoch 769/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 63.7905 - val_loss: 218.3543\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 132.92938\n",
      "Epoch 770/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 52.9962 - val_loss: 177.6404\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 132.92938\n",
      "Epoch 771/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 48.4222 - val_loss: 179.4519\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 132.92938\n",
      "Epoch 772/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 41.1000 - val_loss: 192.5038\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 132.92938\n",
      "Epoch 773/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 46.8773 - val_loss: 186.3624\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 132.92938\n",
      "Epoch 774/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 49.9264 - val_loss: 185.8579\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 132.92938\n",
      "Epoch 775/1000\n",
      "96/96 [==============================] - 0s 597us/step - loss: 42.3214 - val_loss: 209.4936\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 132.92938\n",
      "Epoch 776/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 40.1274 - val_loss: 190.0300\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 132.92938\n",
      "Epoch 777/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 47.4292 - val_loss: 179.4350\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 132.92938\n",
      "Epoch 778/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 44.8034 - val_loss: 212.2548\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 132.92938\n",
      "Epoch 779/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 58.0992 - val_loss: 204.5303\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 132.92938\n",
      "Epoch 780/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 53.3826 - val_loss: 200.5954\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 132.92938\n",
      "Epoch 781/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 50.8219 - val_loss: 206.9447\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 132.92938\n",
      "Epoch 782/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 50.6048 - val_loss: 200.2240\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 132.92938\n",
      "Epoch 783/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 50.0122 - val_loss: 195.7347\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 132.92938\n",
      "Epoch 784/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 67.5842 - val_loss: 200.0356\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 132.92938\n",
      "Epoch 785/1000\n",
      "96/96 [==============================] - 0s 591us/step - loss: 61.8833 - val_loss: 202.8051\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 132.92938\n",
      "Epoch 786/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 37.7112 - val_loss: 205.2434\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 132.92938\n",
      "Epoch 787/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 41.3078 - val_loss: 198.4970\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 132.92938\n",
      "Epoch 788/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 69.9972 - val_loss: 182.3414\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 132.92938\n",
      "Epoch 789/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 69.4334 - val_loss: 186.9263\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 132.92938\n",
      "Epoch 790/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 45.3609 - val_loss: 191.1241\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 132.92938\n",
      "Epoch 791/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 43.8048 - val_loss: 201.1970\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 132.92938\n",
      "Epoch 792/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 83.6235 - val_loss: 200.5307\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 132.92938\n",
      "Epoch 793/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 53.2498 - val_loss: 187.5174\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 132.92938\n",
      "Epoch 794/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 54.2514 - val_loss: 187.6922\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 132.92938\n",
      "Epoch 795/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 42.4694 - val_loss: 193.3886\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 132.92938\n",
      "Epoch 796/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 39.7452 - val_loss: 182.0599\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 132.92938\n",
      "Epoch 797/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 40.8629 - val_loss: 191.5161\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 132.92938\n",
      "Epoch 798/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 44.7826 - val_loss: 188.4315\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 132.92938\n",
      "Epoch 799/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 36.7953 - val_loss: 191.2053\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 132.92938\n",
      "Epoch 800/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 51.8620 - val_loss: 202.9968\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 132.92938\n",
      "Epoch 801/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 35.9513 - val_loss: 191.7769\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 132.92938\n",
      "Epoch 802/1000\n",
      "96/96 [==============================] - 0s 598us/step - loss: 36.5028 - val_loss: 183.7695\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 132.92938\n",
      "Epoch 803/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 40.9423 - val_loss: 182.9269\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 132.92938\n",
      "Epoch 804/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 49.0686 - val_loss: 180.8316\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 132.92938\n",
      "Epoch 805/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 74.3061 - val_loss: 180.8983\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 132.92938\n",
      "Epoch 806/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 629us/step - loss: 53.3962 - val_loss: 176.6355\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 132.92938\n",
      "Epoch 807/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 50.9243 - val_loss: 175.0098\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 132.92938\n",
      "Epoch 808/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 55.9747 - val_loss: 174.6010\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 132.92938\n",
      "Epoch 809/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 69.3365 - val_loss: 184.3459\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 132.92938\n",
      "Epoch 810/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 61.6366 - val_loss: 187.4262\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 132.92938\n",
      "Epoch 811/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 57.2636 - val_loss: 181.5602\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 132.92938\n",
      "Epoch 812/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 59.1980 - val_loss: 194.6587\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 132.92938\n",
      "Epoch 813/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 59.2874 - val_loss: 200.5883\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 132.92938\n",
      "Epoch 814/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 45.9228 - val_loss: 194.2590\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 132.92938\n",
      "Epoch 815/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 73.0298 - val_loss: 200.7965\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 132.92938\n",
      "Epoch 816/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 75.2696 - val_loss: 188.3454\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 132.92938\n",
      "Epoch 817/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 55.8452 - val_loss: 182.5371\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 132.92938\n",
      "Epoch 818/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 55.3787 - val_loss: 190.2216\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 132.92938\n",
      "Epoch 819/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 48.6895 - val_loss: 171.8228\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 132.92938\n",
      "Epoch 820/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 61.3838 - val_loss: 176.4250\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 132.92938\n",
      "Epoch 821/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 44.9019 - val_loss: 181.7572\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 132.92938\n",
      "Epoch 822/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 81.8927 - val_loss: 172.2032\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 132.92938\n",
      "Epoch 823/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 63.3270 - val_loss: 171.7398\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 132.92938\n",
      "Epoch 824/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 59.7346 - val_loss: 191.9382\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 132.92938\n",
      "Epoch 825/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 50.9881 - val_loss: 173.7207\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 132.92938\n",
      "Epoch 826/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 56.8725 - val_loss: 179.3842\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 132.92938\n",
      "Epoch 827/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 39.9363 - val_loss: 204.5945\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 132.92938\n",
      "Epoch 828/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 54.5094 - val_loss: 197.5530\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 132.92938\n",
      "Epoch 829/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 47.0032 - val_loss: 196.1132\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 132.92938\n",
      "Epoch 830/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.6183 - val_loss: 203.3001\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 132.92938\n",
      "Epoch 831/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 42.5223 - val_loss: 203.1341\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 132.92938\n",
      "Epoch 832/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 52.3250 - val_loss: 174.6847\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 132.92938\n",
      "Epoch 833/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 47.9046 - val_loss: 167.9948\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 132.92938\n",
      "Epoch 834/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 40.8544 - val_loss: 169.7878\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 132.92938\n",
      "Epoch 835/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 50.3185 - val_loss: 172.6684\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 132.92938\n",
      "Epoch 836/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 50.9678 - val_loss: 173.4756\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 132.92938\n",
      "Epoch 837/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 41.6004 - val_loss: 178.5457\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 132.92938\n",
      "Epoch 838/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 53.1335 - val_loss: 181.8082\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 132.92938\n",
      "Epoch 839/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 51.8475 - val_loss: 189.4775\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 132.92938\n",
      "Epoch 840/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 44.2890 - val_loss: 180.8479\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 132.92938\n",
      "Epoch 841/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 59.3957 - val_loss: 181.4032\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 132.92938\n",
      "Epoch 842/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 49.6075 - val_loss: 181.1639\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 132.92938\n",
      "Epoch 843/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 49.9242 - val_loss: 197.5570\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 132.92938\n",
      "Epoch 844/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 50.2799 - val_loss: 180.1205\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 132.92938\n",
      "Epoch 845/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 44.4587 - val_loss: 179.7994\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 132.92938\n",
      "Epoch 846/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 48.9424 - val_loss: 176.5734\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 132.92938\n",
      "Epoch 847/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 42.0492 - val_loss: 183.5095\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 132.92938\n",
      "Epoch 848/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 44.5154 - val_loss: 185.7004\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 132.92938\n",
      "Epoch 849/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 38.8524 - val_loss: 189.7731\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 132.92938\n",
      "Epoch 850/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 50.5137 - val_loss: 189.7058\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 132.92938\n",
      "Epoch 851/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 48.2961 - val_loss: 184.3696\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 132.92938\n",
      "Epoch 852/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 50.4475 - val_loss: 203.6660\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 132.92938\n",
      "Epoch 853/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 57.1744 - val_loss: 200.5406\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 132.92938\n",
      "Epoch 854/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 43.6913 - val_loss: 198.8190\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 132.92938\n",
      "Epoch 855/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 45.0403 - val_loss: 195.8174\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 132.92938\n",
      "Epoch 856/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 35.6423 - val_loss: 193.1148\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 132.92938\n",
      "Epoch 857/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 612us/step - loss: 47.3026 - val_loss: 185.7420\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 132.92938\n",
      "Epoch 858/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 60.3136 - val_loss: 187.7455\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 132.92938\n",
      "Epoch 859/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 56.2143 - val_loss: 187.1045\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 132.92938\n",
      "Epoch 860/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 42.5003 - val_loss: 209.0562\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 132.92938\n",
      "Epoch 861/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 72.0620 - val_loss: 187.9988\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 132.92938\n",
      "Epoch 862/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 49.1927 - val_loss: 182.1625\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 132.92938\n",
      "Epoch 863/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 48.5992 - val_loss: 187.9498\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 132.92938\n",
      "Epoch 864/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 48.6681 - val_loss: 181.2818\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 132.92938\n",
      "Epoch 865/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 61.0550 - val_loss: 177.0897\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 132.92938\n",
      "Epoch 866/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 55.4865 - val_loss: 179.1156\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 132.92938\n",
      "Epoch 867/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 55.1250 - val_loss: 173.3023\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 132.92938\n",
      "Epoch 868/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 77.5954 - val_loss: 178.3785\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 132.92938\n",
      "Epoch 869/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 52.7050 - val_loss: 174.2448\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 132.92938\n",
      "Epoch 870/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 45.6869 - val_loss: 171.7283\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 132.92938\n",
      "Epoch 871/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.8407 - val_loss: 173.4736\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 132.92938\n",
      "Epoch 872/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 72.6891 - val_loss: 170.7791\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 132.92938\n",
      "Epoch 873/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 43.0005 - val_loss: 169.5741\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 132.92938\n",
      "Epoch 874/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 58.0594 - val_loss: 171.8394\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 132.92938\n",
      "Epoch 875/1000\n",
      "96/96 [==============================] - 0s 661us/step - loss: 43.6943 - val_loss: 165.2663\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 132.92938\n",
      "Epoch 876/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 42.0715 - val_loss: 169.2101\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 132.92938\n",
      "Epoch 877/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 37.3404 - val_loss: 174.2379\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 132.92938\n",
      "Epoch 878/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 46.5235 - val_loss: 166.7170\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 132.92938\n",
      "Epoch 879/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 43.9753 - val_loss: 166.9109\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 132.92938\n",
      "Epoch 880/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 38.2091 - val_loss: 171.6782\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 132.92938\n",
      "Epoch 881/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 43.2739 - val_loss: 167.8215\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 132.92938\n",
      "Epoch 882/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 37.7441 - val_loss: 170.6417\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 132.92938\n",
      "Epoch 883/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 43.1594 - val_loss: 174.0159\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 132.92938\n",
      "Epoch 884/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 35.8798 - val_loss: 180.0737\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 132.92938\n",
      "Epoch 885/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 37.7560 - val_loss: 177.3711\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 132.92938\n",
      "Epoch 886/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 43.3759 - val_loss: 178.0008\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 132.92938\n",
      "Epoch 887/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 44.6270 - val_loss: 182.0566\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 132.92938\n",
      "Epoch 888/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 38.8397 - val_loss: 180.8874\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 132.92938\n",
      "Epoch 889/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 47.6986 - val_loss: 184.2353\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 132.92938\n",
      "Epoch 890/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 48.2875 - val_loss: 202.6301\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 132.92938\n",
      "Epoch 891/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 37.4328 - val_loss: 182.8048\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 132.92938\n",
      "Epoch 892/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 47.4923 - val_loss: 183.1669\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 132.92938\n",
      "Epoch 893/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 35.6231 - val_loss: 189.6539\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 132.92938\n",
      "Epoch 894/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 68.2629 - val_loss: 192.9570\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 132.92938\n",
      "Epoch 895/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 39.3548 - val_loss: 217.4246\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 132.92938\n",
      "Epoch 896/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 38.8780 - val_loss: 198.7302\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 132.92938\n",
      "Epoch 897/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 48.5109 - val_loss: 186.6396\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 132.92938\n",
      "Epoch 898/1000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 52.8725 - val_loss: 182.9575\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 132.92938\n",
      "Epoch 899/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 52.5504 - val_loss: 202.4878\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 132.92938\n",
      "Epoch 900/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 42.3738 - val_loss: 182.6403\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 132.92938\n",
      "Epoch 901/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 49.4079 - val_loss: 177.3807\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 132.92938\n",
      "Epoch 902/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 37.0966 - val_loss: 183.0350\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 132.92938\n",
      "Epoch 903/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 33.4895 - val_loss: 178.7049\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 132.92938\n",
      "Epoch 904/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 42.8374 - val_loss: 175.1671\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 132.92938\n",
      "Epoch 905/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 41.3243 - val_loss: 178.0317\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 132.92938\n",
      "Epoch 906/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 45.5786 - val_loss: 182.0339\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 132.92938\n",
      "Epoch 907/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 46.2497 - val_loss: 180.5366\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 132.92938\n",
      "Epoch 908/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 613us/step - loss: 52.1997 - val_loss: 216.3400\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 132.92938\n",
      "Epoch 909/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 57.1980 - val_loss: 184.2151\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 132.92938\n",
      "Epoch 910/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 62.9133 - val_loss: 183.8972\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 132.92938\n",
      "Epoch 911/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 48.9230 - val_loss: 179.8173\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 132.92938\n",
      "Epoch 912/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 41.9807 - val_loss: 179.9567\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 132.92938\n",
      "Epoch 913/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 41.7828 - val_loss: 184.2334\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 132.92938\n",
      "Epoch 914/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 43.2254 - val_loss: 183.1763\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 132.92938\n",
      "Epoch 915/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 43.3868 - val_loss: 179.8136\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 132.92938\n",
      "Epoch 916/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 37.8832 - val_loss: 179.8945\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 132.92938\n",
      "Epoch 917/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 54.2834 - val_loss: 174.3171\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 132.92938\n",
      "Epoch 918/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 53.3768 - val_loss: 183.6440\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 132.92938\n",
      "Epoch 919/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 57.8205 - val_loss: 179.3125\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 132.92938\n",
      "Epoch 920/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 71.7300 - val_loss: 174.8528\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 132.92938\n",
      "Epoch 921/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.0421 - val_loss: 174.4583\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 132.92938\n",
      "Epoch 922/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 38.8103 - val_loss: 177.6871\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 132.92938\n",
      "Epoch 923/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 42.4460 - val_loss: 184.7597\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 132.92938\n",
      "Epoch 924/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 51.4949 - val_loss: 186.1651\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 132.92938\n",
      "Epoch 925/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 35.4990 - val_loss: 185.3337\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 132.92938\n",
      "Epoch 926/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 53.9658 - val_loss: 169.8989\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 132.92938\n",
      "Epoch 927/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 40.1970 - val_loss: 172.7050\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 132.92938\n",
      "Epoch 928/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 37.8559 - val_loss: 175.6329\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 132.92938\n",
      "Epoch 929/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 42.9309 - val_loss: 173.0264\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 132.92938\n",
      "Epoch 930/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 37.2808 - val_loss: 174.9919\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 132.92938\n",
      "Epoch 931/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 36.7888 - val_loss: 172.6951\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 132.92938\n",
      "Epoch 932/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 43.2779 - val_loss: 175.8184\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 132.92938\n",
      "Epoch 933/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 43.1583 - val_loss: 179.9330\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 132.92938\n",
      "Epoch 934/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 44.4478 - val_loss: 194.3374\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 132.92938\n",
      "Epoch 935/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 42.0604 - val_loss: 204.5445\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 132.92938\n",
      "Epoch 936/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 41.7141 - val_loss: 191.9121\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 132.92938\n",
      "Epoch 937/1000\n",
      "96/96 [==============================] - 0s 649us/step - loss: 64.3120 - val_loss: 181.0683\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 132.92938\n",
      "Epoch 938/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 42.9999 - val_loss: 183.1811\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 132.92938\n",
      "Epoch 939/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 49.1415 - val_loss: 178.3978\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 132.92938\n",
      "Epoch 940/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 39.0616 - val_loss: 175.3641\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 132.92938\n",
      "Epoch 941/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 48.7841 - val_loss: 177.3657\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 132.92938\n",
      "Epoch 942/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 48.6630 - val_loss: 177.0540\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 132.92938\n",
      "Epoch 943/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 44.3819 - val_loss: 181.7688\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 132.92938\n",
      "Epoch 944/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 73.5094 - val_loss: 191.4678\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 132.92938\n",
      "Epoch 945/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 53.6932 - val_loss: 191.5047\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 132.92938\n",
      "Epoch 946/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 45.2244 - val_loss: 194.5609\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 132.92938\n",
      "Epoch 947/1000\n",
      "96/96 [==============================] - 0s 588us/step - loss: 59.7952 - val_loss: 196.9716\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 132.92938\n",
      "Epoch 948/1000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 68.7410 - val_loss: 215.3851\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 132.92938\n",
      "Epoch 949/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 42.5295 - val_loss: 208.7514\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 132.92938\n",
      "Epoch 950/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 51.4486 - val_loss: 183.0507\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 132.92938\n",
      "Epoch 951/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 41.1134 - val_loss: 180.2502\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 132.92938\n",
      "Epoch 952/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 45.2783 - val_loss: 183.4626\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 132.92938\n",
      "Epoch 953/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 42.2170 - val_loss: 181.1434\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 132.92938\n",
      "Epoch 954/1000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 44.4971 - val_loss: 198.0783\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 132.92938\n",
      "Epoch 955/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 43.0352 - val_loss: 220.4955\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 132.92938\n",
      "Epoch 956/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 39.7333 - val_loss: 212.0265\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 132.92938\n",
      "Epoch 957/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 44.3425 - val_loss: 214.8232\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 132.92938\n",
      "Epoch 958/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 41.8273 - val_loss: 194.3420\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 132.92938\n",
      "Epoch 959/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 629us/step - loss: 67.4775 - val_loss: 189.3867\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 132.92938\n",
      "Epoch 960/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 59.1395 - val_loss: 193.4916\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 132.92938\n",
      "Epoch 961/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 49.0093 - val_loss: 183.2703\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 132.92938\n",
      "Epoch 962/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 46.1078 - val_loss: 185.7465\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 132.92938\n",
      "Epoch 963/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 38.7137 - val_loss: 190.6364\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 132.92938\n",
      "Epoch 964/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 38.4648 - val_loss: 185.3187\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 132.92938\n",
      "Epoch 965/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 34.4137 - val_loss: 185.6229\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 132.92938\n",
      "Epoch 966/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 46.8278 - val_loss: 184.0707\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 132.92938\n",
      "Epoch 967/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 42.7515 - val_loss: 188.3813\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 132.92938\n",
      "Epoch 968/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 43.8478 - val_loss: 191.9305\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 132.92938\n",
      "Epoch 969/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 37.4893 - val_loss: 197.4246\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 132.92938\n",
      "Epoch 970/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 39.0589 - val_loss: 188.5913\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 132.92938\n",
      "Epoch 971/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 43.6050 - val_loss: 181.5692\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 132.92938\n",
      "Epoch 972/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.4725 - val_loss: 194.9868\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 132.92938\n",
      "Epoch 973/1000\n",
      "96/96 [==============================] - 0s 651us/step - loss: 59.4762 - val_loss: 206.6154\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 132.92938\n",
      "Epoch 974/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 52.2250 - val_loss: 207.8372\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 132.92938\n",
      "Epoch 975/1000\n",
      "96/96 [==============================] - 0s 662us/step - loss: 50.9013 - val_loss: 211.1666\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 132.92938\n",
      "Epoch 976/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 70.5780 - val_loss: 222.9587\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 132.92938\n",
      "Epoch 977/1000\n",
      "96/96 [==============================] - 0s 595us/step - loss: 45.0397 - val_loss: 188.6914\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 132.92938\n",
      "Epoch 978/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 37.0033 - val_loss: 184.3951\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 132.92938\n",
      "Epoch 979/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 48.5412 - val_loss: 184.4072\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 132.92938\n",
      "Epoch 980/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 46.0335 - val_loss: 188.7602\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 132.92938\n",
      "Epoch 981/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 55.8631 - val_loss: 192.1262\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 132.92938\n",
      "Epoch 982/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 36.1737 - val_loss: 204.2463\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 132.92938\n",
      "Epoch 983/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 48.9415 - val_loss: 208.6624\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 132.92938\n",
      "Epoch 984/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 43.0103 - val_loss: 210.1167\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 132.92938\n",
      "Epoch 985/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 70.8197 - val_loss: 197.5032\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 132.92938\n",
      "Epoch 986/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 49.0804 - val_loss: 218.7313\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 132.92938\n",
      "Epoch 987/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 57.5887 - val_loss: 186.9939\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 132.92938\n",
      "Epoch 988/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 50.4416 - val_loss: 200.6787\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 132.92938\n",
      "Epoch 989/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 42.7893 - val_loss: 206.0073\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 132.92938\n",
      "Epoch 990/1000\n",
      "96/96 [==============================] - 0s 663us/step - loss: 50.6067 - val_loss: 206.0202\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 132.92938\n",
      "Epoch 991/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 42.7705 - val_loss: 220.7909\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 132.92938\n",
      "Epoch 992/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 46.7814 - val_loss: 195.5867\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 132.92938\n",
      "Epoch 993/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 36.5502 - val_loss: 200.0723\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 132.92938\n",
      "Epoch 994/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 51.8766 - val_loss: 203.3881\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 132.92938\n",
      "Epoch 995/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 47.4961 - val_loss: 193.5690\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 132.92938\n",
      "Epoch 996/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 49.3409 - val_loss: 213.9148\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 132.92938\n",
      "Epoch 997/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 58.2276 - val_loss: 218.6853\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 132.92938\n",
      "Epoch 998/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 37.4610 - val_loss: 224.0775\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 132.92938\n",
      "Epoch 999/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 54.2433 - val_loss: 209.7319\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 132.92938\n",
      "Epoch 1000/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 51.4121 - val_loss: 221.2864\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 132.92938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69c467aa90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "vae.fit(x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None), \n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_output = sess.run( z_mean_std, feed_dict={'encoder_input: 0':x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00054633, 0.02389398], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m = np.mean(z_output,axis=0)\n",
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01052657e+00, 6.28591109e-09],\n",
       "       [6.28591109e-09, 1.01052615e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(z_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Pearson correlation among Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f69d47f7a58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(np.corrcoef(z_output.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00054633, 0.02389398], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, latent_dim):\n",
    "    plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num = i,\n",
    "                 z_m_m = z_m_m ,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
