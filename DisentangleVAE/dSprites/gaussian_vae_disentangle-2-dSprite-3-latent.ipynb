{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7fe913b1bd30>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2376x2376 with 121 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz'\n",
    "                      , encoding='bytes')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]\n",
    "\n",
    "# Define number of values per latents and functions to convert to indices\n",
    "latents_sizes =  np.array([ 1,  3,  6, 40, 32, 32])\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "  return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "# Helper function to show images\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "  ncols = int(np.ceil(num_images**0.5))\n",
    "  nrows = int(np.ceil(num_images / ncols))\n",
    "  _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for ax_i, ax in enumerate(axes):\n",
    "    if ax_i < num_images:\n",
    "      ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "    else:\n",
    "      ax.axis('off')\n",
    "\n",
    "def show_density(imgs):\n",
    "  _, ax = plt.subplots()\n",
    "  ax.imshow(imgs.mean(axis=0), interpolation='nearest', cmap='Greys_r')\n",
    "  ax.grid('off')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "from copy import deepcopy\n",
    "## Fix posX latent to left\n",
    "#latents_sampled = sample_latent(size=5000)\n",
    "latents_sampled = deepcopy(latents_classes)\n",
    "latents_sampled[:, [4,5]] = 15\n",
    "latents_sampled[:,2]= 5\n",
    "\n",
    "\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[np.unique(indices_sampled)]\n",
    "\n",
    "#np.unique(indices_sampled)\n",
    "\n",
    "# Samples\n",
    "show_images_grid(imgs_sampled,len(np.unique(indices_sampled)))\n",
    "\n",
    "# Show the density too to check\n",
    "show_density(imgs_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(69)\n",
    "ff = imgs_sampled\n",
    "n_data =  ff.shape[0]\n",
    "n_train = int(np.ceil(n_data*0.8))\n",
    "\n",
    "print(n_train)\n",
    "idx_train = random.sample(range(n_data), n_train)\n",
    "idx_test = np.delete(range(n_data),idx_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   6,  14,  15,  20,  22,  23,  24,  28,  32,  43,  46,\n",
       "        51,  55,  67,  85,  92,  93,  96, 104, 105, 110, 118])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape[0]*0.8\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train / validation folds\n",
    "#np.random.seed(42)\n",
    "\n",
    "img_rows = ff.shape[1]\n",
    "img_cols = ff.shape[2]\n",
    "\n",
    "n_pixels = img_rows * img_cols\n",
    "x_train = ff[idx_train]\n",
    "x_test = ff[idx_test]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32') \n",
    "x_train = x_train.reshape((len(x_train), n_pixels))\n",
    "x_test = x_test.reshape((len(x_test), n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4608x4608 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_examples(data, n=None, n_cols=20, thumbnail_cb=None):\n",
    "    if n is None:\n",
    "        n = len(data)    \n",
    "    n_rows = int(np.ceil(n / float(n_cols)))\n",
    "    figure = np.zeros((img_rows * n_rows, img_cols * n_cols))\n",
    "    for k, x in enumerate(data[:n]):\n",
    "        r = k // n_cols\n",
    "        c = k % n_cols\n",
    "        figure[r * img_rows: (r + 1) * img_rows,\n",
    "               c * img_cols: (c + 1) * img_cols] = x\n",
    "        if thumbnail_cb is not None:\n",
    "            thumbnail_cb(locals())\n",
    "        \n",
    "    plt.figure(figsize=(64, 64))\n",
    "    plt.imshow(figure)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "show_examples(ff, n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "#     return z_mean + K.exp(0.5 ) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num,\n",
    "                 z_m_m, \n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_face\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    #x_test = data\n",
    "    latent_dim = latent_dim\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(model_name, \"face_over_latent.png\")\n",
    "    n = 20\n",
    "    #digit_size = 28\n",
    "    img_rows, img_cols = 64, 64\n",
    "    figure = np.zeros((img_rows , img_cols * n))\n",
    "    grid_x = np.linspace(-5, 5, n)\n",
    "    #grid_x = np.linspace(-2, 2, n)\n",
    "    \n",
    "    #grid_y = np.linspace(-5, 5, n)[::-1]\n",
    "    z_sample = np.zeros((1,latent_dim))\n",
    "    z_sample[0,:] = z_m_m \n",
    "    \n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample[0,latent_num] = xi\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_rows, img_cols)\n",
    "        figure[0: img_rows,j * img_cols: (j + 1) * img_cols] = digit\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #start_range = digit_size // 2\n",
    "    #end_range = n * digit_size + start_range + 1\n",
    "    #pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    #sample_range_x = np.round(grid_x, 1)\n",
    "    #sample_range_y = np.round(grid_y, 1)\n",
    "    #plt.xticks(pixel_range, sample_range_x)\n",
    "    #plt.yticks(pixel_range, sample_range_y)\n",
    "    #plt.xlabel(\"z[0]\")\n",
    "    #plt.ylabel(\"z[1]\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "original_dim = n_pixels\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim1 = 512\n",
    "intermediate_dim2 = 256\n",
    "intermediate_dim3 = 64\n",
    "\n",
    "batch_size = 20\n",
    "latent_dim = 3\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x1)\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(x2)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x3)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the mean of z, so that mean(m_z)=0 and cov(m_z)=I\n",
    "def standardize(z_mean):\n",
    "    z_m_m = K.mean(z_mean,axis=0, keepdims=True)\n",
    "    z1 = z_mean - z_m_m\n",
    "    n = tf.cast(K.shape(z_mean)[0], tf.float32)\n",
    "    cov = K.transpose(z1) @ z1 /n\n",
    "    \n",
    "    D = tf.diag(tf.diag_part(cov)) ** 0.5\n",
    "    \n",
    "    L = tf.linalg.inv(tf.transpose(tf.cholesky(cov)))\n",
    "       \n",
    "#     z2 = z1 @ L @ D +  z_m_m\n",
    "    z2 = z1 @ L  +  z_m_m\n",
    "\n",
    "    return( z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_std = Lambda(standardize, output_shape=(latent_dim,), name='z_mean_std')(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 3)            195         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 3)            195         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean_std (Lambda)             (None, 3)            0           z_mean[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 3)            0           z_mean_std[0][0]                 \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,245,830\n",
      "Trainable params: 2,245,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "#z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_std, z_log_var])\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 2,249,728\n",
      "Trainable params: 2,249,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z')\n",
    "\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(latent_inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x3)\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(x2)\n",
    "\n",
    "# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x1)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 3), (None, 3), (N 2245830   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 4096)              2249728   \n",
      "=================================================================\n",
      "Total params: 4,495,558\n",
      "Trainable params: 4,495,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test )\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "\n",
    "#     reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(1E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/1000\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 2686.1944 - val_loss: 1825.6948\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1825.69478, saving model to weights.hdf5\n",
      "Epoch 2/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 1134.4837 - val_loss: 413.8739\n",
      "\n",
      "Epoch 00002: val_loss improved from 1825.69478 to 413.87394, saving model to weights.hdf5\n",
      "Epoch 3/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 328.0602 - val_loss: 294.8328\n",
      "\n",
      "Epoch 00003: val_loss improved from 413.87394 to 294.83282, saving model to weights.hdf5\n",
      "Epoch 4/1000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 262.5441 - val_loss: 265.1851\n",
      "\n",
      "Epoch 00004: val_loss improved from 294.83282 to 265.18515, saving model to weights.hdf5\n",
      "Epoch 5/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 244.8803 - val_loss: 252.6741\n",
      "\n",
      "Epoch 00005: val_loss improved from 265.18515 to 252.67405, saving model to weights.hdf5\n",
      "Epoch 6/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 230.0676 - val_loss: 236.1677\n",
      "\n",
      "Epoch 00006: val_loss improved from 252.67405 to 236.16772, saving model to weights.hdf5\n",
      "Epoch 7/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 223.5024 - val_loss: 235.1213\n",
      "\n",
      "Epoch 00007: val_loss improved from 236.16772 to 235.12133, saving model to weights.hdf5\n",
      "Epoch 8/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 220.9832 - val_loss: 226.1390\n",
      "\n",
      "Epoch 00008: val_loss improved from 235.12133 to 226.13904, saving model to weights.hdf5\n",
      "Epoch 9/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 216.5453 - val_loss: 232.0184\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 226.13904\n",
      "Epoch 10/1000\n",
      "96/96 [==============================] - 0s 585us/step - loss: 215.9306 - val_loss: 226.0551\n",
      "\n",
      "Epoch 00010: val_loss improved from 226.13904 to 226.05515, saving model to weights.hdf5\n",
      "Epoch 11/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 213.7051 - val_loss: 221.9339\n",
      "\n",
      "Epoch 00011: val_loss improved from 226.05515 to 221.93392, saving model to weights.hdf5\n",
      "Epoch 12/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 210.0699 - val_loss: 219.0843\n",
      "\n",
      "Epoch 00012: val_loss improved from 221.93392 to 219.08433, saving model to weights.hdf5\n",
      "Epoch 13/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 207.2151 - val_loss: 218.1113\n",
      "\n",
      "Epoch 00013: val_loss improved from 219.08433 to 218.11132, saving model to weights.hdf5\n",
      "Epoch 14/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 202.8889 - val_loss: 213.9385\n",
      "\n",
      "Epoch 00014: val_loss improved from 218.11132 to 213.93851, saving model to weights.hdf5\n",
      "Epoch 15/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 200.7538 - val_loss: 215.5862\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 213.93851\n",
      "Epoch 16/1000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 195.8612 - val_loss: 210.4817\n",
      "\n",
      "Epoch 00016: val_loss improved from 213.93851 to 210.48173, saving model to weights.hdf5\n",
      "Epoch 17/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 190.2013 - val_loss: 203.7261\n",
      "\n",
      "Epoch 00017: val_loss improved from 210.48173 to 203.72612, saving model to weights.hdf5\n",
      "Epoch 18/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 186.4941 - val_loss: 202.9930\n",
      "\n",
      "Epoch 00018: val_loss improved from 203.72612 to 202.99297, saving model to weights.hdf5\n",
      "Epoch 19/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 182.6928 - val_loss: 205.6652\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 202.99297\n",
      "Epoch 20/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 177.8691 - val_loss: 196.7652\n",
      "\n",
      "Epoch 00020: val_loss improved from 202.99297 to 196.76519, saving model to weights.hdf5\n",
      "Epoch 21/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 175.7224 - val_loss: 200.3987\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 196.76519\n",
      "Epoch 22/1000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 171.5817 - val_loss: 192.0365\n",
      "\n",
      "Epoch 00022: val_loss improved from 196.76519 to 192.03646, saving model to weights.hdf5\n",
      "Epoch 23/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 172.4493 - val_loss: 202.0835\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 192.03646\n",
      "Epoch 24/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 173.0447 - val_loss: 202.3281\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 192.03646\n",
      "Epoch 25/1000\n",
      "96/96 [==============================] - 0s 597us/step - loss: 163.5413 - val_loss: 202.8481\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 192.03646\n",
      "Epoch 26/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 165.2730 - val_loss: 199.4560\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 192.03646\n",
      "Epoch 27/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 163.1736 - val_loss: 199.1767\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 192.03646\n",
      "Epoch 28/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 163.9196 - val_loss: 188.2276\n",
      "\n",
      "Epoch 00028: val_loss improved from 192.03646 to 188.22761, saving model to weights.hdf5\n",
      "Epoch 29/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 165.0458 - val_loss: 198.9428\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 188.22761\n",
      "Epoch 30/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 159.2904 - val_loss: 187.9516\n",
      "\n",
      "Epoch 00030: val_loss improved from 188.22761 to 187.95156, saving model to weights.hdf5\n",
      "Epoch 31/1000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 155.9634 - val_loss: 187.7470\n",
      "\n",
      "Epoch 00031: val_loss improved from 187.95156 to 187.74700, saving model to weights.hdf5\n",
      "Epoch 32/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 148.6665 - val_loss: 188.8631\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 187.74700\n",
      "Epoch 33/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 144.0675 - val_loss: 185.9132\n",
      "\n",
      "Epoch 00033: val_loss improved from 187.74700 to 185.91319, saving model to weights.hdf5\n",
      "Epoch 34/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 145.3375 - val_loss: 185.9162\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 185.91319\n",
      "Epoch 35/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 143.8736 - val_loss: 181.0108\n",
      "\n",
      "Epoch 00035: val_loss improved from 185.91319 to 181.01084, saving model to weights.hdf5\n",
      "Epoch 36/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 140.6946 - val_loss: 194.3343\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 181.01084\n",
      "Epoch 37/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 145.9435 - val_loss: 192.7934\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 181.01084\n",
      "Epoch 38/1000\n",
      "96/96 [==============================] - 0s 587us/step - loss: 146.4198 - val_loss: 192.9960\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 181.01084\n",
      "Epoch 39/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 141.5055 - val_loss: 186.5530\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 181.01084\n",
      "Epoch 40/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 137.8530 - val_loss: 182.6296\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 181.01084\n",
      "Epoch 41/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 136.4858 - val_loss: 179.9784\n",
      "\n",
      "Epoch 00041: val_loss improved from 181.01084 to 179.97845, saving model to weights.hdf5\n",
      "Epoch 42/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 143.4344 - val_loss: 190.1825\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 179.97845\n",
      "Epoch 43/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 137.6966 - val_loss: 173.5001\n",
      "\n",
      "Epoch 00043: val_loss improved from 179.97845 to 173.50015, saving model to weights.hdf5\n",
      "Epoch 44/1000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 131.7487 - val_loss: 177.7988\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 173.50015\n",
      "Epoch 45/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 131.1735 - val_loss: 180.6151\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 173.50015\n",
      "Epoch 46/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 137.6766 - val_loss: 182.0959\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 173.50015\n",
      "Epoch 47/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 128.3019 - val_loss: 179.7824\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 173.50015\n",
      "Epoch 48/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 132.0501 - val_loss: 180.7996\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 173.50015\n",
      "Epoch 49/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 128.7603 - val_loss: 182.0248\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 173.50015\n",
      "Epoch 50/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 128.0604 - val_loss: 184.7865\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 173.50015\n",
      "Epoch 51/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 129.6025 - val_loss: 188.8058\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 173.50015\n",
      "Epoch 52/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 125.7989 - val_loss: 187.8483\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 173.50015\n",
      "Epoch 53/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 128.2405 - val_loss: 188.5846\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 173.50015\n",
      "Epoch 54/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 130.8504 - val_loss: 176.3284\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 173.50015\n",
      "Epoch 55/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 126.2723 - val_loss: 173.3245\n",
      "\n",
      "Epoch 00055: val_loss improved from 173.50015 to 173.32445, saving model to weights.hdf5\n",
      "Epoch 56/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 124.9519 - val_loss: 184.9596\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 173.32445\n",
      "Epoch 57/1000\n",
      "96/96 [==============================] - 0s 584us/step - loss: 124.5921 - val_loss: 177.9976\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 173.32445\n",
      "Epoch 58/1000\n",
      "96/96 [==============================] - 0s 587us/step - loss: 119.4244 - val_loss: 173.2951\n",
      "\n",
      "Epoch 00058: val_loss improved from 173.32445 to 173.29508, saving model to weights.hdf5\n",
      "Epoch 59/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 119.8158 - val_loss: 178.2959\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 173.29508\n",
      "Epoch 60/1000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 121.3648 - val_loss: 173.1890\n",
      "\n",
      "Epoch 00060: val_loss improved from 173.29508 to 173.18903, saving model to weights.hdf5\n",
      "Epoch 61/1000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 119.7992 - val_loss: 171.0199\n",
      "\n",
      "Epoch 00061: val_loss improved from 173.18903 to 171.01988, saving model to weights.hdf5\n",
      "Epoch 62/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 117.3033 - val_loss: 175.6788\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 171.01988\n",
      "Epoch 63/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 117.1307 - val_loss: 181.0431\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 171.01988\n",
      "Epoch 64/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 117.9713 - val_loss: 177.8148\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 171.01988\n",
      "Epoch 65/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 120.3674 - val_loss: 174.2055\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 171.01988\n",
      "Epoch 66/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 125.2444 - val_loss: 171.8241\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 171.01988\n",
      "Epoch 67/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 115.7375 - val_loss: 172.4790\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 171.01988\n",
      "Epoch 68/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 112.5900 - val_loss: 159.1723\n",
      "\n",
      "Epoch 00068: val_loss improved from 171.01988 to 159.17225, saving model to weights.hdf5\n",
      "Epoch 69/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 112.5541 - val_loss: 176.3525\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 159.17225\n",
      "Epoch 70/1000\n",
      "96/96 [==============================] - 0s 578us/step - loss: 109.4203 - val_loss: 170.9891\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 159.17225\n",
      "Epoch 71/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 122.8001 - val_loss: 157.1357\n",
      "\n",
      "Epoch 00071: val_loss improved from 159.17225 to 157.13570, saving model to weights.hdf5\n",
      "Epoch 72/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 109.1578 - val_loss: 164.8064\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 157.13570\n",
      "Epoch 73/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 109.3565 - val_loss: 172.7996\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 157.13570\n",
      "Epoch 74/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 107.2106 - val_loss: 164.2382\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 157.13570\n",
      "Epoch 75/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 110.0973 - val_loss: 148.4926\n",
      "\n",
      "Epoch 00075: val_loss improved from 157.13570 to 148.49255, saving model to weights.hdf5\n",
      "Epoch 76/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 102.3457 - val_loss: 168.1479\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 148.49255\n",
      "Epoch 77/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 106.1408 - val_loss: 164.5915\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 148.49255\n",
      "Epoch 78/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 100.4910 - val_loss: 167.2393\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 148.49255\n",
      "Epoch 79/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 101.0361 - val_loss: 165.8580\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 148.49255\n",
      "Epoch 80/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 103.8838 - val_loss: 168.2185\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 148.49255\n",
      "Epoch 81/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 103.2901 - val_loss: 167.5278\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 148.49255\n",
      "Epoch 82/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 102.2296 - val_loss: 152.4780\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 148.49255\n",
      "Epoch 83/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 102.2558 - val_loss: 162.5988\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 148.49255\n",
      "Epoch 84/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 102.6507 - val_loss: 157.0000\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 148.49255\n",
      "Epoch 85/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 97.2948 - val_loss: 166.8506\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 148.49255\n",
      "Epoch 86/1000\n",
      "96/96 [==============================] - 0s 588us/step - loss: 98.8208 - val_loss: 160.8406\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 148.49255\n",
      "Epoch 87/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 95.8913 - val_loss: 164.1073\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 148.49255\n",
      "Epoch 88/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 95.7630 - val_loss: 164.2171\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 148.49255\n",
      "Epoch 89/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 94.5843 - val_loss: 157.9390\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 148.49255\n",
      "Epoch 90/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 95.1863 - val_loss: 164.9788\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 148.49255\n",
      "Epoch 91/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 97.5773 - val_loss: 164.8253\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 148.49255\n",
      "Epoch 92/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 97.6866 - val_loss: 175.4207\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 148.49255\n",
      "Epoch 93/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 103.3512 - val_loss: 166.1461\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 148.49255\n",
      "Epoch 94/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 106.5687 - val_loss: 161.8955\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 148.49255\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 610us/step - loss: 100.9497 - val_loss: 181.0987\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 148.49255\n",
      "Epoch 96/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 92.9241 - val_loss: 175.8358\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 148.49255\n",
      "Epoch 97/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 94.6278 - val_loss: 162.7617\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 148.49255\n",
      "Epoch 98/1000\n",
      "96/96 [==============================] - 0s 657us/step - loss: 95.6146 - val_loss: 173.7194\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 148.49255\n",
      "Epoch 99/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 94.3244 - val_loss: 164.7033\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 148.49255\n",
      "Epoch 100/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 85.5145 - val_loss: 164.9605\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 148.49255\n",
      "Epoch 101/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 85.0333 - val_loss: 161.1667\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 148.49255\n",
      "Epoch 102/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 88.6139 - val_loss: 163.1525\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 148.49255\n",
      "Epoch 103/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 83.5478 - val_loss: 165.9710\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 148.49255\n",
      "Epoch 104/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 84.9759 - val_loss: 170.0156\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 148.49255\n",
      "Epoch 105/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 83.5812 - val_loss: 160.2380\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 148.49255\n",
      "Epoch 106/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 90.4611 - val_loss: 174.9565\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 148.49255\n",
      "Epoch 107/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 89.4659 - val_loss: 165.4075\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 148.49255\n",
      "Epoch 108/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 85.0278 - val_loss: 168.3233\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 148.49255\n",
      "Epoch 109/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 88.4615 - val_loss: 152.5245\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 148.49255\n",
      "Epoch 110/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 86.2516 - val_loss: 161.7236\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 148.49255\n",
      "Epoch 111/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 81.4847 - val_loss: 168.2850\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 148.49255\n",
      "Epoch 112/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 80.1140 - val_loss: 162.9335\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 148.49255\n",
      "Epoch 113/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 89.0751 - val_loss: 152.7719\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 148.49255\n",
      "Epoch 114/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 86.8081 - val_loss: 162.5801\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 148.49255\n",
      "Epoch 115/1000\n",
      "96/96 [==============================] - 0s 657us/step - loss: 87.8968 - val_loss: 166.9353\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 148.49255\n",
      "Epoch 116/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 85.4030 - val_loss: 163.6337\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 148.49255\n",
      "Epoch 117/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 91.4234 - val_loss: 158.4965\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 148.49255\n",
      "Epoch 118/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 87.1391 - val_loss: 151.1774\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 148.49255\n",
      "Epoch 119/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 89.9907 - val_loss: 160.4993\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 148.49255\n",
      "Epoch 120/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 84.8625 - val_loss: 153.2076\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 148.49255\n",
      "Epoch 121/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 84.0841 - val_loss: 157.1941\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 148.49255\n",
      "Epoch 122/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 97.0011 - val_loss: 167.3980\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 148.49255\n",
      "Epoch 123/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 85.5630 - val_loss: 150.6412\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 148.49255\n",
      "Epoch 124/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 81.8090 - val_loss: 149.7629\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 148.49255\n",
      "Epoch 125/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 79.2482 - val_loss: 157.4103\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 148.49255\n",
      "Epoch 126/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 91.4025 - val_loss: 159.6568\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 148.49255\n",
      "Epoch 127/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 87.6562 - val_loss: 156.9972\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 148.49255\n",
      "Epoch 128/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 79.8601 - val_loss: 172.0640\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 148.49255\n",
      "Epoch 129/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 87.1151 - val_loss: 145.2842\n",
      "\n",
      "Epoch 00129: val_loss improved from 148.49255 to 145.28416, saving model to weights.hdf5\n",
      "Epoch 130/1000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 79.8581 - val_loss: 158.2002\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 145.28416\n",
      "Epoch 131/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 75.6600 - val_loss: 161.0288\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 145.28416\n",
      "Epoch 132/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 81.8796 - val_loss: 161.1355\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 145.28416\n",
      "Epoch 133/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 82.4115 - val_loss: 143.0299\n",
      "\n",
      "Epoch 00133: val_loss improved from 145.28416 to 143.02985, saving model to weights.hdf5\n",
      "Epoch 134/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 76.5550 - val_loss: 158.8337\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 143.02985\n",
      "Epoch 135/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 77.2515 - val_loss: 156.0352\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 143.02985\n",
      "Epoch 136/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 74.3735 - val_loss: 146.3653\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 143.02985\n",
      "Epoch 137/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 78.2587 - val_loss: 159.0022\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 143.02985\n",
      "Epoch 138/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 80.3029 - val_loss: 169.9157\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 143.02985\n",
      "Epoch 139/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 80.2488 - val_loss: 151.7575\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 143.02985\n",
      "Epoch 140/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 85.2209 - val_loss: 178.9811\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 143.02985\n",
      "Epoch 141/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 85.0359 - val_loss: 174.0032\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 143.02985\n",
      "Epoch 142/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 87.2351 - val_loss: 174.5260\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 143.02985\n",
      "Epoch 143/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 88.1229 - val_loss: 157.2544\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 143.02985\n",
      "Epoch 144/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 87.0349 - val_loss: 156.5859\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 143.02985\n",
      "Epoch 145/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 84.4788 - val_loss: 158.2823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00145: val_loss did not improve from 143.02985\n",
      "Epoch 146/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 79.3201 - val_loss: 158.1461\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 143.02985\n",
      "Epoch 147/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 84.3541 - val_loss: 168.9736\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 143.02985\n",
      "Epoch 148/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 79.6067 - val_loss: 167.3096\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 143.02985\n",
      "Epoch 149/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 79.0426 - val_loss: 178.6101\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 143.02985\n",
      "Epoch 150/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 74.5755 - val_loss: 163.9495\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 143.02985\n",
      "Epoch 151/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 72.9592 - val_loss: 162.9502\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 143.02985\n",
      "Epoch 152/1000\n",
      "96/96 [==============================] - 0s 595us/step - loss: 77.5972 - val_loss: 171.5675\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 143.02985\n",
      "Epoch 153/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 77.7723 - val_loss: 183.9078\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 143.02985\n",
      "Epoch 154/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 73.1799 - val_loss: 154.2367\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 143.02985\n",
      "Epoch 155/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 82.6531 - val_loss: 153.8090\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 143.02985\n",
      "Epoch 156/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 73.6798 - val_loss: 158.6266\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 143.02985\n",
      "Epoch 157/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 80.7612 - val_loss: 151.1145\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 143.02985\n",
      "Epoch 158/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 76.1381 - val_loss: 158.8148\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 143.02985\n",
      "Epoch 159/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 79.0640 - val_loss: 173.1528\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 143.02985\n",
      "Epoch 160/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 71.5415 - val_loss: 158.7546\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 143.02985\n",
      "Epoch 161/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 68.6125 - val_loss: 159.4529\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 143.02985\n",
      "Epoch 162/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 70.5158 - val_loss: 166.3276\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 143.02985\n",
      "Epoch 163/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 73.7930 - val_loss: 155.8721\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 143.02985\n",
      "Epoch 164/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 67.2640 - val_loss: 163.2139\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 143.02985\n",
      "Epoch 165/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 80.4467 - val_loss: 160.4882\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 143.02985\n",
      "Epoch 166/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 67.5786 - val_loss: 148.6743\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 143.02985\n",
      "Epoch 167/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 75.1198 - val_loss: 151.6884\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 143.02985\n",
      "Epoch 168/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 72.4358 - val_loss: 149.1748\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 143.02985\n",
      "Epoch 169/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 70.2375 - val_loss: 151.4185\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 143.02985\n",
      "Epoch 170/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 69.1706 - val_loss: 155.0435\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 143.02985\n",
      "Epoch 171/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 72.1955 - val_loss: 146.6006\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 143.02985\n",
      "Epoch 172/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 76.7739 - val_loss: 153.6726\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 143.02985\n",
      "Epoch 173/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 84.9144 - val_loss: 143.3536\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 143.02985\n",
      "Epoch 174/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 81.8676 - val_loss: 186.1326\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 143.02985\n",
      "Epoch 175/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 102.0354 - val_loss: 160.2605\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 143.02985\n",
      "Epoch 176/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 74.6937 - val_loss: 158.4609\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 143.02985\n",
      "Epoch 177/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 76.5823 - val_loss: 175.1900\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 143.02985\n",
      "Epoch 178/1000\n",
      "96/96 [==============================] - 0s 653us/step - loss: 72.2636 - val_loss: 165.5454\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 143.02985\n",
      "Epoch 179/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 73.4761 - val_loss: 164.3397\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 143.02985\n",
      "Epoch 180/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 70.3656 - val_loss: 162.8816\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 143.02985\n",
      "Epoch 181/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 71.2608 - val_loss: 166.1309\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 143.02985\n",
      "Epoch 182/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 80.8487 - val_loss: 161.0785\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 143.02985\n",
      "Epoch 183/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 65.8901 - val_loss: 164.5927\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 143.02985\n",
      "Epoch 184/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 66.2869 - val_loss: 158.0411\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 143.02985\n",
      "Epoch 185/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 70.7478 - val_loss: 152.0174\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 143.02985\n",
      "Epoch 186/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 68.8513 - val_loss: 157.5809\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 143.02985\n",
      "Epoch 187/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 79.5565 - val_loss: 159.0995\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 143.02985\n",
      "Epoch 188/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 66.9175 - val_loss: 166.4330\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 143.02985\n",
      "Epoch 189/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 75.8027 - val_loss: 158.4278\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 143.02985\n",
      "Epoch 190/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 75.5679 - val_loss: 160.7269\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 143.02985\n",
      "Epoch 191/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 66.6652 - val_loss: 154.7266\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 143.02985\n",
      "Epoch 192/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 66.2267 - val_loss: 144.1364\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 143.02985\n",
      "Epoch 193/1000\n",
      "96/96 [==============================] - 0s 658us/step - loss: 65.2194 - val_loss: 160.9730\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 143.02985\n",
      "Epoch 194/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 66.0401 - val_loss: 143.5379\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 143.02985\n",
      "Epoch 195/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 65.5610 - val_loss: 145.6677\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 143.02985\n",
      "Epoch 196/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 67.4801 - val_loss: 158.3190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00196: val_loss did not improve from 143.02985\n",
      "Epoch 197/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 64.7976 - val_loss: 169.6975\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 143.02985\n",
      "Epoch 198/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 69.0616 - val_loss: 158.1995\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 143.02985\n",
      "Epoch 199/1000\n",
      "96/96 [==============================] - 0s 689us/step - loss: 62.1678 - val_loss: 169.3780\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 143.02985\n",
      "Epoch 200/1000\n",
      "96/96 [==============================] - 0s 580us/step - loss: 66.4284 - val_loss: 147.3550\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 143.02985\n",
      "Epoch 201/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 69.1051 - val_loss: 167.1759\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 143.02985\n",
      "Epoch 202/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 68.2812 - val_loss: 132.9267\n",
      "\n",
      "Epoch 00202: val_loss improved from 143.02985 to 132.92675, saving model to weights.hdf5\n",
      "Epoch 203/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 70.0864 - val_loss: 135.4511\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 132.92675\n",
      "Epoch 204/1000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 70.7835 - val_loss: 151.4665\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 132.92675\n",
      "Epoch 205/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 68.4668 - val_loss: 145.7489\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 132.92675\n",
      "Epoch 206/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 71.7517 - val_loss: 168.6577\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 132.92675\n",
      "Epoch 207/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 65.3649 - val_loss: 142.1624\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 132.92675\n",
      "Epoch 208/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 65.8779 - val_loss: 147.5486\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 132.92675\n",
      "Epoch 209/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 59.7743 - val_loss: 151.1121\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 132.92675\n",
      "Epoch 210/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 63.6197 - val_loss: 159.0494\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 132.92675\n",
      "Epoch 211/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 61.0470 - val_loss: 154.9571\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 132.92675\n",
      "Epoch 212/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 62.1102 - val_loss: 166.9657\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 132.92675\n",
      "Epoch 213/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 73.5573 - val_loss: 146.3166\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 132.92675\n",
      "Epoch 214/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 66.6418 - val_loss: 158.9164\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 132.92675\n",
      "Epoch 215/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 71.2725 - val_loss: 179.1951\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 132.92675\n",
      "Epoch 216/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 61.7226 - val_loss: 165.7634\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 132.92675\n",
      "Epoch 217/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 64.8638 - val_loss: 163.5059\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 132.92675\n",
      "Epoch 218/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 62.0031 - val_loss: 160.0900\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 132.92675\n",
      "Epoch 219/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 67.0413 - val_loss: 170.3384\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 132.92675\n",
      "Epoch 220/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 65.7880 - val_loss: 179.8426\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 132.92675\n",
      "Epoch 221/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 67.3503 - val_loss: 158.1025\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 132.92675\n",
      "Epoch 222/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 71.2919 - val_loss: 154.6085\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 132.92675\n",
      "Epoch 223/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 65.8159 - val_loss: 169.3509\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 132.92675\n",
      "Epoch 224/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 67.2783 - val_loss: 156.2426\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 132.92675\n",
      "Epoch 225/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 72.8752 - val_loss: 162.0994\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 132.92675\n",
      "Epoch 226/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 60.8125 - val_loss: 175.4021\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 132.92675\n",
      "Epoch 227/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 60.7701 - val_loss: 158.0809\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 132.92675\n",
      "Epoch 228/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 65.0408 - val_loss: 158.0423\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 132.92675\n",
      "Epoch 229/1000\n",
      "96/96 [==============================] - 0s 653us/step - loss: 66.7298 - val_loss: 162.6915\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 132.92675\n",
      "Epoch 230/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 59.3777 - val_loss: 151.2434\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 132.92675\n",
      "Epoch 231/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 62.7478 - val_loss: 142.4891\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 132.92675\n",
      "Epoch 232/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 62.2653 - val_loss: 156.1043\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 132.92675\n",
      "Epoch 233/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 77.0894 - val_loss: 168.9297\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 132.92675\n",
      "Epoch 234/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 69.4753 - val_loss: 144.5348\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 132.92675\n",
      "Epoch 235/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 62.0736 - val_loss: 149.5579\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 132.92675\n",
      "Epoch 236/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 58.6571 - val_loss: 158.7912\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 132.92675\n",
      "Epoch 237/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 58.8769 - val_loss: 133.9228\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 132.92675\n",
      "Epoch 238/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 56.2013 - val_loss: 151.7604\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 132.92675\n",
      "Epoch 239/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 55.2999 - val_loss: 150.1381\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 132.92675\n",
      "Epoch 240/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 60.3719 - val_loss: 151.7636\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 132.92675\n",
      "Epoch 241/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 54.4032 - val_loss: 161.7304\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 132.92675\n",
      "Epoch 242/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 54.9808 - val_loss: 157.0621\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 132.92675\n",
      "Epoch 243/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 75.3810 - val_loss: 156.6917\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 132.92675\n",
      "Epoch 244/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 63.9483 - val_loss: 178.3858\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 132.92675\n",
      "Epoch 245/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 61.4039 - val_loss: 156.3354\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 132.92675\n",
      "Epoch 246/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 61.2529 - val_loss: 165.4545\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 132.92675\n",
      "Epoch 247/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 65.2130 - val_loss: 152.8388\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 132.92675\n",
      "Epoch 248/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 68.4905 - val_loss: 170.5560\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 132.92675\n",
      "Epoch 249/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 62.0416 - val_loss: 172.1474\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 132.92675\n",
      "Epoch 250/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 55.8984 - val_loss: 184.4295\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 132.92675\n",
      "Epoch 251/1000\n",
      "96/96 [==============================] - 0s 655us/step - loss: 63.2492 - val_loss: 177.1187\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 132.92675\n",
      "Epoch 252/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 75.1662 - val_loss: 153.7931\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 132.92675\n",
      "Epoch 253/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 61.9510 - val_loss: 169.8236\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 132.92675\n",
      "Epoch 254/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 59.5470 - val_loss: 171.9894\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 132.92675\n",
      "Epoch 255/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 59.7561 - val_loss: 158.6175\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 132.92675\n",
      "Epoch 256/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 56.5180 - val_loss: 168.7108\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 132.92675\n",
      "Epoch 257/1000\n",
      "96/96 [==============================] - 0s 649us/step - loss: 61.4367 - val_loss: 138.6071\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 132.92675\n",
      "Epoch 258/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 59.0709 - val_loss: 170.1027\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 132.92675\n",
      "Epoch 259/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 53.0745 - val_loss: 144.0719\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 132.92675\n",
      "Epoch 260/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 74.0089 - val_loss: 159.4859\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 132.92675\n",
      "Epoch 261/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 53.2110 - val_loss: 160.2129\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 132.92675\n",
      "Epoch 262/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 51.9482 - val_loss: 169.4707\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 132.92675\n",
      "Epoch 263/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 59.4310 - val_loss: 164.0076\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 132.92675\n",
      "Epoch 264/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 62.7574 - val_loss: 164.8608\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 132.92675\n",
      "Epoch 265/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 69.3436 - val_loss: 157.4112\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 132.92675\n",
      "Epoch 266/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 61.3221 - val_loss: 160.1563\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 132.92675\n",
      "Epoch 267/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 54.4287 - val_loss: 148.4013\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 132.92675\n",
      "Epoch 268/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 55.6185 - val_loss: 147.2775\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 132.92675\n",
      "Epoch 269/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 59.7967 - val_loss: 157.1789\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 132.92675\n",
      "Epoch 270/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 64.0676 - val_loss: 158.4212\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 132.92675\n",
      "Epoch 271/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 71.4157 - val_loss: 146.6739\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 132.92675\n",
      "Epoch 272/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 56.9421 - val_loss: 139.3570\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 132.92675\n",
      "Epoch 273/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 66.6709 - val_loss: 163.4124\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 132.92675\n",
      "Epoch 274/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 61.6927 - val_loss: 170.1013\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 132.92675\n",
      "Epoch 275/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 62.2562 - val_loss: 160.3824\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 132.92675\n",
      "Epoch 276/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 57.8714 - val_loss: 180.5282\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 132.92675\n",
      "Epoch 277/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 59.3226 - val_loss: 168.2269\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 132.92675\n",
      "Epoch 278/1000\n",
      "96/96 [==============================] - 0s 658us/step - loss: 67.9785 - val_loss: 161.3781\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 132.92675\n",
      "Epoch 279/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 59.0949 - val_loss: 166.8867\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 132.92675\n",
      "Epoch 280/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 50.8560 - val_loss: 152.0177\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 132.92675\n",
      "Epoch 281/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 61.3287 - val_loss: 149.9578\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 132.92675\n",
      "Epoch 282/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 53.3014 - val_loss: 133.4976\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 132.92675\n",
      "Epoch 283/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 51.0371 - val_loss: 158.7898\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 132.92675\n",
      "Epoch 284/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 51.7548 - val_loss: 170.7851\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 132.92675\n",
      "Epoch 285/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 60.8527 - val_loss: 175.5470\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 132.92675\n",
      "Epoch 286/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 56.0180 - val_loss: 140.7538\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 132.92675\n",
      "Epoch 287/1000\n",
      "96/96 [==============================] - 0s 659us/step - loss: 61.4052 - val_loss: 160.9077\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 132.92675\n",
      "Epoch 288/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 63.4104 - val_loss: 162.6377\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 132.92675\n",
      "Epoch 289/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 54.1872 - val_loss: 160.6258\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 132.92675\n",
      "Epoch 290/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 60.7941 - val_loss: 155.6552\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 132.92675\n",
      "Epoch 291/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 55.0582 - val_loss: 168.4051\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 132.92675\n",
      "Epoch 292/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 50.1203 - val_loss: 186.0840\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 132.92675\n",
      "Epoch 293/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 60.8029 - val_loss: 172.5550\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 132.92675\n",
      "Epoch 294/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 55.7072 - val_loss: 173.6304\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 132.92675\n",
      "Epoch 295/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 60.3988 - val_loss: 176.8140\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 132.92675\n",
      "Epoch 296/1000\n",
      "96/96 [==============================] - 0s 657us/step - loss: 60.3875 - val_loss: 156.5197\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 132.92675\n",
      "Epoch 297/1000\n",
      "96/96 [==============================] - 0s 661us/step - loss: 48.5659 - val_loss: 177.4303\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 132.92675\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 639us/step - loss: 50.7831 - val_loss: 172.7406\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 132.92675\n",
      "Epoch 299/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 53.3735 - val_loss: 146.1104\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 132.92675\n",
      "Epoch 300/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 55.7112 - val_loss: 161.7711\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 132.92675\n",
      "Epoch 301/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 49.2482 - val_loss: 141.8808\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 132.92675\n",
      "Epoch 302/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 64.6880 - val_loss: 152.5097\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 132.92675\n",
      "Epoch 303/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 94.7294 - val_loss: 158.6598\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 132.92675\n",
      "Epoch 304/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 67.1419 - val_loss: 161.4658\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 132.92675\n",
      "Epoch 305/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 88.5174 - val_loss: 165.5225\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 132.92675\n",
      "Epoch 306/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 64.5281 - val_loss: 169.9772\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 132.92675\n",
      "Epoch 307/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 65.6045 - val_loss: 158.2737\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 132.92675\n",
      "Epoch 308/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 52.8041 - val_loss: 152.3445\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 132.92675\n",
      "Epoch 309/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 55.5692 - val_loss: 147.3087\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 132.92675\n",
      "Epoch 310/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 62.1946 - val_loss: 146.3530\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 132.92675\n",
      "Epoch 311/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 55.5044 - val_loss: 161.9824\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 132.92675\n",
      "Epoch 312/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 62.3820 - val_loss: 142.6551\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 132.92675\n",
      "Epoch 313/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 74.1215 - val_loss: 148.0357\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 132.92675\n",
      "Epoch 314/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 54.8270 - val_loss: 160.1022\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 132.92675\n",
      "Epoch 315/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 55.7791 - val_loss: 149.8304\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 132.92675\n",
      "Epoch 316/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 54.6241 - val_loss: 162.5231\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 132.92675\n",
      "Epoch 317/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 54.2133 - val_loss: 155.4630\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 132.92675\n",
      "Epoch 318/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 56.5860 - val_loss: 149.0932\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 132.92675\n",
      "Epoch 319/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 56.2405 - val_loss: 151.9563\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 132.92675\n",
      "Epoch 320/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 56.5335 - val_loss: 160.7148\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 132.92675\n",
      "Epoch 321/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 58.5200 - val_loss: 155.0666\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 132.92675\n",
      "Epoch 322/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 60.8235 - val_loss: 157.7389\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 132.92675\n",
      "Epoch 323/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 53.0352 - val_loss: 158.9099\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 132.92675\n",
      "Epoch 324/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 71.8087 - val_loss: 147.6636\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 132.92675\n",
      "Epoch 325/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 57.0399 - val_loss: 157.4752\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 132.92675\n",
      "Epoch 326/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 51.6961 - val_loss: 157.8797\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 132.92675\n",
      "Epoch 327/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 50.8585 - val_loss: 161.0526\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 132.92675\n",
      "Epoch 328/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 50.2576 - val_loss: 155.2968\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 132.92675\n",
      "Epoch 329/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 53.2228 - val_loss: 150.7226\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 132.92675\n",
      "Epoch 330/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 49.8153 - val_loss: 151.3966\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 132.92675\n",
      "Epoch 331/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 52.7065 - val_loss: 152.5342\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 132.92675\n",
      "Epoch 332/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 48.8666 - val_loss: 155.7208\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 132.92675\n",
      "Epoch 333/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 58.9205 - val_loss: 142.1318\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 132.92675\n",
      "Epoch 334/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 57.1448 - val_loss: 152.4405\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 132.92675\n",
      "Epoch 335/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 62.2300 - val_loss: 162.9870\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 132.92675\n",
      "Epoch 336/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 62.6898 - val_loss: 167.3718\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 132.92675\n",
      "Epoch 337/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 51.6985 - val_loss: 162.8113\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 132.92675\n",
      "Epoch 338/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 50.2648 - val_loss: 154.8637\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 132.92675\n",
      "Epoch 339/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 61.6922 - val_loss: 167.6174\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 132.92675\n",
      "Epoch 340/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 56.9455 - val_loss: 153.0411\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 132.92675\n",
      "Epoch 341/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 60.4918 - val_loss: 148.5457\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 132.92675\n",
      "Epoch 342/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 45.2069 - val_loss: 151.7206\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 132.92675\n",
      "Epoch 343/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 47.1900 - val_loss: 150.1998\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 132.92675\n",
      "Epoch 344/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 56.9959 - val_loss: 146.8445\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 132.92675\n",
      "Epoch 345/1000\n",
      "96/96 [==============================] - 0s 678us/step - loss: 52.7916 - val_loss: 155.2040\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 132.92675\n",
      "Epoch 346/1000\n",
      "96/96 [==============================] - 0s 669us/step - loss: 44.0427 - val_loss: 154.7328\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 132.92675\n",
      "Epoch 347/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 48.6832 - val_loss: 157.2817\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 132.92675\n",
      "Epoch 348/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 48.5721 - val_loss: 151.2595\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 132.92675\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 631us/step - loss: 52.8671 - val_loss: 157.4337\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 132.92675\n",
      "Epoch 350/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 48.4902 - val_loss: 152.8826\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 132.92675\n",
      "Epoch 351/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 50.9832 - val_loss: 160.5031\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 132.92675\n",
      "Epoch 352/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 42.5366 - val_loss: 161.2096\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 132.92675\n",
      "Epoch 353/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 49.8243 - val_loss: 163.3355\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 132.92675\n",
      "Epoch 354/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 64.0114 - val_loss: 161.6276\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 132.92675\n",
      "Epoch 355/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 49.4631 - val_loss: 160.1670\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 132.92675\n",
      "Epoch 356/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.5420 - val_loss: 157.3523\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 132.92675\n",
      "Epoch 357/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 60.5009 - val_loss: 176.5290\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 132.92675\n",
      "Epoch 358/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 53.2687 - val_loss: 182.1568\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 132.92675\n",
      "Epoch 359/1000\n",
      "96/96 [==============================] - 0s 654us/step - loss: 55.2769 - val_loss: 161.2596\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 132.92675\n",
      "Epoch 360/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 52.3389 - val_loss: 161.4510\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 132.92675\n",
      "Epoch 361/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 49.4940 - val_loss: 175.0528\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 132.92675\n",
      "Epoch 362/1000\n",
      "96/96 [==============================] - 0s 670us/step - loss: 54.9908 - val_loss: 162.2587\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 132.92675\n",
      "Epoch 363/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 52.1275 - val_loss: 185.3995\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 132.92675\n",
      "Epoch 364/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 54.0149 - val_loss: 197.0139\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 132.92675\n",
      "Epoch 365/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 59.2778 - val_loss: 161.3519\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 132.92675\n",
      "Epoch 366/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 63.2994 - val_loss: 163.8474\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 132.92675\n",
      "Epoch 367/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 56.7646 - val_loss: 178.6240\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 132.92675\n",
      "Epoch 368/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 59.8368 - val_loss: 177.2849\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 132.92675\n",
      "Epoch 369/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 45.1015 - val_loss: 179.6201\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 132.92675\n",
      "Epoch 370/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 56.2229 - val_loss: 173.1541\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 132.92675\n",
      "Epoch 371/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 44.7522 - val_loss: 171.4533\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 132.92675\n",
      "Epoch 372/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 51.1143 - val_loss: 183.5869\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 132.92675\n",
      "Epoch 373/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 47.8689 - val_loss: 163.4207\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 132.92675\n",
      "Epoch 374/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 51.8384 - val_loss: 167.2506\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 132.92675\n",
      "Epoch 375/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 47.6030 - val_loss: 158.3466\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 132.92675\n",
      "Epoch 376/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 45.6960 - val_loss: 157.3544\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 132.92675\n",
      "Epoch 377/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 41.8954 - val_loss: 165.0790\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 132.92675\n",
      "Epoch 378/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 47.4903 - val_loss: 166.1505\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 132.92675\n",
      "Epoch 379/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 52.2397 - val_loss: 164.4690\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 132.92675\n",
      "Epoch 380/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 50.2520 - val_loss: 165.6481\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 132.92675\n",
      "Epoch 381/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 52.8440 - val_loss: 158.5409\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 132.92675\n",
      "Epoch 382/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 44.6075 - val_loss: 156.1119\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 132.92675\n",
      "Epoch 383/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 42.1162 - val_loss: 166.9436\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 132.92675\n",
      "Epoch 384/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 54.8327 - val_loss: 162.7220\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 132.92675\n",
      "Epoch 385/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 60.1856 - val_loss: 173.5378\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 132.92675\n",
      "Epoch 386/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 46.3323 - val_loss: 167.6613\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 132.92675\n",
      "Epoch 387/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 44.5203 - val_loss: 167.0759\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 132.92675\n",
      "Epoch 388/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 53.3387 - val_loss: 161.7594\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 132.92675\n",
      "Epoch 389/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 48.0896 - val_loss: 169.5854\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 132.92675\n",
      "Epoch 390/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 50.7416 - val_loss: 157.6656\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 132.92675\n",
      "Epoch 391/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 47.9614 - val_loss: 172.8267\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 132.92675\n",
      "Epoch 392/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 56.9240 - val_loss: 177.7657\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 132.92675\n",
      "Epoch 393/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 58.7741 - val_loss: 161.6187\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 132.92675\n",
      "Epoch 394/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 43.3833 - val_loss: 175.5373\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 132.92675\n",
      "Epoch 395/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 50.5013 - val_loss: 167.3992\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 132.92675\n",
      "Epoch 396/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 47.8558 - val_loss: 159.0001\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 132.92675\n",
      "Epoch 397/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 60.2910 - val_loss: 158.8671\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 132.92675\n",
      "Epoch 398/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 49.1482 - val_loss: 155.6907\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 132.92675\n",
      "Epoch 399/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 51.8351 - val_loss: 166.9299\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 132.92675\n",
      "Epoch 400/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 618us/step - loss: 53.0243 - val_loss: 169.9224\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 132.92675\n",
      "Epoch 401/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 51.6162 - val_loss: 178.8691\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 132.92675\n",
      "Epoch 402/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 52.6158 - val_loss: 173.4466\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 132.92675\n",
      "Epoch 403/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 41.5721 - val_loss: 167.8775\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 132.92675\n",
      "Epoch 404/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 46.2988 - val_loss: 159.1517\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 132.92675\n",
      "Epoch 405/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 45.6794 - val_loss: 153.7901\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 132.92675\n",
      "Epoch 406/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 45.8568 - val_loss: 163.0737\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 132.92675\n",
      "Epoch 407/1000\n",
      "96/96 [==============================] - 0s 659us/step - loss: 37.6916 - val_loss: 177.0657\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 132.92675\n",
      "Epoch 408/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 45.0913 - val_loss: 165.2145\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 132.92675\n",
      "Epoch 409/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 53.3963 - val_loss: 175.2041\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 132.92675\n",
      "Epoch 410/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 51.4232 - val_loss: 174.3247\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 132.92675\n",
      "Epoch 411/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 51.4356 - val_loss: 182.0026\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 132.92675\n",
      "Epoch 412/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 44.5561 - val_loss: 170.7549\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 132.92675\n",
      "Epoch 413/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 52.7500 - val_loss: 160.5590\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 132.92675\n",
      "Epoch 414/1000\n",
      "96/96 [==============================] - 0s 651us/step - loss: 42.4089 - val_loss: 174.5537\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 132.92675\n",
      "Epoch 415/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 46.7368 - val_loss: 166.9168\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 132.92675\n",
      "Epoch 416/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 45.9497 - val_loss: 190.5175\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 132.92675\n",
      "Epoch 417/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 41.3550 - val_loss: 157.9892\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 132.92675\n",
      "Epoch 418/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 46.4009 - val_loss: 174.7669\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 132.92675\n",
      "Epoch 419/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 40.3573 - val_loss: 180.8308\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 132.92675\n",
      "Epoch 420/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 50.3885 - val_loss: 182.3197\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 132.92675\n",
      "Epoch 421/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 49.1998 - val_loss: 183.6190\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 132.92675\n",
      "Epoch 422/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 42.7511 - val_loss: 179.0783\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 132.92675\n",
      "Epoch 423/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 46.8880 - val_loss: 185.5922\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 132.92675\n",
      "Epoch 424/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 60.3976 - val_loss: 173.7772\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 132.92675\n",
      "Epoch 425/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 41.2709 - val_loss: 174.0150\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 132.92675\n",
      "Epoch 426/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 43.5083 - val_loss: 171.3229\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 132.92675\n",
      "Epoch 427/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 56.6328 - val_loss: 168.1510\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 132.92675\n",
      "Epoch 428/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 72.2246 - val_loss: 169.3846\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 132.92675\n",
      "Epoch 429/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 56.4412 - val_loss: 196.7703\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 132.92675\n",
      "Epoch 430/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 57.7819 - val_loss: 192.2055\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 132.92675\n",
      "Epoch 431/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 66.8927 - val_loss: 171.1869\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 132.92675\n",
      "Epoch 432/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 54.5765 - val_loss: 170.2185\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 132.92675\n",
      "Epoch 433/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 53.0203 - val_loss: 185.7497\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 132.92675\n",
      "Epoch 434/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 50.3420 - val_loss: 177.9665\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 132.92675\n",
      "Epoch 435/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 44.8051 - val_loss: 180.8469\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 132.92675\n",
      "Epoch 436/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 41.7305 - val_loss: 168.6188\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 132.92675\n",
      "Epoch 437/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 58.4931 - val_loss: 190.4765\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 132.92675\n",
      "Epoch 438/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 53.2009 - val_loss: 184.3294\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 132.92675\n",
      "Epoch 439/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 55.4138 - val_loss: 185.5060\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 132.92675\n",
      "Epoch 440/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 44.4110 - val_loss: 211.0748\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 132.92675\n",
      "Epoch 441/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 49.4198 - val_loss: 170.2021\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 132.92675\n",
      "Epoch 442/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 49.4260 - val_loss: 168.2788\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 132.92675\n",
      "Epoch 443/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 44.9954 - val_loss: 181.7822\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 132.92675\n",
      "Epoch 444/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 53.9871 - val_loss: 169.4556\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 132.92675\n",
      "Epoch 445/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 49.0499 - val_loss: 169.0722\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 132.92675\n",
      "Epoch 446/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 54.3376 - val_loss: 169.8194\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 132.92675\n",
      "Epoch 447/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 49.9509 - val_loss: 167.9477\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 132.92675\n",
      "Epoch 448/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 52.9907 - val_loss: 160.3110\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 132.92675\n",
      "Epoch 449/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 60.9644 - val_loss: 177.9245\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 132.92675\n",
      "Epoch 450/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 61.4524 - val_loss: 160.5226\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 132.92675\n",
      "Epoch 451/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 632us/step - loss: 47.3239 - val_loss: 178.8522\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 132.92675\n",
      "Epoch 452/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 58.1173 - val_loss: 164.9356\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 132.92675\n",
      "Epoch 453/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 54.6245 - val_loss: 158.3432\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 132.92675\n",
      "Epoch 454/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 61.4614 - val_loss: 197.2897\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 132.92675\n",
      "Epoch 455/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 46.9946 - val_loss: 185.2405\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 132.92675\n",
      "Epoch 456/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 42.9391 - val_loss: 173.2411\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 132.92675\n",
      "Epoch 457/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 44.7420 - val_loss: 175.9894\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 132.92675\n",
      "Epoch 458/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 44.4542 - val_loss: 150.8580\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 132.92675\n",
      "Epoch 459/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 48.6513 - val_loss: 155.4343\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 132.92675\n",
      "Epoch 460/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 48.0184 - val_loss: 159.1733\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 132.92675\n",
      "Epoch 461/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 39.6776 - val_loss: 164.7239\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 132.92675\n",
      "Epoch 462/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 46.3223 - val_loss: 171.8034\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 132.92675\n",
      "Epoch 463/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 42.7612 - val_loss: 163.2163\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 132.92675\n",
      "Epoch 464/1000\n",
      "96/96 [==============================] - 0s 662us/step - loss: 41.3766 - val_loss: 164.0752\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 132.92675\n",
      "Epoch 465/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 45.5698 - val_loss: 170.0641\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 132.92675\n",
      "Epoch 466/1000\n",
      "96/96 [==============================] - 0s 652us/step - loss: 56.8482 - val_loss: 178.9343\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 132.92675\n",
      "Epoch 467/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 54.2760 - val_loss: 184.2066\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 132.92675\n",
      "Epoch 468/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 41.3698 - val_loss: 177.8096\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 132.92675\n",
      "Epoch 469/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 51.4566 - val_loss: 162.0767\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 132.92675\n",
      "Epoch 470/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 46.4455 - val_loss: 172.6117\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 132.92675\n",
      "Epoch 471/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 52.3765 - val_loss: 149.4129\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 132.92675\n",
      "Epoch 472/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 41.1852 - val_loss: 158.8648\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 132.92675\n",
      "Epoch 473/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 44.4213 - val_loss: 170.7826\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 132.92675\n",
      "Epoch 474/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 45.1321 - val_loss: 166.3530\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 132.92675\n",
      "Epoch 475/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 47.6648 - val_loss: 158.0783\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 132.92675\n",
      "Epoch 476/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 47.9224 - val_loss: 164.3575\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 132.92675\n",
      "Epoch 477/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 46.4903 - val_loss: 190.9522\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 132.92675\n",
      "Epoch 478/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 41.8249 - val_loss: 182.0710\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 132.92675\n",
      "Epoch 479/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 50.1882 - val_loss: 181.0412\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 132.92675\n",
      "Epoch 480/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 44.9891 - val_loss: 169.9266\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 132.92675\n",
      "Epoch 481/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 42.7668 - val_loss: 164.9649\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 132.92675\n",
      "Epoch 482/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 43.2828 - val_loss: 170.0889\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 132.92675\n",
      "Epoch 483/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 44.5768 - val_loss: 187.5780\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 132.92675\n",
      "Epoch 484/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 50.9134 - val_loss: 179.3960\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 132.92675\n",
      "Epoch 485/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 47.0424 - val_loss: 185.7259\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 132.92675\n",
      "Epoch 486/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 39.9926 - val_loss: 175.7471\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 132.92675\n",
      "Epoch 487/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 44.5627 - val_loss: 164.9170\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 132.92675\n",
      "Epoch 488/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 43.8127 - val_loss: 169.1082\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 132.92675\n",
      "Epoch 489/1000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 51.3622 - val_loss: 178.9575\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 132.92675\n",
      "Epoch 490/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 41.3541 - val_loss: 175.1473\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 132.92675\n",
      "Epoch 491/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 40.6588 - val_loss: 167.8325\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 132.92675\n",
      "Epoch 492/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 50.3913 - val_loss: 190.6708\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 132.92675\n",
      "Epoch 493/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 48.1324 - val_loss: 174.4241\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 132.92675\n",
      "Epoch 494/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 47.6464 - val_loss: 179.4428\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 132.92675\n",
      "Epoch 495/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 40.0655 - val_loss: 179.3702\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 132.92675\n",
      "Epoch 496/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 50.0557 - val_loss: 162.5909\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 132.92675\n",
      "Epoch 497/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 47.4882 - val_loss: 165.1678\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 132.92675\n",
      "Epoch 498/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 46.5717 - val_loss: 151.2259\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 132.92675\n",
      "Epoch 499/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 37.6339 - val_loss: 157.3089\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 132.92675\n",
      "Epoch 500/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.0205 - val_loss: 177.3295\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 132.92675\n",
      "Epoch 501/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 51.3611 - val_loss: 164.1994\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 132.92675\n",
      "Epoch 502/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 622us/step - loss: 38.8831 - val_loss: 171.7381\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 132.92675\n",
      "Epoch 503/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 41.8760 - val_loss: 174.5739\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 132.92675\n",
      "Epoch 504/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 46.6524 - val_loss: 185.2073\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 132.92675\n",
      "Epoch 505/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 42.5213 - val_loss: 177.3979\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 132.92675\n",
      "Epoch 506/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 40.6276 - val_loss: 177.9826\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 132.92675\n",
      "Epoch 507/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 41.9212 - val_loss: 168.6542\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 132.92675\n",
      "Epoch 508/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 40.6673 - val_loss: 174.7410\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 132.92675\n",
      "Epoch 509/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 44.7993 - val_loss: 168.4133\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 132.92675\n",
      "Epoch 510/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 37.7831 - val_loss: 175.2441\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 132.92675\n",
      "Epoch 511/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 45.3807 - val_loss: 212.4030\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 132.92675\n",
      "Epoch 512/1000\n",
      "96/96 [==============================] - 0s 651us/step - loss: 41.0584 - val_loss: 188.2732\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 132.92675\n",
      "Epoch 513/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 40.0264 - val_loss: 170.8171\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 132.92675\n",
      "Epoch 514/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 43.9370 - val_loss: 170.3725\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 132.92675\n",
      "Epoch 515/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 38.5784 - val_loss: 164.9434\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 132.92675\n",
      "Epoch 516/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 52.0728 - val_loss: 169.1042\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 132.92675\n",
      "Epoch 517/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 43.1268 - val_loss: 194.4304\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 132.92675\n",
      "Epoch 518/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 37.5684 - val_loss: 195.4646\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 132.92675\n",
      "Epoch 519/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 42.0489 - val_loss: 196.9725\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 132.92675\n",
      "Epoch 520/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 52.2804 - val_loss: 198.5740\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 132.92675\n",
      "Epoch 521/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 49.2236 - val_loss: 173.2423\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 132.92675\n",
      "Epoch 522/1000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 47.8193 - val_loss: 182.1786\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 132.92675\n",
      "Epoch 523/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 44.9284 - val_loss: 186.5877\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 132.92675\n",
      "Epoch 524/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 53.1444 - val_loss: 232.2391\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 132.92675\n",
      "Epoch 525/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 47.1552 - val_loss: 193.9433\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 132.92675\n",
      "Epoch 526/1000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 40.8992 - val_loss: 179.7753\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 132.92675\n",
      "Epoch 527/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 52.8196 - val_loss: 178.2754\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 132.92675\n",
      "Epoch 528/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 37.2775 - val_loss: 184.3297\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 132.92675\n",
      "Epoch 529/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 46.6136 - val_loss: 178.4585\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 132.92675\n",
      "Epoch 530/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 42.5955 - val_loss: 168.8964\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 132.92675\n",
      "Epoch 531/1000\n",
      "96/96 [==============================] - 0s 664us/step - loss: 46.3733 - val_loss: 168.5282\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 132.92675\n",
      "Epoch 532/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 50.2085 - val_loss: 173.8310\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 132.92675\n",
      "Epoch 533/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 40.9510 - val_loss: 174.8381\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 132.92675\n",
      "Epoch 534/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 42.2775 - val_loss: 179.6495\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 132.92675\n",
      "Epoch 535/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 36.2718 - val_loss: 170.6841\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 132.92675\n",
      "Epoch 536/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 46.3118 - val_loss: 176.9989\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 132.92675\n",
      "Epoch 537/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 38.3336 - val_loss: 187.0243\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 132.92675\n",
      "Epoch 538/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 37.4097 - val_loss: 182.6806\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 132.92675\n",
      "Epoch 539/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 46.0618 - val_loss: 187.0724\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 132.92675\n",
      "Epoch 540/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 39.4388 - val_loss: 207.3505\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 132.92675\n",
      "Epoch 541/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 40.6530 - val_loss: 196.8608\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 132.92675\n",
      "Epoch 542/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 42.2554 - val_loss: 202.3090\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 132.92675\n",
      "Epoch 543/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 48.5329 - val_loss: 186.5705\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 132.92675\n",
      "Epoch 544/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 41.9761 - val_loss: 184.7379\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 132.92675\n",
      "Epoch 545/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 45.1000 - val_loss: 185.9601\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 132.92675\n",
      "Epoch 546/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 43.0070 - val_loss: 189.9955\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 132.92675\n",
      "Epoch 547/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 44.2568 - val_loss: 193.4668\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 132.92675\n",
      "Epoch 548/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 55.2568 - val_loss: 202.6185\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 132.92675\n",
      "Epoch 549/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 48.8362 - val_loss: 203.7257\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 132.92675\n",
      "Epoch 550/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 42.6965 - val_loss: 208.2619\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 132.92675\n",
      "Epoch 551/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 49.1976 - val_loss: 175.7521\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 132.92675\n",
      "Epoch 552/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 42.7361 - val_loss: 180.5572\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 132.92675\n",
      "Epoch 553/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 627us/step - loss: 39.5794 - val_loss: 182.6268\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 132.92675\n",
      "Epoch 554/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 41.8075 - val_loss: 182.8613\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 132.92675\n",
      "Epoch 555/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 44.9262 - val_loss: 166.4367\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 132.92675\n",
      "Epoch 556/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 41.0149 - val_loss: 177.1949\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 132.92675\n",
      "Epoch 557/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 40.4481 - val_loss: 183.3868\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 132.92675\n",
      "Epoch 558/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 45.8454 - val_loss: 174.3191\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 132.92675\n",
      "Epoch 559/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 42.4370 - val_loss: 161.3346\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 132.92675\n",
      "Epoch 560/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 40.8803 - val_loss: 175.8942\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 132.92675\n",
      "Epoch 561/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 40.2930 - val_loss: 175.6738\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 132.92675\n",
      "Epoch 562/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 49.0459 - val_loss: 185.0400\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 132.92675\n",
      "Epoch 563/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 41.3660 - val_loss: 199.7380\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 132.92675\n",
      "Epoch 564/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 42.8598 - val_loss: 208.6131\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 132.92675\n",
      "Epoch 565/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 44.9875 - val_loss: 208.1508\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 132.92675\n",
      "Epoch 566/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 39.9544 - val_loss: 206.8800\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 132.92675\n",
      "Epoch 567/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 37.2175 - val_loss: 192.7577\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 132.92675\n",
      "Epoch 568/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 38.4462 - val_loss: 190.8864\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 132.92675\n",
      "Epoch 569/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 36.4465 - val_loss: 188.0390\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 132.92675\n",
      "Epoch 570/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 41.2045 - val_loss: 189.9159\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 132.92675\n",
      "Epoch 571/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 38.4354 - val_loss: 184.3150\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 132.92675\n",
      "Epoch 572/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 37.5527 - val_loss: 172.9928\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 132.92675\n",
      "Epoch 573/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 43.3809 - val_loss: 193.5042\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 132.92675\n",
      "Epoch 574/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 35.8943 - val_loss: 187.0044\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 132.92675\n",
      "Epoch 575/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 48.0602 - val_loss: 194.3322\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 132.92675\n",
      "Epoch 576/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 39.2611 - val_loss: 190.1912\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 132.92675\n",
      "Epoch 577/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 49.5611 - val_loss: 174.0493\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 132.92675\n",
      "Epoch 578/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 45.2714 - val_loss: 194.9382\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 132.92675\n",
      "Epoch 579/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 33.0020 - val_loss: 195.8604\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 132.92675\n",
      "Epoch 580/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 36.1726 - val_loss: 208.6333\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 132.92675\n",
      "Epoch 581/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 47.9657 - val_loss: 192.6884\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 132.92675\n",
      "Epoch 582/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 36.7210 - val_loss: 183.8735\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 132.92675\n",
      "Epoch 583/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 49.4438 - val_loss: 219.3086\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 132.92675\n",
      "Epoch 584/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 46.7183 - val_loss: 209.3691\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 132.92675\n",
      "Epoch 585/1000\n",
      "96/96 [==============================] - 0s 655us/step - loss: 39.6884 - val_loss: 194.9562\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 132.92675\n",
      "Epoch 586/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 39.4537 - val_loss: 212.2250\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 132.92675\n",
      "Epoch 587/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 44.7712 - val_loss: 199.4613\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 132.92675\n",
      "Epoch 588/1000\n",
      "96/96 [==============================] - 0s 663us/step - loss: 73.6439 - val_loss: 179.0085\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 132.92675\n",
      "Epoch 589/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 56.3592 - val_loss: 193.7411\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 132.92675\n",
      "Epoch 590/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 74.4148 - val_loss: 209.6224\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 132.92675\n",
      "Epoch 591/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 61.6555 - val_loss: 214.7410\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 132.92675\n",
      "Epoch 592/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 59.1148 - val_loss: 207.7087\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 132.92675\n",
      "Epoch 593/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 53.2968 - val_loss: 215.3753\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 132.92675\n",
      "Epoch 594/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 49.0352 - val_loss: 204.9692\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 132.92675\n",
      "Epoch 595/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 52.1183 - val_loss: 213.3088\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 132.92675\n",
      "Epoch 596/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 49.9612 - val_loss: 211.6169\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 132.92675\n",
      "Epoch 597/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 43.7620 - val_loss: 193.4358\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 132.92675\n",
      "Epoch 598/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 42.8774 - val_loss: 200.0283\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 132.92675\n",
      "Epoch 599/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 46.2910 - val_loss: 197.7231\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 132.92675\n",
      "Epoch 600/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 42.4924 - val_loss: 197.8754\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 132.92675\n",
      "Epoch 601/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 50.8951 - val_loss: 221.8076\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 132.92675\n",
      "Epoch 602/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 41.0947 - val_loss: 222.0698\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 132.92675\n",
      "Epoch 603/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 48.6814 - val_loss: 214.4803\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 132.92675\n",
      "Epoch 604/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 606us/step - loss: 39.2659 - val_loss: 210.6837\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 132.92675\n",
      "Epoch 605/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 43.6488 - val_loss: 207.1516\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 132.92675\n",
      "Epoch 606/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 38.7171 - val_loss: 203.2018\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 132.92675\n",
      "Epoch 607/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 47.2210 - val_loss: 201.6555\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 132.92675\n",
      "Epoch 608/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 41.9153 - val_loss: 203.2503\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 132.92675\n",
      "Epoch 609/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 34.9517 - val_loss: 191.2862\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 132.92675\n",
      "Epoch 610/1000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 37.5187 - val_loss: 174.1089\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 132.92675\n",
      "Epoch 611/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 35.3743 - val_loss: 190.3453\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 132.92675\n",
      "Epoch 612/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 36.5216 - val_loss: 200.7728\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 132.92675\n",
      "Epoch 613/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 41.0367 - val_loss: 199.4772\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 132.92675\n",
      "Epoch 614/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 47.5532 - val_loss: 209.1731\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 132.92675\n",
      "Epoch 615/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 47.0797 - val_loss: 212.9580\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 132.92675\n",
      "Epoch 616/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 43.4713 - val_loss: 220.4793\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 132.92675\n",
      "Epoch 617/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 60.5913 - val_loss: 191.2199\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 132.92675\n",
      "Epoch 618/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 54.9717 - val_loss: 185.8150\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 132.92675\n",
      "Epoch 619/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 53.5720 - val_loss: 203.5617\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 132.92675\n",
      "Epoch 620/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 45.4656 - val_loss: 173.6371\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 132.92675\n",
      "Epoch 621/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 37.2478 - val_loss: 204.4256\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 132.92675\n",
      "Epoch 622/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 43.9779 - val_loss: 218.0938\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 132.92675\n",
      "Epoch 623/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 41.7741 - val_loss: 222.9156\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 132.92675\n",
      "Epoch 624/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 46.3359 - val_loss: 214.6767\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 132.92675\n",
      "Epoch 625/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 35.7223 - val_loss: 206.5044\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 132.92675\n",
      "Epoch 626/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.9100 - val_loss: 210.8023\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 132.92675\n",
      "Epoch 627/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 42.5004 - val_loss: 211.7283\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 132.92675\n",
      "Epoch 628/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 50.8861 - val_loss: 223.5239\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 132.92675\n",
      "Epoch 629/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 43.9642 - val_loss: 230.9121\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 132.92675\n",
      "Epoch 630/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 39.7126 - val_loss: 234.3751\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 132.92675\n",
      "Epoch 631/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 51.4885 - val_loss: 222.6255\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 132.92675\n",
      "Epoch 632/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 38.9903 - val_loss: 212.9942\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 132.92675\n",
      "Epoch 633/1000\n",
      "96/96 [==============================] - 0s 610us/step - loss: 44.1401 - val_loss: 216.9962\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 132.92675\n",
      "Epoch 634/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 40.7470 - val_loss: 211.1463\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 132.92675\n",
      "Epoch 635/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 43.5623 - val_loss: 195.8564\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 132.92675\n",
      "Epoch 636/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 44.1482 - val_loss: 172.5152\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 132.92675\n",
      "Epoch 637/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 44.4588 - val_loss: 185.8777\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 132.92675\n",
      "Epoch 638/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 47.1308 - val_loss: 197.6656\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 132.92675\n",
      "Epoch 639/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 43.3040 - val_loss: 191.0634\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 132.92675\n",
      "Epoch 640/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 46.2440 - val_loss: 198.1565\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 132.92675\n",
      "Epoch 641/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 36.5475 - val_loss: 209.7405\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 132.92675\n",
      "Epoch 642/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 40.2517 - val_loss: 186.3420\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 132.92675\n",
      "Epoch 643/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 38.2715 - val_loss: 189.4323\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 132.92675\n",
      "Epoch 644/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 41.8635 - val_loss: 188.6369\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 132.92675\n",
      "Epoch 645/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 36.5395 - val_loss: 183.9986\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 132.92675\n",
      "Epoch 646/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 33.8526 - val_loss: 171.8963\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 132.92675\n",
      "Epoch 647/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 38.4994 - val_loss: 179.1478\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 132.92675\n",
      "Epoch 648/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 43.4990 - val_loss: 191.8697\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 132.92675\n",
      "Epoch 649/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 34.3257 - val_loss: 180.9185\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 132.92675\n",
      "Epoch 650/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 48.7261 - val_loss: 213.1099\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 132.92675\n",
      "Epoch 651/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 45.7540 - val_loss: 169.3403\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 132.92675\n",
      "Epoch 652/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 45.9925 - val_loss: 188.3334\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 132.92675\n",
      "Epoch 653/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 40.1560 - val_loss: 193.9440\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 132.92675\n",
      "Epoch 654/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 53.2412 - val_loss: 184.5648\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 132.92675\n",
      "Epoch 655/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 615us/step - loss: 44.5490 - val_loss: 191.1454\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 132.92675\n",
      "Epoch 656/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.0369 - val_loss: 190.7459\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 132.92675\n",
      "Epoch 657/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 41.1163 - val_loss: 186.1536\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 132.92675\n",
      "Epoch 658/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 47.1402 - val_loss: 197.6987\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 132.92675\n",
      "Epoch 659/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 47.4877 - val_loss: 188.0831\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 132.92675\n",
      "Epoch 660/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 41.9021 - val_loss: 213.8183\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 132.92675\n",
      "Epoch 661/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 66.9708 - val_loss: 164.4789\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 132.92675\n",
      "Epoch 662/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 44.7144 - val_loss: 159.0600\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 132.92675\n",
      "Epoch 663/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 46.2585 - val_loss: 165.3423\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 132.92675\n",
      "Epoch 664/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 48.3980 - val_loss: 187.6784\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 132.92675\n",
      "Epoch 665/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 48.0890 - val_loss: 186.8147\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 132.92675\n",
      "Epoch 666/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 43.8819 - val_loss: 192.9689\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 132.92675\n",
      "Epoch 667/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 42.4352 - val_loss: 197.0971\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 132.92675\n",
      "Epoch 668/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 36.3837 - val_loss: 230.8746\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 132.92675\n",
      "Epoch 669/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 57.2438 - val_loss: 215.7243\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 132.92675\n",
      "Epoch 670/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 33.9525 - val_loss: 188.9210\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 132.92675\n",
      "Epoch 671/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 34.8372 - val_loss: 209.6836\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 132.92675\n",
      "Epoch 672/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 43.9854 - val_loss: 204.4925\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 132.92675\n",
      "Epoch 673/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 35.5316 - val_loss: 203.1974\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 132.92675\n",
      "Epoch 674/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 34.5625 - val_loss: 216.6772\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 132.92675\n",
      "Epoch 675/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 43.5458 - val_loss: 217.5235\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 132.92675\n",
      "Epoch 676/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 43.8435 - val_loss: 222.0743\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 132.92675\n",
      "Epoch 677/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 35.8389 - val_loss: 207.9739\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 132.92675\n",
      "Epoch 678/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 45.7683 - val_loss: 199.6662\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 132.92675\n",
      "Epoch 679/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 43.0055 - val_loss: 200.5521\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 132.92675\n",
      "Epoch 680/1000\n",
      "96/96 [==============================] - 0s 660us/step - loss: 44.9838 - val_loss: 173.8745\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 132.92675\n",
      "Epoch 681/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 40.2596 - val_loss: 166.9421\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 132.92675\n",
      "Epoch 682/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 39.6260 - val_loss: 175.3198\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 132.92675\n",
      "Epoch 683/1000\n",
      "96/96 [==============================] - 0s 696us/step - loss: 36.0664 - val_loss: 209.7126\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 132.92675\n",
      "Epoch 684/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 39.9963 - val_loss: 180.4532\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 132.92675\n",
      "Epoch 685/1000\n",
      "96/96 [==============================] - 0s 603us/step - loss: 34.8688 - val_loss: 185.7935\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 132.92675\n",
      "Epoch 686/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 37.2890 - val_loss: 195.1959\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 132.92675\n",
      "Epoch 687/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 35.6801 - val_loss: 199.7588\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 132.92675\n",
      "Epoch 688/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 40.0757 - val_loss: 212.1947\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 132.92675\n",
      "Epoch 689/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 43.3699 - val_loss: 217.0256\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 132.92675\n",
      "Epoch 690/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 40.6540 - val_loss: 197.4514\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 132.92675\n",
      "Epoch 691/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 46.1177 - val_loss: 186.2463\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 132.92675\n",
      "Epoch 692/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 45.4580 - val_loss: 209.4479\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 132.92675\n",
      "Epoch 693/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 39.6796 - val_loss: 196.7014\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 132.92675\n",
      "Epoch 694/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 40.0658 - val_loss: 180.5814\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 132.92675\n",
      "Epoch 695/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 57.6482 - val_loss: 184.2397\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 132.92675\n",
      "Epoch 696/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 48.1768 - val_loss: 186.5129\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 132.92675\n",
      "Epoch 697/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 39.8628 - val_loss: 187.0523\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 132.92675\n",
      "Epoch 698/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 37.4383 - val_loss: 176.3447\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 132.92675\n",
      "Epoch 699/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 32.5636 - val_loss: 181.6463\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 132.92675\n",
      "Epoch 700/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 45.2352 - val_loss: 208.2413\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 132.92675\n",
      "Epoch 701/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 37.2077 - val_loss: 176.3464\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 132.92675\n",
      "Epoch 702/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 45.8645 - val_loss: 177.2444\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 132.92675\n",
      "Epoch 703/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 32.9704 - val_loss: 184.4982\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 132.92675\n",
      "Epoch 704/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 33.6119 - val_loss: 178.6386\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 132.92675\n",
      "Epoch 705/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 48.0522 - val_loss: 212.7765\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 132.92675\n",
      "Epoch 706/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 636us/step - loss: 39.4081 - val_loss: 189.8864\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 132.92675\n",
      "Epoch 707/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 53.5755 - val_loss: 181.7772\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 132.92675\n",
      "Epoch 708/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 39.7327 - val_loss: 212.7752\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 132.92675\n",
      "Epoch 709/1000\n",
      "96/96 [==============================] - 0s 656us/step - loss: 41.6084 - val_loss: 205.7997\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 132.92675\n",
      "Epoch 710/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 51.2098 - val_loss: 185.1794\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 132.92675\n",
      "Epoch 711/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 41.0300 - val_loss: 198.3839\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 132.92675\n",
      "Epoch 712/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 40.3722 - val_loss: 217.0045\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 132.92675\n",
      "Epoch 713/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 49.5207 - val_loss: 222.8745\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 132.92675\n",
      "Epoch 714/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 44.9340 - val_loss: 193.8088\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 132.92675\n",
      "Epoch 715/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 43.2771 - val_loss: 168.9575\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 132.92675\n",
      "Epoch 716/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 41.1218 - val_loss: 182.9485\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 132.92675\n",
      "Epoch 717/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 32.7599 - val_loss: 202.8781\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 132.92675\n",
      "Epoch 718/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 35.0868 - val_loss: 170.4919\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 132.92675\n",
      "Epoch 719/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 41.7079 - val_loss: 173.7341\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 132.92675\n",
      "Epoch 720/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 35.5407 - val_loss: 183.0001\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 132.92675\n",
      "Epoch 721/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 35.3554 - val_loss: 184.4623\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 132.92675\n",
      "Epoch 722/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 37.5820 - val_loss: 183.4377\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 132.92675\n",
      "Epoch 723/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 33.5131 - val_loss: 180.5831\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 132.92675\n",
      "Epoch 724/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 35.4730 - val_loss: 189.1907\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 132.92675\n",
      "Epoch 725/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 37.6784 - val_loss: 217.1859\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 132.92675\n",
      "Epoch 726/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 40.0008 - val_loss: 206.6063\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 132.92675\n",
      "Epoch 727/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 38.7810 - val_loss: 165.0151\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 132.92675\n",
      "Epoch 728/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 59.3818 - val_loss: 213.2192\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 132.92675\n",
      "Epoch 729/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 44.2119 - val_loss: 203.5935\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 132.92675\n",
      "Epoch 730/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 47.5914 - val_loss: 182.8834\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 132.92675\n",
      "Epoch 731/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 32.3694 - val_loss: 178.9147\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 132.92675\n",
      "Epoch 732/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 50.4717 - val_loss: 169.5056\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 132.92675\n",
      "Epoch 733/1000\n",
      "96/96 [==============================] - 0s 650us/step - loss: 39.8838 - val_loss: 173.3845\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 132.92675\n",
      "Epoch 734/1000\n",
      "96/96 [==============================] - 0s 659us/step - loss: 33.0894 - val_loss: 192.7867\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 132.92675\n",
      "Epoch 735/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 42.0935 - val_loss: 219.4866\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 132.92675\n",
      "Epoch 736/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 41.1883 - val_loss: 206.3508\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 132.92675\n",
      "Epoch 737/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 30.8255 - val_loss: 197.8626\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 132.92675\n",
      "Epoch 738/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 37.4415 - val_loss: 220.8162\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 132.92675\n",
      "Epoch 739/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 34.2348 - val_loss: 218.8613\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 132.92675\n",
      "Epoch 740/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 38.4797 - val_loss: 192.3381\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 132.92675\n",
      "Epoch 741/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 44.2087 - val_loss: 196.6228\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 132.92675\n",
      "Epoch 742/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 43.9146 - val_loss: 197.1453\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 132.92675\n",
      "Epoch 743/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 41.2366 - val_loss: 182.1490\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 132.92675\n",
      "Epoch 744/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 37.2247 - val_loss: 184.5923\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 132.92675\n",
      "Epoch 745/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 43.9944 - val_loss: 199.5461\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 132.92675\n",
      "Epoch 746/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 43.9149 - val_loss: 221.1230\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 132.92675\n",
      "Epoch 747/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 39.7763 - val_loss: 216.6471\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 132.92675\n",
      "Epoch 748/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 39.2731 - val_loss: 223.9361\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 132.92675\n",
      "Epoch 749/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 42.7435 - val_loss: 248.3052\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 132.92675\n",
      "Epoch 750/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 38.8721 - val_loss: 237.2042\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 132.92675\n",
      "Epoch 751/1000\n",
      "96/96 [==============================] - 0s 687us/step - loss: 39.3632 - val_loss: 215.2698\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 132.92675\n",
      "Epoch 752/1000\n",
      "96/96 [==============================] - 0s 598us/step - loss: 34.4887 - val_loss: 220.0745\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 132.92675\n",
      "Epoch 753/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 51.4381 - val_loss: 246.8119\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 132.92675\n",
      "Epoch 754/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 40.6015 - val_loss: 252.6126\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 132.92675\n",
      "Epoch 755/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 68.5857 - val_loss: 277.7961\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 132.92675\n",
      "Epoch 756/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 51.0922 - val_loss: 219.3210\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 132.92675\n",
      "Epoch 757/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 625us/step - loss: 43.0463 - val_loss: 178.4633\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 132.92675\n",
      "Epoch 758/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 38.7633 - val_loss: 200.5033\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 132.92675\n",
      "Epoch 759/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 46.0624 - val_loss: 189.8361\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 132.92675\n",
      "Epoch 760/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 41.8883 - val_loss: 201.5064\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 132.92675\n",
      "Epoch 761/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 39.8856 - val_loss: 213.4151\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 132.92675\n",
      "Epoch 762/1000\n",
      "96/96 [==============================] - 0s 648us/step - loss: 40.2814 - val_loss: 219.2701\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 132.92675\n",
      "Epoch 763/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 28.4730 - val_loss: 213.3756\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 132.92675\n",
      "Epoch 764/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 46.3843 - val_loss: 245.9348\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 132.92675\n",
      "Epoch 765/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 48.8135 - val_loss: 234.7030\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 132.92675\n",
      "Epoch 766/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.1725 - val_loss: 213.8079\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 132.92675\n",
      "Epoch 767/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 41.6365 - val_loss: 205.1020\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 132.92675\n",
      "Epoch 768/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 45.9308 - val_loss: 201.2354\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 132.92675\n",
      "Epoch 769/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 38.9754 - val_loss: 174.5602\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 132.92675\n",
      "Epoch 770/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 43.3804 - val_loss: 172.0278\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 132.92675\n",
      "Epoch 771/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 40.3702 - val_loss: 166.3088\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 132.92675\n",
      "Epoch 772/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 41.4411 - val_loss: 192.8732\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 132.92675\n",
      "Epoch 773/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 35.4854 - val_loss: 226.1650\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 132.92675\n",
      "Epoch 774/1000\n",
      "96/96 [==============================] - 0s 604us/step - loss: 44.6087 - val_loss: 190.8035\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 132.92675\n",
      "Epoch 775/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 39.1709 - val_loss: 199.6976\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 132.92675\n",
      "Epoch 776/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 45.6793 - val_loss: 202.0544\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 132.92675\n",
      "Epoch 777/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 31.4267 - val_loss: 211.6262\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 132.92675\n",
      "Epoch 778/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 46.1916 - val_loss: 243.9482\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 132.92675\n",
      "Epoch 779/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 37.4453 - val_loss: 231.8455\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 132.92675\n",
      "Epoch 780/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 39.7735 - val_loss: 201.0519\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 132.92675\n",
      "Epoch 781/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 31.0150 - val_loss: 212.4822\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 132.92675\n",
      "Epoch 782/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 44.9454 - val_loss: 192.7089\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 132.92675\n",
      "Epoch 783/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 37.3760 - val_loss: 203.8119\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 132.92675\n",
      "Epoch 784/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 48.8651 - val_loss: 195.2045\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 132.92675\n",
      "Epoch 785/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 44.6220 - val_loss: 176.8001\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 132.92675\n",
      "Epoch 786/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 34.4429 - val_loss: 180.1126\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 132.92675\n",
      "Epoch 787/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 37.4441 - val_loss: 194.2675\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 132.92675\n",
      "Epoch 788/1000\n",
      "96/96 [==============================] - 0s 598us/step - loss: 42.1453 - val_loss: 201.2600\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 132.92675\n",
      "Epoch 789/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 40.3294 - val_loss: 197.3338\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 132.92675\n",
      "Epoch 790/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 40.1300 - val_loss: 197.5895\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 132.92675\n",
      "Epoch 791/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 46.5683 - val_loss: 209.9599\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 132.92675\n",
      "Epoch 792/1000\n",
      "96/96 [==============================] - 0s 609us/step - loss: 46.7838 - val_loss: 180.7718\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 132.92675\n",
      "Epoch 793/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 45.5890 - val_loss: 181.1433\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 132.92675\n",
      "Epoch 794/1000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 39.9047 - val_loss: 185.5517\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 132.92675\n",
      "Epoch 795/1000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 34.0965 - val_loss: 193.1675\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 132.92675\n",
      "Epoch 796/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 40.7763 - val_loss: 217.2555\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 132.92675\n",
      "Epoch 797/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 40.1515 - val_loss: 187.4776\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 132.92675\n",
      "Epoch 798/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 43.9312 - val_loss: 184.7837\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 132.92675\n",
      "Epoch 799/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 36.2954 - val_loss: 182.8530\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 132.92675\n",
      "Epoch 800/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 41.0930 - val_loss: 201.7083\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 132.92675\n",
      "Epoch 801/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 39.4449 - val_loss: 223.4404\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 132.92675\n",
      "Epoch 802/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 43.2417 - val_loss: 210.7563\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 132.92675\n",
      "Epoch 803/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 43.1216 - val_loss: 205.3461\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 132.92675\n",
      "Epoch 804/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 35.6753 - val_loss: 190.0738\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 132.92675\n",
      "Epoch 805/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 39.5718 - val_loss: 192.7849\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 132.92675\n",
      "Epoch 806/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 37.7540 - val_loss: 175.1330\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 132.92675\n",
      "Epoch 807/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 51.4544 - val_loss: 191.6351\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 132.92675\n",
      "Epoch 808/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 628us/step - loss: 43.2406 - val_loss: 211.6586\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 132.92675\n",
      "Epoch 809/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 48.9878 - val_loss: 222.1256\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 132.92675\n",
      "Epoch 810/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.2906 - val_loss: 207.8587\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 132.92675\n",
      "Epoch 811/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 34.9570 - val_loss: 220.1507\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 132.92675\n",
      "Epoch 812/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 42.6029 - val_loss: 213.8104\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 132.92675\n",
      "Epoch 813/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 33.1370 - val_loss: 213.9255\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 132.92675\n",
      "Epoch 814/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 48.8766 - val_loss: 175.9164\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 132.92675\n",
      "Epoch 815/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 43.9086 - val_loss: 190.8651\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 132.92675\n",
      "Epoch 816/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 54.0499 - val_loss: 195.7143\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 132.92675\n",
      "Epoch 817/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 37.0576 - val_loss: 203.6399\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 132.92675\n",
      "Epoch 818/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 35.9996 - val_loss: 212.6974\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 132.92675\n",
      "Epoch 819/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 48.0358 - val_loss: 210.7970\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 132.92675\n",
      "Epoch 820/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 36.1762 - val_loss: 187.3075\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 132.92675\n",
      "Epoch 821/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 43.1231 - val_loss: 180.9508\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 132.92675\n",
      "Epoch 822/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 30.0626 - val_loss: 185.9013\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 132.92675\n",
      "Epoch 823/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 35.0505 - val_loss: 186.0399\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 132.92675\n",
      "Epoch 824/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 35.4658 - val_loss: 181.6297\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 132.92675\n",
      "Epoch 825/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 32.4823 - val_loss: 195.0083\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 132.92675\n",
      "Epoch 826/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 38.6717 - val_loss: 217.1050\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 132.92675\n",
      "Epoch 827/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 29.9884 - val_loss: 227.8854\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 132.92675\n",
      "Epoch 828/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 34.6562 - val_loss: 237.9135\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 132.92675\n",
      "Epoch 829/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 30.5961 - val_loss: 219.2358\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 132.92675\n",
      "Epoch 830/1000\n",
      "96/96 [==============================] - 0s 641us/step - loss: 33.0678 - val_loss: 195.2940\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 132.92675\n",
      "Epoch 831/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 32.1867 - val_loss: 200.8592\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 132.92675\n",
      "Epoch 832/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 41.4549 - val_loss: 211.2341\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 132.92675\n",
      "Epoch 833/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 32.8732 - val_loss: 191.0041\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 132.92675\n",
      "Epoch 834/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 32.8938 - val_loss: 203.1688\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 132.92675\n",
      "Epoch 835/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 36.5311 - val_loss: 225.1477\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 132.92675\n",
      "Epoch 836/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 41.2268 - val_loss: 221.3570\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 132.92675\n",
      "Epoch 837/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 31.0510 - val_loss: 203.8749\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 132.92675\n",
      "Epoch 838/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 33.0933 - val_loss: 205.2935\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 132.92675\n",
      "Epoch 839/1000\n",
      "96/96 [==============================] - 0s 666us/step - loss: 34.3374 - val_loss: 222.4787\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 132.92675\n",
      "Epoch 840/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 38.5494 - val_loss: 232.0329\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 132.92675\n",
      "Epoch 841/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 36.7283 - val_loss: 205.9839\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 132.92675\n",
      "Epoch 842/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 37.3103 - val_loss: 206.4433\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 132.92675\n",
      "Epoch 843/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 36.4885 - val_loss: 216.2423\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 132.92675\n",
      "Epoch 844/1000\n",
      "96/96 [==============================] - 0s 648us/step - loss: 46.2561 - val_loss: 197.7426\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 132.92675\n",
      "Epoch 845/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 29.1108 - val_loss: 203.0617\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 132.92675\n",
      "Epoch 846/1000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 30.9037 - val_loss: 217.6898\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 132.92675\n",
      "Epoch 847/1000\n",
      "96/96 [==============================] - 0s 605us/step - loss: 32.1035 - val_loss: 203.6299\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 132.92675\n",
      "Epoch 848/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 28.8753 - val_loss: 212.8988\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 132.92675\n",
      "Epoch 849/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 31.5916 - val_loss: 221.8332\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 132.92675\n",
      "Epoch 850/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 30.2268 - val_loss: 189.3507\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 132.92675\n",
      "Epoch 851/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 41.6145 - val_loss: 192.9189\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 132.92675\n",
      "Epoch 852/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 44.0776 - val_loss: 179.9632\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 132.92675\n",
      "Epoch 853/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 39.0412 - val_loss: 185.5766\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 132.92675\n",
      "Epoch 854/1000\n",
      "96/96 [==============================] - 0s 644us/step - loss: 46.0095 - val_loss: 189.6801\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 132.92675\n",
      "Epoch 855/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 54.6319 - val_loss: 213.1237\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 132.92675\n",
      "Epoch 856/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 50.6420 - val_loss: 237.7929\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 132.92675\n",
      "Epoch 857/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 43.7547 - val_loss: 221.8664\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 132.92675\n",
      "Epoch 858/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 44.8536 - val_loss: 201.2521\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 132.92675\n",
      "Epoch 859/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 648us/step - loss: 46.1218 - val_loss: 215.0852\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 132.92675\n",
      "Epoch 860/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 52.9715 - val_loss: 235.2433\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 132.92675\n",
      "Epoch 861/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 37.6348 - val_loss: 212.9415\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 132.92675\n",
      "Epoch 862/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 35.9974 - val_loss: 229.9758\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 132.92675\n",
      "Epoch 863/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 48.4753 - val_loss: 282.2614\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 132.92675\n",
      "Epoch 864/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 43.7744 - val_loss: 253.7795\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 132.92675\n",
      "Epoch 865/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 33.7919 - val_loss: 239.0772\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 132.92675\n",
      "Epoch 866/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 42.5171 - val_loss: 251.2645\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 132.92675\n",
      "Epoch 867/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 37.5599 - val_loss: 258.4452\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 132.92675\n",
      "Epoch 868/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 38.2812 - val_loss: 232.6774\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 132.92675\n",
      "Epoch 869/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 33.6788 - val_loss: 228.4992\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 132.92675\n",
      "Epoch 870/1000\n",
      "96/96 [==============================] - 0s 672us/step - loss: 68.3575 - val_loss: 201.1762\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 132.92675\n",
      "Epoch 871/1000\n",
      "96/96 [==============================] - 0s 655us/step - loss: 48.0175 - val_loss: 203.3176\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 132.92675\n",
      "Epoch 872/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 54.2869 - val_loss: 225.3064\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 132.92675\n",
      "Epoch 873/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 59.5266 - val_loss: 177.8034\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 132.92675\n",
      "Epoch 874/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 43.9934 - val_loss: 225.1791\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 132.92675\n",
      "Epoch 875/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 46.6825 - val_loss: 206.7050\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 132.92675\n",
      "Epoch 876/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 53.3413 - val_loss: 193.0191\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 132.92675\n",
      "Epoch 877/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 37.9643 - val_loss: 201.9776\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 132.92675\n",
      "Epoch 878/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 48.1634 - val_loss: 200.9668\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 132.92675\n",
      "Epoch 879/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 45.6757 - val_loss: 199.8437\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 132.92675\n",
      "Epoch 880/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 33.7781 - val_loss: 175.5607\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 132.92675\n",
      "Epoch 881/1000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 40.1786 - val_loss: 207.6955\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 132.92675\n",
      "Epoch 882/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 39.0365 - val_loss: 205.8956\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 132.92675\n",
      "Epoch 883/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 29.9833 - val_loss: 186.5107\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 132.92675\n",
      "Epoch 884/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 33.7997 - val_loss: 190.6290\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 132.92675\n",
      "Epoch 885/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 42.5118 - val_loss: 224.3999\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 132.92675\n",
      "Epoch 886/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 29.2124 - val_loss: 200.4092\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 132.92675\n",
      "Epoch 887/1000\n",
      "96/96 [==============================] - 0s 650us/step - loss: 54.6311 - val_loss: 207.4104\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 132.92675\n",
      "Epoch 888/1000\n",
      "96/96 [==============================] - 0s 647us/step - loss: 32.8867 - val_loss: 212.2872\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 132.92675\n",
      "Epoch 889/1000\n",
      "96/96 [==============================] - 0s 608us/step - loss: 50.7491 - val_loss: 207.5410\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 132.92675\n",
      "Epoch 890/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 41.7009 - val_loss: 222.4362\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 132.92675\n",
      "Epoch 891/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 40.7560 - val_loss: 262.0822\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 132.92675\n",
      "Epoch 892/1000\n",
      "96/96 [==============================] - 0s 653us/step - loss: 43.6445 - val_loss: 220.7431\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 132.92675\n",
      "Epoch 893/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 42.1739 - val_loss: 218.5655\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 132.92675\n",
      "Epoch 894/1000\n",
      "96/96 [==============================] - 0s 655us/step - loss: 36.4393 - val_loss: 255.4382\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 132.92675\n",
      "Epoch 895/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 30.9408 - val_loss: 258.9923\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 132.92675\n",
      "Epoch 896/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 36.0969 - val_loss: 214.6742\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 132.92675\n",
      "Epoch 897/1000\n",
      "96/96 [==============================] - 0s 635us/step - loss: 48.3027 - val_loss: 203.0113\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 132.92675\n",
      "Epoch 898/1000\n",
      "96/96 [==============================] - 0s 614us/step - loss: 46.1933 - val_loss: 178.4255\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 132.92675\n",
      "Epoch 899/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 36.6445 - val_loss: 214.3132\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 132.92675\n",
      "Epoch 900/1000\n",
      "96/96 [==============================] - 0s 618us/step - loss: 38.5354 - val_loss: 238.0723\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 132.92675\n",
      "Epoch 901/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 40.6319 - val_loss: 215.6526\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 132.92675\n",
      "Epoch 902/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 37.9111 - val_loss: 226.3584\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 132.92675\n",
      "Epoch 903/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 37.8307 - val_loss: 221.1388\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 132.92675\n",
      "Epoch 904/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 33.6903 - val_loss: 220.2994\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 132.92675\n",
      "Epoch 905/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 40.8447 - val_loss: 217.5902\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 132.92675\n",
      "Epoch 906/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 46.1705 - val_loss: 202.5014\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 132.92675\n",
      "Epoch 907/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 38.2341 - val_loss: 191.5015\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 132.92675\n",
      "Epoch 908/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 38.9663 - val_loss: 195.0232\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 132.92675\n",
      "Epoch 909/1000\n",
      "96/96 [==============================] - 0s 585us/step - loss: 37.0000 - val_loss: 189.2942\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 132.92675\n",
      "Epoch 910/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 586us/step - loss: 31.7724 - val_loss: 203.0051\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 132.92675\n",
      "Epoch 911/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 36.3644 - val_loss: 192.9616\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 132.92675\n",
      "Epoch 912/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 32.3315 - val_loss: 211.4452\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 132.92675\n",
      "Epoch 913/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 35.2105 - val_loss: 214.6767\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 132.92675\n",
      "Epoch 914/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 64.4245 - val_loss: 211.5584\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 132.92675\n",
      "Epoch 915/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 29.6356 - val_loss: 247.0730\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 132.92675\n",
      "Epoch 916/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 36.7939 - val_loss: 226.9812\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 132.92675\n",
      "Epoch 917/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 32.4863 - val_loss: 230.0165\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 132.92675\n",
      "Epoch 918/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 29.3247 - val_loss: 227.6447\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 132.92675\n",
      "Epoch 919/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 48.8557 - val_loss: 216.8660\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 132.92675\n",
      "Epoch 920/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 37.9554 - val_loss: 226.0201\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 132.92675\n",
      "Epoch 921/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 45.5008 - val_loss: 226.8299\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 132.92675\n",
      "Epoch 922/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 28.0543 - val_loss: 217.1503\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 132.92675\n",
      "Epoch 923/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 34.9284 - val_loss: 188.6865\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 132.92675\n",
      "Epoch 924/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 31.1558 - val_loss: 205.9307\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 132.92675\n",
      "Epoch 925/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 42.9218 - val_loss: 191.3196\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 132.92675\n",
      "Epoch 926/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 37.3505 - val_loss: 192.6204\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 132.92675\n",
      "Epoch 927/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 31.7557 - val_loss: 179.8545\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 132.92675\n",
      "Epoch 928/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 39.3711 - val_loss: 207.6068\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 132.92675\n",
      "Epoch 929/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 29.9626 - val_loss: 182.5618\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 132.92675\n",
      "Epoch 930/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 26.7669 - val_loss: 185.9519\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 132.92675\n",
      "Epoch 931/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 38.1998 - val_loss: 191.2921\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 132.92675\n",
      "Epoch 932/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 29.3857 - val_loss: 184.1835\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 132.92675\n",
      "Epoch 933/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 33.9920 - val_loss: 193.0113\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 132.92675\n",
      "Epoch 934/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 30.8943 - val_loss: 210.8701\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 132.92675\n",
      "Epoch 935/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 31.6568 - val_loss: 219.7818\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 132.92675\n",
      "Epoch 936/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 34.7356 - val_loss: 247.5635\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 132.92675\n",
      "Epoch 937/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 26.6205 - val_loss: 238.2061\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 132.92675\n",
      "Epoch 938/1000\n",
      "96/96 [==============================] - 0s 656us/step - loss: 40.6103 - val_loss: 210.5643\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 132.92675\n",
      "Epoch 939/1000\n",
      "96/96 [==============================] - 0s 666us/step - loss: 38.2443 - val_loss: 210.3251\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 132.92675\n",
      "Epoch 940/1000\n",
      "96/96 [==============================] - 0s 630us/step - loss: 32.6174 - val_loss: 250.8291\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 132.92675\n",
      "Epoch 941/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 36.6044 - val_loss: 228.4819\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 132.92675\n",
      "Epoch 942/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 28.6245 - val_loss: 184.5320\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 132.92675\n",
      "Epoch 943/1000\n",
      "96/96 [==============================] - 0s 645us/step - loss: 37.2136 - val_loss: 208.9485\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 132.92675\n",
      "Epoch 944/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 30.8177 - val_loss: 229.1710\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 132.92675\n",
      "Epoch 945/1000\n",
      "96/96 [==============================] - 0s 624us/step - loss: 33.4431 - val_loss: 211.5392\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 132.92675\n",
      "Epoch 946/1000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 34.9260 - val_loss: 228.0947\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 132.92675\n",
      "Epoch 947/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 35.7845 - val_loss: 196.1741\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 132.92675\n",
      "Epoch 948/1000\n",
      "96/96 [==============================] - 0s 658us/step - loss: 33.3983 - val_loss: 194.9160\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 132.92675\n",
      "Epoch 949/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 32.8206 - val_loss: 221.8215\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 132.92675\n",
      "Epoch 950/1000\n",
      "96/96 [==============================] - 0s 616us/step - loss: 27.8221 - val_loss: 211.4535\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 132.92675\n",
      "Epoch 951/1000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 31.3594 - val_loss: 207.1291\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 132.92675\n",
      "Epoch 952/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 31.4507 - val_loss: 214.7513\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 132.92675\n",
      "Epoch 953/1000\n",
      "96/96 [==============================] - 0s 619us/step - loss: 31.9432 - val_loss: 248.2085\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 132.92675\n",
      "Epoch 954/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 52.0865 - val_loss: 199.0243\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 132.92675\n",
      "Epoch 955/1000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 33.0908 - val_loss: 207.4593\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 132.92675\n",
      "Epoch 956/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 43.3611 - val_loss: 309.5731\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 132.92675\n",
      "Epoch 957/1000\n",
      "96/96 [==============================] - 0s 629us/step - loss: 31.3120 - val_loss: 287.1775\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 132.92675\n",
      "Epoch 958/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 35.3161 - val_loss: 246.5275\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 132.92675\n",
      "Epoch 959/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 40.4378 - val_loss: 269.3864\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 132.92675\n",
      "Epoch 960/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 33.7046 - val_loss: 239.0328\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 132.92675\n",
      "Epoch 961/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 614us/step - loss: 39.5596 - val_loss: 199.7929\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 132.92675\n",
      "Epoch 962/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 43.9438 - val_loss: 189.9886\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 132.92675\n",
      "Epoch 963/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 35.0661 - val_loss: 197.8250\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 132.92675\n",
      "Epoch 964/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 34.9129 - val_loss: 195.7858\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 132.92675\n",
      "Epoch 965/1000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 38.0052 - val_loss: 210.1721\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 132.92675\n",
      "Epoch 966/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 34.6689 - val_loss: 223.4246\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 132.92675\n",
      "Epoch 967/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 31.7163 - val_loss: 211.0853\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 132.92675\n",
      "Epoch 968/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 28.1949 - val_loss: 207.2041\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 132.92675\n",
      "Epoch 969/1000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 31.9974 - val_loss: 219.4693\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 132.92675\n",
      "Epoch 970/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 33.1484 - val_loss: 232.7742\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 132.92675\n",
      "Epoch 971/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 45.1633 - val_loss: 241.0347\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 132.92675\n",
      "Epoch 972/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 35.0735 - val_loss: 240.0222\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 132.92675\n",
      "Epoch 973/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 35.4773 - val_loss: 200.5916\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 132.92675\n",
      "Epoch 974/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.5368 - val_loss: 197.8645\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 132.92675\n",
      "Epoch 975/1000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 29.8063 - val_loss: 209.3830\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 132.92675\n",
      "Epoch 976/1000\n",
      "96/96 [==============================] - 0s 615us/step - loss: 30.1571 - val_loss: 222.4362\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 132.92675\n",
      "Epoch 977/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 49.7428 - val_loss: 238.6017\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 132.92675\n",
      "Epoch 978/1000\n",
      "96/96 [==============================] - 0s 622us/step - loss: 36.5878 - val_loss: 224.8046\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 132.92675\n",
      "Epoch 979/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 46.0021 - val_loss: 175.0640\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 132.92675\n",
      "Epoch 980/1000\n",
      "96/96 [==============================] - 0s 617us/step - loss: 32.3740 - val_loss: 196.3874\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 132.92675\n",
      "Epoch 981/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 36.5386 - val_loss: 209.5843\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 132.92675\n",
      "Epoch 982/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 24.5539 - val_loss: 193.8762\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 132.92675\n",
      "Epoch 983/1000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 40.3748 - val_loss: 190.0012\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 132.92675\n",
      "Epoch 984/1000\n",
      "96/96 [==============================] - 0s 631us/step - loss: 28.8035 - val_loss: 213.7206\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 132.92675\n",
      "Epoch 985/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 30.3839 - val_loss: 197.3664\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 132.92675\n",
      "Epoch 986/1000\n",
      "96/96 [==============================] - 0s 636us/step - loss: 56.4559 - val_loss: 177.8284\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 132.92675\n",
      "Epoch 987/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 31.3544 - val_loss: 189.3392\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 132.92675\n",
      "Epoch 988/1000\n",
      "96/96 [==============================] - 0s 632us/step - loss: 33.5994 - val_loss: 183.5441\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 132.92675\n",
      "Epoch 989/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 34.6482 - val_loss: 196.1976\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 132.92675\n",
      "Epoch 990/1000\n",
      "96/96 [==============================] - 0s 640us/step - loss: 41.4623 - val_loss: 211.2933\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 132.92675\n",
      "Epoch 991/1000\n",
      "96/96 [==============================] - 0s 623us/step - loss: 43.1386 - val_loss: 169.6465\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 132.92675\n",
      "Epoch 992/1000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 40.5081 - val_loss: 200.7832\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 132.92675\n",
      "Epoch 993/1000\n",
      "96/96 [==============================] - 0s 633us/step - loss: 48.5705 - val_loss: 214.1639\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 132.92675\n",
      "Epoch 994/1000\n",
      "96/96 [==============================] - 0s 621us/step - loss: 29.5582 - val_loss: 211.1506\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 132.92675\n",
      "Epoch 995/1000\n",
      "96/96 [==============================] - 0s 634us/step - loss: 70.8506 - val_loss: 198.1762\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 132.92675\n",
      "Epoch 996/1000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 40.9298 - val_loss: 202.2402\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 132.92675\n",
      "Epoch 997/1000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 50.9026 - val_loss: 188.2328\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 132.92675\n",
      "Epoch 998/1000\n",
      "96/96 [==============================] - 0s 625us/step - loss: 37.9414 - val_loss: 210.3479\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 132.92675\n",
      "Epoch 999/1000\n",
      "96/96 [==============================] - 0s 620us/step - loss: 39.6363 - val_loss: 247.7677\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 132.92675\n",
      "Epoch 1000/1000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 40.3748 - val_loss: 214.2676\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 132.92675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe85434b7b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "vae.fit(x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None), \n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_output = sess.run( z_mean_std, feed_dict={'encoder_input: 0':x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17869286, -0.08096359,  0.04486062], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m = np.mean(z_output,axis=0)\n",
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01052645e+00, -2.98681525e-09,  7.20108540e-09],\n",
       "       [-2.98681525e-09,  1.01052618e+00, -1.08011360e-08],\n",
       "       [ 7.20108540e-09, -1.08011360e-08,  1.01052639e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(z_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Pearson correlation among Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe855a18a90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(np.corrcoef(z_output.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17869286, -0.08096359,  0.04486062], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, latent_dim):\n",
    "    plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num = i,\n",
    "                 z_m_m = z_m_m ,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
