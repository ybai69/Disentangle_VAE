{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7f8ee0864cc0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2376x2376 with 121 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Change figure aesthetics\n",
    "%matplotlib inline\n",
    "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz'\n",
    "                      , encoding='bytes')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]\n",
    "\n",
    "# Define number of values per latents and functions to convert to indices\n",
    "latents_sizes =  np.array([ 1,  3,  6, 40, 32, 32])\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "  return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "# Helper function to show images\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "  ncols = int(np.ceil(num_images**0.5))\n",
    "  nrows = int(np.ceil(num_images / ncols))\n",
    "  _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for ax_i, ax in enumerate(axes):\n",
    "    if ax_i < num_images:\n",
    "      ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "    else:\n",
    "      ax.axis('off')\n",
    "\n",
    "def show_density(imgs):\n",
    "  _, ax = plt.subplots()\n",
    "  ax.imshow(imgs.mean(axis=0), interpolation='nearest', cmap='Greys_r')\n",
    "  ax.grid('off')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "from copy import deepcopy\n",
    "## Fix posX latent to left\n",
    "#latents_sampled = sample_latent(size=5000)\n",
    "latents_sampled = deepcopy(latents_classes)\n",
    "latents_sampled[:, [4,5]] = 15\n",
    "latents_sampled[:,2]= 5\n",
    "\n",
    "\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[np.unique(indices_sampled)]\n",
    "\n",
    "#np.unique(indices_sampled)\n",
    "\n",
    "# Samples\n",
    "show_images_grid(imgs_sampled,len(np.unique(indices_sampled)))\n",
    "\n",
    "# Show the density too to check\n",
    "show_density(imgs_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(69)\n",
    "ff = imgs_sampled\n",
    "n_data =  ff.shape[0]\n",
    "n_train = int(np.ceil(n_data*0.8))\n",
    "\n",
    "print(n_train)\n",
    "idx_train = random.sample(range(n_data), n_train)\n",
    "idx_test = np.delete(range(n_data),idx_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   6,  14,  15,  20,  22,  23,  24,  28,  32,  43,  46,\n",
       "        51,  55,  67,  85,  92,  93,  96, 104, 105, 110, 118])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape[0]*0.8\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train / validation folds\n",
    "#np.random.seed(42)\n",
    "\n",
    "img_rows = ff.shape[1]\n",
    "img_cols = ff.shape[2]\n",
    "\n",
    "n_pixels = img_rows * img_cols\n",
    "x_train = ff[idx_train]\n",
    "x_test = ff[idx_test]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32') \n",
    "x_train = x_train.reshape((len(x_train), n_pixels))\n",
    "x_test = x_test.reshape((len(x_test), n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4608x4608 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_examples(data, n=None, n_cols=20, thumbnail_cb=None):\n",
    "    if n is None:\n",
    "        n = len(data)    \n",
    "    n_rows = int(np.ceil(n / float(n_cols)))\n",
    "    figure = np.zeros((img_rows * n_rows, img_cols * n_cols))\n",
    "    for k, x in enumerate(data[:n]):\n",
    "        r = k // n_cols\n",
    "        c = k % n_cols\n",
    "        figure[r * img_rows: (r + 1) * img_rows,\n",
    "               c * img_cols: (c + 1) * img_cols] = x\n",
    "        if thumbnail_cb is not None:\n",
    "            thumbnail_cb(locals())\n",
    "        \n",
    "    plt.figure(figsize=(64, 64))\n",
    "    plt.imshow(figure)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "show_examples(ff, n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "#     return z_mean + K.exp(0.5 ) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num,\n",
    "                 z_m_m, \n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_face\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    #x_test = data\n",
    "    latent_dim = latent_dim\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(model_name, \"face_over_latent.png\")\n",
    "    n = 20\n",
    "    #digit_size = 28\n",
    "    img_rows, img_cols = 64, 64\n",
    "    figure = np.zeros((img_rows , img_cols * n))\n",
    "    grid_x = np.linspace(-5, 5, n)\n",
    "    #grid_y = np.linspace(-5, 5, n)[::-1]\n",
    "    z_sample = np.zeros((1,latent_dim))\n",
    "    z_sample[0,:] = z_m_m \n",
    "    \n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample[0,latent_num] = xi\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_rows, img_cols)\n",
    "        figure[0: img_rows,j * img_cols: (j + 1) * img_cols] = digit\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    #start_range = digit_size // 2\n",
    "    #end_range = n * digit_size + start_range + 1\n",
    "    #pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    #sample_range_x = np.round(grid_x, 1)\n",
    "    #sample_range_y = np.round(grid_y, 1)\n",
    "    #plt.xticks(pixel_range, sample_range_x)\n",
    "    #plt.yticks(pixel_range, sample_range_y)\n",
    "    #plt.xlabel(\"z[0]\")\n",
    "    #plt.ylabel(\"z[1]\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "original_dim = n_pixels\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim1 = 512\n",
    "intermediate_dim2 = 256\n",
    "intermediate_dim3 = 64\n",
    "\n",
    "batch_size = 20\n",
    "latent_dim = 2\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x1)\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(x2)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x3)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the mean of z, so that mean(m_z)=0 and cov(m_z)=I\n",
    "def standardize(z_mean):\n",
    "    z_m_m = K.mean(z_mean,axis=0, keepdims=True)\n",
    "    z1 = z_mean - z_m_m\n",
    "    n = tf.cast(K.shape(z_mean)[0], tf.float32)\n",
    "    cov = K.transpose(z1) @ z1 /n\n",
    "    \n",
    "    D = tf.diag(tf.diag_part(cov)) ** 0.5\n",
    "    \n",
    "    L = tf.linalg.inv(tf.transpose(tf.cholesky(cov)))\n",
    "       \n",
    "#     z2 = z1 @ L @ D +  z_m_m\n",
    "    z2 = z1 @ L  +  z_m_m\n",
    "\n",
    "    return( z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_mean_std = Lambda(standardize, output_shape=(latent_dim,), name='z_mean_std')(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            130         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            130         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,245,700\n",
      "Trainable params: 2,245,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 2,249,664\n",
      "Trainable params: 2,249,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z')\n",
    "\n",
    "x3 = Dense(intermediate_dim3, activation='relu')(latent_inputs)\n",
    "x2 = Dense(intermediate_dim2, activation='relu')(x3)\n",
    "x1 = Dense(intermediate_dim1, activation='relu')(x2)\n",
    "\n",
    "# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x1)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 2245700   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 4096)              2249664   \n",
      "=================================================================\n",
      "Total params: 4,495,364\n",
      "Trainable params: 4,495,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test )\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "\n",
    "#     reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(1E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/1000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 2742.3188 - val_loss: 2098.8696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2098.86963, saving model to weights.hdf5\n",
      "Epoch 2/1000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 1199.2041 - val_loss: 711.0702\n",
      "\n",
      "Epoch 00002: val_loss improved from 2098.86963 to 711.07024, saving model to weights.hdf5\n",
      "Epoch 3/1000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 631.8150 - val_loss: 481.2399\n",
      "\n",
      "Epoch 00003: val_loss improved from 711.07024 to 481.23988, saving model to weights.hdf5\n",
      "Epoch 4/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 371.0574 - val_loss: 307.8800\n",
      "\n",
      "Epoch 00004: val_loss improved from 481.23988 to 307.88002, saving model to weights.hdf5\n",
      "Epoch 5/1000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 283.8853 - val_loss: 269.6936\n",
      "\n",
      "Epoch 00005: val_loss improved from 307.88002 to 269.69364, saving model to weights.hdf5\n",
      "Epoch 6/1000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 254.6653 - val_loss: 238.4522\n",
      "\n",
      "Epoch 00006: val_loss improved from 269.69364 to 238.45217, saving model to weights.hdf5\n",
      "Epoch 7/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 237.7566 - val_loss: 242.7907\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 238.45217\n",
      "Epoch 8/1000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 229.9064 - val_loss: 233.4883\n",
      "\n",
      "Epoch 00008: val_loss improved from 238.45217 to 233.48832, saving model to weights.hdf5\n",
      "Epoch 9/1000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 226.2844 - val_loss: 231.2293\n",
      "\n",
      "Epoch 00009: val_loss improved from 233.48832 to 231.22929, saving model to weights.hdf5\n",
      "Epoch 10/1000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 220.3967 - val_loss: 228.2128\n",
      "\n",
      "Epoch 00010: val_loss improved from 231.22929 to 228.21275, saving model to weights.hdf5\n",
      "Epoch 11/1000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 218.8115 - val_loss: 225.0003\n",
      "\n",
      "Epoch 00011: val_loss improved from 228.21275 to 225.00027, saving model to weights.hdf5\n",
      "Epoch 12/1000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 216.1675 - val_loss: 223.7682\n",
      "\n",
      "Epoch 00012: val_loss improved from 225.00027 to 223.76821, saving model to weights.hdf5\n",
      "Epoch 13/1000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 215.3268 - val_loss: 223.4966\n",
      "\n",
      "Epoch 00013: val_loss improved from 223.76821 to 223.49657, saving model to weights.hdf5\n",
      "Epoch 14/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 214.5145 - val_loss: 220.6201\n",
      "\n",
      "Epoch 00014: val_loss improved from 223.49657 to 220.62007, saving model to weights.hdf5\n",
      "Epoch 15/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 214.0066 - val_loss: 221.6958\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 220.62007\n",
      "Epoch 16/1000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 212.9143 - val_loss: 221.9533\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 220.62007\n",
      "Epoch 17/1000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 212.7952 - val_loss: 221.6856\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 220.62007\n",
      "Epoch 18/1000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 212.3073 - val_loss: 220.1281\n",
      "\n",
      "Epoch 00018: val_loss improved from 220.62007 to 220.12807, saving model to weights.hdf5\n",
      "Epoch 19/1000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 211.6091 - val_loss: 219.4317\n",
      "\n",
      "Epoch 00019: val_loss improved from 220.12807 to 219.43167, saving model to weights.hdf5\n",
      "Epoch 20/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 211.6576 - val_loss: 220.4266\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 219.43167\n",
      "Epoch 21/1000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 211.1270 - val_loss: 219.9468\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 219.43167\n",
      "Epoch 22/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 211.0963 - val_loss: 218.3297\n",
      "\n",
      "Epoch 00022: val_loss improved from 219.43167 to 218.32970, saving model to weights.hdf5\n",
      "Epoch 23/1000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 211.3670 - val_loss: 220.9309\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 218.32970\n",
      "Epoch 24/1000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 211.7630 - val_loss: 219.6279\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 218.32970\n",
      "Epoch 25/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 210.8348 - val_loss: 219.5053\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 218.32970\n",
      "Epoch 26/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 210.1464 - val_loss: 217.4693\n",
      "\n",
      "Epoch 00026: val_loss improved from 218.32970 to 217.46932, saving model to weights.hdf5\n",
      "Epoch 27/1000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 210.3331 - val_loss: 217.1730\n",
      "\n",
      "Epoch 00027: val_loss improved from 217.46932 to 217.17301, saving model to weights.hdf5\n",
      "Epoch 28/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 210.1396 - val_loss: 220.0963\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 217.17301\n",
      "Epoch 29/1000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 210.1551 - val_loss: 219.5925\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 217.17301\n",
      "Epoch 30/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 208.9909 - val_loss: 218.1004\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 217.17301\n",
      "Epoch 31/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 209.3586 - val_loss: 216.0337\n",
      "\n",
      "Epoch 00031: val_loss improved from 217.17301 to 216.03374, saving model to weights.hdf5\n",
      "Epoch 32/1000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 208.9504 - val_loss: 217.7728\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 216.03374\n",
      "Epoch 33/1000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 207.7753 - val_loss: 218.6914\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 216.03374\n",
      "Epoch 34/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 205.2589 - val_loss: 215.8898\n",
      "\n",
      "Epoch 00034: val_loss improved from 216.03374 to 215.88983, saving model to weights.hdf5\n",
      "Epoch 35/1000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 202.5746 - val_loss: 213.5337\n",
      "\n",
      "Epoch 00035: val_loss improved from 215.88983 to 213.53370, saving model to weights.hdf5\n",
      "Epoch 36/1000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 199.0242 - val_loss: 207.5607\n",
      "\n",
      "Epoch 00036: val_loss improved from 213.53370 to 207.56071, saving model to weights.hdf5\n",
      "Epoch 37/1000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 191.9870 - val_loss: 204.7652\n",
      "\n",
      "Epoch 00037: val_loss improved from 207.56071 to 204.76516, saving model to weights.hdf5\n",
      "Epoch 38/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 189.3980 - val_loss: 202.0367\n",
      "\n",
      "Epoch 00038: val_loss improved from 204.76516 to 202.03674, saving model to weights.hdf5\n",
      "Epoch 39/1000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 188.0555 - val_loss: 196.4986\n",
      "\n",
      "Epoch 00039: val_loss improved from 202.03674 to 196.49861, saving model to weights.hdf5\n",
      "Epoch 40/1000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 186.5096 - val_loss: 196.5313\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 196.49861\n",
      "Epoch 41/1000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 185.6685 - val_loss: 193.3566\n",
      "\n",
      "Epoch 00041: val_loss improved from 196.49861 to 193.35658, saving model to weights.hdf5\n",
      "Epoch 42/1000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 185.0025 - val_loss: 194.8824\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 193.35658\n",
      "Epoch 43/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 184.0848 - val_loss: 193.3272\n",
      "\n",
      "Epoch 00043: val_loss improved from 193.35658 to 193.32718, saving model to weights.hdf5\n",
      "Epoch 44/1000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 182.8465 - val_loss: 190.7897\n",
      "\n",
      "Epoch 00044: val_loss improved from 193.32718 to 190.78968, saving model to weights.hdf5\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 417us/step - loss: 182.7694 - val_loss: 191.0022\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 190.78968\n",
      "Epoch 46/1000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 182.2860 - val_loss: 191.7539\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 190.78968\n",
      "Epoch 47/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 181.8246 - val_loss: 190.9644\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 190.78968\n",
      "Epoch 48/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 181.3391 - val_loss: 189.6814\n",
      "\n",
      "Epoch 00048: val_loss improved from 190.78968 to 189.68136, saving model to weights.hdf5\n",
      "Epoch 49/1000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 181.2598 - val_loss: 191.6513\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 189.68136\n",
      "Epoch 50/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 181.0310 - val_loss: 190.9663\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 189.68136\n",
      "Epoch 51/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 179.9909 - val_loss: 187.4167\n",
      "\n",
      "Epoch 00051: val_loss improved from 189.68136 to 187.41672, saving model to weights.hdf5\n",
      "Epoch 52/1000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 178.9090 - val_loss: 188.0096\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 187.41672\n",
      "Epoch 53/1000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 178.1084 - val_loss: 187.0148\n",
      "\n",
      "Epoch 00053: val_loss improved from 187.41672 to 187.01480, saving model to weights.hdf5\n",
      "Epoch 54/1000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 176.6172 - val_loss: 186.0972\n",
      "\n",
      "Epoch 00054: val_loss improved from 187.01480 to 186.09715, saving model to weights.hdf5\n",
      "Epoch 55/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 174.9829 - val_loss: 187.2563\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 186.09715\n",
      "Epoch 56/1000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 174.3933 - val_loss: 188.2951\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 186.09715\n",
      "Epoch 57/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 171.3008 - val_loss: 183.0273\n",
      "\n",
      "Epoch 00057: val_loss improved from 186.09715 to 183.02727, saving model to weights.hdf5\n",
      "Epoch 58/1000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 168.6157 - val_loss: 189.9440\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 183.02727\n",
      "Epoch 59/1000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 165.7012 - val_loss: 179.1236\n",
      "\n",
      "Epoch 00059: val_loss improved from 183.02727 to 179.12361, saving model to weights.hdf5\n",
      "Epoch 60/1000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 163.7429 - val_loss: 175.9075\n",
      "\n",
      "Epoch 00060: val_loss improved from 179.12361 to 175.90748, saving model to weights.hdf5\n",
      "Epoch 61/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 158.8113 - val_loss: 171.7458\n",
      "\n",
      "Epoch 00061: val_loss improved from 175.90748 to 171.74577, saving model to weights.hdf5\n",
      "Epoch 62/1000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 156.5649 - val_loss: 176.3148\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 171.74577\n",
      "Epoch 63/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 154.0139 - val_loss: 169.3536\n",
      "\n",
      "Epoch 00063: val_loss improved from 171.74577 to 169.35365, saving model to weights.hdf5\n",
      "Epoch 64/1000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 149.5861 - val_loss: 164.0132\n",
      "\n",
      "Epoch 00064: val_loss improved from 169.35365 to 164.01315, saving model to weights.hdf5\n",
      "Epoch 65/1000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 146.1172 - val_loss: 163.1938\n",
      "\n",
      "Epoch 00065: val_loss improved from 164.01315 to 163.19380, saving model to weights.hdf5\n",
      "Epoch 66/1000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 142.7306 - val_loss: 160.1690\n",
      "\n",
      "Epoch 00066: val_loss improved from 163.19380 to 160.16896, saving model to weights.hdf5\n",
      "Epoch 67/1000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 139.1368 - val_loss: 157.4759\n",
      "\n",
      "Epoch 00067: val_loss improved from 160.16896 to 157.47593, saving model to weights.hdf5\n",
      "Epoch 68/1000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 138.8146 - val_loss: 157.3974\n",
      "\n",
      "Epoch 00068: val_loss improved from 157.47593 to 157.39735, saving model to weights.hdf5\n",
      "Epoch 69/1000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 136.2977 - val_loss: 158.6181\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 157.39735\n",
      "Epoch 70/1000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 134.7478 - val_loss: 158.5725\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 157.39735\n",
      "Epoch 71/1000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 133.3719 - val_loss: 156.2301\n",
      "\n",
      "Epoch 00071: val_loss improved from 157.39735 to 156.23014, saving model to weights.hdf5\n",
      "Epoch 72/1000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 132.6700 - val_loss: 166.3958\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 156.23014\n",
      "Epoch 73/1000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 131.9313 - val_loss: 153.2796\n",
      "\n",
      "Epoch 00073: val_loss improved from 156.23014 to 153.27959, saving model to weights.hdf5\n",
      "Epoch 74/1000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 131.0508 - val_loss: 149.9708\n",
      "\n",
      "Epoch 00074: val_loss improved from 153.27959 to 149.97085, saving model to weights.hdf5\n",
      "Epoch 75/1000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 127.9848 - val_loss: 150.9150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 149.97085\n",
      "Epoch 76/1000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 127.8305 - val_loss: 153.9373\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 149.97085\n",
      "Epoch 77/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 130.6427 - val_loss: 161.4532\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 149.97085\n",
      "Epoch 78/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 130.4881 - val_loss: 145.8320\n",
      "\n",
      "Epoch 00078: val_loss improved from 149.97085 to 145.83205, saving model to weights.hdf5\n",
      "Epoch 79/1000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 127.4263 - val_loss: 145.1309\n",
      "\n",
      "Epoch 00079: val_loss improved from 145.83205 to 145.13093, saving model to weights.hdf5\n",
      "Epoch 80/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 125.9189 - val_loss: 149.7987\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 145.13093\n",
      "Epoch 81/1000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 121.3819 - val_loss: 142.6067\n",
      "\n",
      "Epoch 00081: val_loss improved from 145.13093 to 142.60673, saving model to weights.hdf5\n",
      "Epoch 82/1000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 116.3628 - val_loss: 136.3472\n",
      "\n",
      "Epoch 00082: val_loss improved from 142.60673 to 136.34722, saving model to weights.hdf5\n",
      "Epoch 83/1000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 113.7815 - val_loss: 136.2498\n",
      "\n",
      "Epoch 00083: val_loss improved from 136.34722 to 136.24981, saving model to weights.hdf5\n",
      "Epoch 84/1000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 112.9634 - val_loss: 146.1900\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 136.24981\n",
      "Epoch 85/1000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 111.6380 - val_loss: 134.7664\n",
      "\n",
      "Epoch 00085: val_loss improved from 136.24981 to 134.76641, saving model to weights.hdf5\n",
      "Epoch 86/1000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 109.3820 - val_loss: 135.2111\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 134.76641\n",
      "Epoch 87/1000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 108.9096 - val_loss: 137.4718\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 134.76641\n",
      "Epoch 88/1000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 105.7214 - val_loss: 132.5907\n",
      "\n",
      "Epoch 00088: val_loss improved from 134.76641 to 132.59068, saving model to weights.hdf5\n",
      "Epoch 89/1000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 103.4386 - val_loss: 128.7253\n",
      "\n",
      "Epoch 00089: val_loss improved from 132.59068 to 128.72531, saving model to weights.hdf5\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 430us/step - loss: 103.1798 - val_loss: 129.7861\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 128.72531\n",
      "Epoch 91/1000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 103.2939 - val_loss: 128.8779\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 128.72531\n",
      "Epoch 92/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 101.9666 - val_loss: 125.9282\n",
      "\n",
      "Epoch 00092: val_loss improved from 128.72531 to 125.92819, saving model to weights.hdf5\n",
      "Epoch 93/1000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 100.7218 - val_loss: 130.5104\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 125.92819\n",
      "Epoch 94/1000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 101.5799 - val_loss: 127.9463\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 125.92819\n",
      "Epoch 95/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 98.3982 - val_loss: 126.1054\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 125.92819\n",
      "Epoch 96/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 97.7278 - val_loss: 130.2846\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 125.92819\n",
      "Epoch 97/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 97.2163 - val_loss: 122.3302\n",
      "\n",
      "Epoch 00097: val_loss improved from 125.92819 to 122.33019, saving model to weights.hdf5\n",
      "Epoch 98/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 94.8890 - val_loss: 118.0845\n",
      "\n",
      "Epoch 00098: val_loss improved from 122.33019 to 118.08455, saving model to weights.hdf5\n",
      "Epoch 99/1000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 93.7899 - val_loss: 126.1919\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 118.08455\n",
      "Epoch 100/1000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 92.3509 - val_loss: 117.3233\n",
      "\n",
      "Epoch 00100: val_loss improved from 118.08455 to 117.32335, saving model to weights.hdf5\n",
      "Epoch 101/1000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 91.5137 - val_loss: 120.0525\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 117.32335\n",
      "Epoch 102/1000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 92.1595 - val_loss: 116.6896\n",
      "\n",
      "Epoch 00102: val_loss improved from 117.32335 to 116.68959, saving model to weights.hdf5\n",
      "Epoch 103/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 91.4046 - val_loss: 120.1204\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 116.68959\n",
      "Epoch 104/1000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 91.7352 - val_loss: 128.0115\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 116.68959\n",
      "Epoch 105/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 92.9065 - val_loss: 116.6178\n",
      "\n",
      "Epoch 00105: val_loss improved from 116.68959 to 116.61778, saving model to weights.hdf5\n",
      "Epoch 106/1000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 91.1067 - val_loss: 118.5859\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 116.61778\n",
      "Epoch 107/1000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 88.2465 - val_loss: 115.4992\n",
      "\n",
      "Epoch 00107: val_loss improved from 116.61778 to 115.49919, saving model to weights.hdf5\n",
      "Epoch 108/1000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 87.0431 - val_loss: 116.1633\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 115.49919\n",
      "Epoch 109/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 85.2708 - val_loss: 117.5999\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 115.49919\n",
      "Epoch 110/1000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 86.1291 - val_loss: 115.5925\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 115.49919\n",
      "Epoch 111/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 85.3334 - val_loss: 120.0287\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 115.49919\n",
      "Epoch 112/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 86.9632 - val_loss: 115.0705\n",
      "\n",
      "Epoch 00112: val_loss improved from 115.49919 to 115.07054, saving model to weights.hdf5\n",
      "Epoch 113/1000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 87.1520 - val_loss: 111.9127\n",
      "\n",
      "Epoch 00113: val_loss improved from 115.07054 to 111.91274, saving model to weights.hdf5\n",
      "Epoch 114/1000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 85.2976 - val_loss: 110.9150\n",
      "\n",
      "Epoch 00114: val_loss improved from 111.91274 to 110.91502, saving model to weights.hdf5\n",
      "Epoch 115/1000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 82.4358 - val_loss: 113.7615\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 110.91502\n",
      "Epoch 116/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 80.4979 - val_loss: 112.7397\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 110.91502\n",
      "Epoch 117/1000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 79.0345 - val_loss: 106.0044\n",
      "\n",
      "Epoch 00117: val_loss improved from 110.91502 to 106.00443, saving model to weights.hdf5\n",
      "Epoch 118/1000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 76.9493 - val_loss: 106.8906\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 106.00443\n",
      "Epoch 119/1000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 76.1774 - val_loss: 107.1548\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 106.00443\n",
      "Epoch 120/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 76.5255 - val_loss: 108.3651\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 106.00443\n",
      "Epoch 121/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 74.7508 - val_loss: 107.7121\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 106.00443\n",
      "Epoch 122/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 75.1552 - val_loss: 105.1890\n",
      "\n",
      "Epoch 00122: val_loss improved from 106.00443 to 105.18900, saving model to weights.hdf5\n",
      "Epoch 123/1000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 73.1162 - val_loss: 105.8324\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 105.18900\n",
      "Epoch 124/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 72.9422 - val_loss: 113.1838\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 105.18900\n",
      "Epoch 125/1000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 74.1055 - val_loss: 114.5445\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 105.18900\n",
      "Epoch 126/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 72.8099 - val_loss: 106.9207\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 105.18900\n",
      "Epoch 127/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 71.8594 - val_loss: 105.7451\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 105.18900\n",
      "Epoch 128/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 69.9441 - val_loss: 108.6092\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 105.18900\n",
      "Epoch 129/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 69.6013 - val_loss: 109.8839\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 105.18900\n",
      "Epoch 130/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 69.2758 - val_loss: 110.4862\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 105.18900\n",
      "Epoch 131/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 69.8745 - val_loss: 107.9601\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 105.18900\n",
      "Epoch 132/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 67.4049 - val_loss: 105.2158\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 105.18900\n",
      "Epoch 133/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 67.2180 - val_loss: 107.0155\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 105.18900\n",
      "Epoch 134/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 65.6679 - val_loss: 104.9571\n",
      "\n",
      "Epoch 00134: val_loss improved from 105.18900 to 104.95709, saving model to weights.hdf5\n",
      "Epoch 135/1000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 65.3792 - val_loss: 105.3310\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 104.95709\n",
      "Epoch 136/1000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 65.5769 - val_loss: 107.5744\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 104.95709\n",
      "Epoch 137/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 63.6979 - val_loss: 103.4357\n",
      "\n",
      "Epoch 00137: val_loss improved from 104.95709 to 103.43565, saving model to weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/1000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 63.3695 - val_loss: 104.8935\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 103.43565\n",
      "Epoch 139/1000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 63.7705 - val_loss: 108.5985\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 103.43565\n",
      "Epoch 140/1000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 63.6324 - val_loss: 107.4518\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 103.43565\n",
      "Epoch 141/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 62.6410 - val_loss: 105.1237\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 103.43565\n",
      "Epoch 142/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 61.4699 - val_loss: 108.7029\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 103.43565\n",
      "Epoch 143/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 60.7690 - val_loss: 110.0805\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 103.43565\n",
      "Epoch 144/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 60.6080 - val_loss: 102.5206\n",
      "\n",
      "Epoch 00144: val_loss improved from 103.43565 to 102.52059, saving model to weights.hdf5\n",
      "Epoch 145/1000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 60.7166 - val_loss: 114.2304\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 102.52059\n",
      "Epoch 146/1000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 63.5173 - val_loss: 114.9849\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 102.52059\n",
      "Epoch 147/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 62.4818 - val_loss: 106.5773\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 102.52059\n",
      "Epoch 148/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 62.0300 - val_loss: 108.1031\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 102.52059\n",
      "Epoch 149/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 60.7310 - val_loss: 110.3585\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 102.52059\n",
      "Epoch 150/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 61.4833 - val_loss: 105.2445\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 102.52059\n",
      "Epoch 151/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 60.0825 - val_loss: 106.1781\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 102.52059\n",
      "Epoch 152/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 61.9875 - val_loss: 108.3959\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 102.52059\n",
      "Epoch 153/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 65.1210 - val_loss: 111.0841\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 102.52059\n",
      "Epoch 154/1000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 68.1746 - val_loss: 114.7234\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 102.52059\n",
      "Epoch 155/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 72.0300 - val_loss: 121.8785\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 102.52059\n",
      "Epoch 156/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 68.2273 - val_loss: 113.3104\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 102.52059\n",
      "Epoch 157/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 64.0591 - val_loss: 117.5428\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 102.52059\n",
      "Epoch 158/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 65.7871 - val_loss: 114.3706\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 102.52059\n",
      "Epoch 159/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 68.1605 - val_loss: 116.3220\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 102.52059\n",
      "Epoch 160/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 61.1810 - val_loss: 110.2337\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 102.52059\n",
      "Epoch 161/1000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 62.4234 - val_loss: 108.0325\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 102.52059\n",
      "Epoch 162/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 61.8421 - val_loss: 114.0887\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 102.52059\n",
      "Epoch 163/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 60.3055 - val_loss: 114.5039\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 102.52059\n",
      "Epoch 164/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 63.9250 - val_loss: 115.5576\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 102.52059\n",
      "Epoch 165/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 58.3939 - val_loss: 105.4656\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 102.52059\n",
      "Epoch 166/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 57.0674 - val_loss: 100.8521\n",
      "\n",
      "Epoch 00166: val_loss improved from 102.52059 to 100.85207, saving model to weights.hdf5\n",
      "Epoch 167/1000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 58.3235 - val_loss: 108.6707\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 100.85207\n",
      "Epoch 168/1000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 56.9402 - val_loss: 105.9059\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 100.85207\n",
      "Epoch 169/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 55.3215 - val_loss: 104.8963\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 100.85207\n",
      "Epoch 170/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 55.5954 - val_loss: 108.8139\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 100.85207\n",
      "Epoch 171/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 54.4461 - val_loss: 106.9491\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 100.85207\n",
      "Epoch 172/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 54.3279 - val_loss: 105.1876\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 100.85207\n",
      "Epoch 173/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 52.6476 - val_loss: 104.3947\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 100.85207\n",
      "Epoch 174/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 52.2501 - val_loss: 103.8313\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 100.85207\n",
      "Epoch 175/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 52.9487 - val_loss: 107.7061\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 100.85207\n",
      "Epoch 176/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 53.3401 - val_loss: 106.7405\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 100.85207\n",
      "Epoch 177/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 53.0889 - val_loss: 105.4382\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 100.85207\n",
      "Epoch 178/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 51.1146 - val_loss: 110.3410\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 100.85207\n",
      "Epoch 179/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 51.1072 - val_loss: 105.0316\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 100.85207\n",
      "Epoch 180/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 50.1764 - val_loss: 103.0349\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 100.85207\n",
      "Epoch 181/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 49.1642 - val_loss: 104.5266\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 100.85207\n",
      "Epoch 182/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 49.7470 - val_loss: 102.7129\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 100.85207\n",
      "Epoch 183/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 48.5077 - val_loss: 104.3701\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 100.85207\n",
      "Epoch 184/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 48.3063 - val_loss: 111.0679\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 100.85207\n",
      "Epoch 185/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 48.7412 - val_loss: 105.3164\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 100.85207\n",
      "Epoch 186/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 49.6935 - val_loss: 109.1092\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 100.85207\n",
      "Epoch 187/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 49.9917 - val_loss: 111.4473\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 100.85207\n",
      "Epoch 188/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 557us/step - loss: 50.4267 - val_loss: 106.8816\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 100.85207\n",
      "Epoch 189/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 52.2417 - val_loss: 103.0314\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 100.85207\n",
      "Epoch 190/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 51.5312 - val_loss: 119.0880\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 100.85207\n",
      "Epoch 191/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 50.9878 - val_loss: 105.9368\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 100.85207\n",
      "Epoch 192/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 48.6882 - val_loss: 116.9570\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 100.85207\n",
      "Epoch 193/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 49.6852 - val_loss: 113.1176\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 100.85207\n",
      "Epoch 194/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 47.3570 - val_loss: 106.7877\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 100.85207\n",
      "Epoch 195/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 47.4375 - val_loss: 109.3406\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 100.85207\n",
      "Epoch 196/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 47.1783 - val_loss: 109.5971\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 100.85207\n",
      "Epoch 197/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 47.6879 - val_loss: 107.1572\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 100.85207\n",
      "Epoch 198/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 46.8972 - val_loss: 118.7372\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 100.85207\n",
      "Epoch 199/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 50.2407 - val_loss: 110.8346\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 100.85207\n",
      "Epoch 200/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 48.8451 - val_loss: 117.0567\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 100.85207\n",
      "Epoch 201/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 46.3713 - val_loss: 110.3919\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 100.85207\n",
      "Epoch 202/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 44.8645 - val_loss: 107.3696\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 100.85207\n",
      "Epoch 203/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 44.2688 - val_loss: 109.1079\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 100.85207\n",
      "Epoch 204/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 45.3964 - val_loss: 111.8378\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 100.85207\n",
      "Epoch 205/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 45.4108 - val_loss: 106.2136\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 100.85207\n",
      "Epoch 206/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 43.9845 - val_loss: 111.4216\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 100.85207\n",
      "Epoch 207/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 44.6271 - val_loss: 111.5950\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 100.85207\n",
      "Epoch 208/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 44.1106 - val_loss: 109.0579\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 100.85207\n",
      "Epoch 209/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 43.8889 - val_loss: 111.4618\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 100.85207\n",
      "Epoch 210/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 42.9050 - val_loss: 110.6893\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 100.85207\n",
      "Epoch 211/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 42.0975 - val_loss: 120.4519\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 100.85207\n",
      "Epoch 212/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 43.7336 - val_loss: 113.2734\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 100.85207\n",
      "Epoch 213/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 42.2511 - val_loss: 110.3832\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 100.85207\n",
      "Epoch 214/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 41.9184 - val_loss: 115.6000\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 100.85207\n",
      "Epoch 215/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 41.1611 - val_loss: 111.2605\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 100.85207\n",
      "Epoch 216/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 41.1744 - val_loss: 113.8220\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 100.85207\n",
      "Epoch 217/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 40.6150 - val_loss: 112.2142\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 100.85207\n",
      "Epoch 218/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 40.2089 - val_loss: 119.1976\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 100.85207\n",
      "Epoch 219/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 42.8518 - val_loss: 109.5916\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 100.85207\n",
      "Epoch 220/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 41.3328 - val_loss: 117.2226\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 100.85207\n",
      "Epoch 221/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 41.4820 - val_loss: 118.6066\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 100.85207\n",
      "Epoch 222/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 39.0571 - val_loss: 114.2918\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 100.85207\n",
      "Epoch 223/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 38.3889 - val_loss: 112.0900\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 100.85207\n",
      "Epoch 224/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 37.6124 - val_loss: 115.7693\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 100.85207\n",
      "Epoch 225/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 37.5017 - val_loss: 115.0358\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 100.85207\n",
      "Epoch 226/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 38.3245 - val_loss: 122.1650\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 100.85207\n",
      "Epoch 227/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 37.1154 - val_loss: 114.9717\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 100.85207\n",
      "Epoch 228/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 38.7524 - val_loss: 120.0283\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 100.85207\n",
      "Epoch 229/1000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 39.0349 - val_loss: 131.6919\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 100.85207\n",
      "Epoch 230/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 39.2825 - val_loss: 120.9037\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 100.85207\n",
      "Epoch 231/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 41.2747 - val_loss: 122.6256\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 100.85207\n",
      "Epoch 232/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 40.5597 - val_loss: 119.5894\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 100.85207\n",
      "Epoch 233/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 40.2327 - val_loss: 115.2631\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 100.85207\n",
      "Epoch 234/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 39.5746 - val_loss: 122.3150\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 100.85207\n",
      "Epoch 235/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 40.0085 - val_loss: 125.7500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 100.85207\n",
      "Epoch 236/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 39.2133 - val_loss: 114.9361\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 100.85207\n",
      "Epoch 237/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 40.6664 - val_loss: 122.3464\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 100.85207\n",
      "Epoch 238/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 39.2595 - val_loss: 125.8854\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 100.85207\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 546us/step - loss: 39.2315 - val_loss: 120.2011\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 100.85207\n",
      "Epoch 240/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 38.2549 - val_loss: 120.6418\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 100.85207\n",
      "Epoch 241/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 38.9156 - val_loss: 119.8536\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 100.85207\n",
      "Epoch 242/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 42.3648 - val_loss: 120.6827\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 100.85207\n",
      "Epoch 243/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 46.0190 - val_loss: 125.6464\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 100.85207\n",
      "Epoch 244/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 43.9371 - val_loss: 126.8853\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 100.85207\n",
      "Epoch 245/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 46.4770 - val_loss: 149.1110\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 100.85207\n",
      "Epoch 246/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 49.2438 - val_loss: 128.8851\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 100.85207\n",
      "Epoch 247/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 46.0670 - val_loss: 125.7281\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 100.85207\n",
      "Epoch 248/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 49.2168 - val_loss: 130.5916\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 100.85207\n",
      "Epoch 249/1000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 47.1059 - val_loss: 122.3626\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 100.85207\n",
      "Epoch 250/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 44.8497 - val_loss: 139.2235\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 100.85207\n",
      "Epoch 251/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 46.3439 - val_loss: 117.9669\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 100.85207\n",
      "Epoch 252/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 40.6399 - val_loss: 125.7059\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 100.85207\n",
      "Epoch 253/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 42.1920 - val_loss: 124.6574\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 100.85207\n",
      "Epoch 254/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 39.2539 - val_loss: 134.9879\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 100.85207\n",
      "Epoch 255/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 39.9326 - val_loss: 124.4647\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 100.85207\n",
      "Epoch 256/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 39.7872 - val_loss: 114.5662\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 100.85207\n",
      "Epoch 257/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 37.7936 - val_loss: 130.1152\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 100.85207\n",
      "Epoch 258/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 37.5396 - val_loss: 121.0780\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 100.85207\n",
      "Epoch 259/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 37.7190 - val_loss: 129.3177\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 100.85207\n",
      "Epoch 260/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 35.3609 - val_loss: 127.5178\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 100.85207\n",
      "Epoch 261/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 35.7142 - val_loss: 125.8001\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 100.85207\n",
      "Epoch 262/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 33.5874 - val_loss: 118.3953\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 100.85207\n",
      "Epoch 263/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 33.4233 - val_loss: 121.0096\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 100.85207\n",
      "Epoch 264/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 32.6687 - val_loss: 123.3670\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 100.85207\n",
      "Epoch 265/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 32.8741 - val_loss: 118.7264\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 100.85207\n",
      "Epoch 266/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 33.1558 - val_loss: 117.6949\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 100.85207\n",
      "Epoch 267/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 30.9293 - val_loss: 125.4215\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 100.85207\n",
      "Epoch 268/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 31.0355 - val_loss: 119.2560\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 100.85207\n",
      "Epoch 269/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 30.4434 - val_loss: 126.2020\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 100.85207\n",
      "Epoch 270/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 29.6130 - val_loss: 119.2193\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 100.85207\n",
      "Epoch 271/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 29.9460 - val_loss: 117.0559\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 100.85207\n",
      "Epoch 272/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 30.1566 - val_loss: 119.9441\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 100.85207\n",
      "Epoch 273/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 29.9566 - val_loss: 126.7402\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 100.85207\n",
      "Epoch 274/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 29.2493 - val_loss: 120.3692\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 100.85207\n",
      "Epoch 275/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 29.0945 - val_loss: 128.3048\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 100.85207\n",
      "Epoch 276/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 29.8570 - val_loss: 122.8718\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 100.85207\n",
      "Epoch 277/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 28.9738 - val_loss: 127.2405\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 100.85207\n",
      "Epoch 278/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 29.0091 - val_loss: 122.8344\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 100.85207\n",
      "Epoch 279/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 28.3481 - val_loss: 123.5719\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 100.85207\n",
      "Epoch 280/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 28.9562 - val_loss: 119.7532\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 100.85207\n",
      "Epoch 281/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 27.8497 - val_loss: 125.5363\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 100.85207\n",
      "Epoch 282/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 28.3884 - val_loss: 122.0681\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 100.85207\n",
      "Epoch 283/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 28.4232 - val_loss: 129.0253\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 100.85207\n",
      "Epoch 284/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 27.5432 - val_loss: 130.6025\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 100.85207\n",
      "Epoch 285/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 27.9895 - val_loss: 125.2043\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 100.85207\n",
      "Epoch 286/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 27.6738 - val_loss: 128.7891\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 100.85207\n",
      "Epoch 287/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 27.5062 - val_loss: 121.3172\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 100.85207\n",
      "Epoch 288/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 28.2951 - val_loss: 131.0812\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 100.85207\n",
      "Epoch 289/1000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 30.0763 - val_loss: 134.4402\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 100.85207\n",
      "Epoch 290/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 512us/step - loss: 31.8559 - val_loss: 147.1877\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 100.85207\n",
      "Epoch 291/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 34.2785 - val_loss: 131.6311\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 100.85207\n",
      "Epoch 292/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 31.7806 - val_loss: 136.2282\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 100.85207\n",
      "Epoch 293/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 31.8005 - val_loss: 134.0110\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 100.85207\n",
      "Epoch 294/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 30.4964 - val_loss: 126.4490\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 100.85207\n",
      "Epoch 295/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 29.0377 - val_loss: 143.0393\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 100.85207\n",
      "Epoch 296/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 29.0022 - val_loss: 132.3160\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 100.85207\n",
      "Epoch 297/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 28.5250 - val_loss: 144.3176\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 100.85207\n",
      "Epoch 298/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 29.5019 - val_loss: 139.5969\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 100.85207\n",
      "Epoch 299/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 32.3601 - val_loss: 143.6206\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 100.85207\n",
      "Epoch 300/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 31.2403 - val_loss: 137.7916\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 100.85207\n",
      "Epoch 301/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 31.3445 - val_loss: 145.1884\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 100.85207\n",
      "Epoch 302/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 31.5258 - val_loss: 145.7249\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 100.85207\n",
      "Epoch 303/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 29.2454 - val_loss: 136.4523\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 100.85207\n",
      "Epoch 304/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 28.8709 - val_loss: 140.0121\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 100.85207\n",
      "Epoch 305/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 27.9141 - val_loss: 142.1732\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 100.85207\n",
      "Epoch 306/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 28.7436 - val_loss: 137.0904\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 100.85207\n",
      "Epoch 307/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 27.1410 - val_loss: 136.9720\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 100.85207\n",
      "Epoch 308/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 25.5686 - val_loss: 139.1209\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 100.85207\n",
      "Epoch 309/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 26.0958 - val_loss: 139.3847\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 100.85207\n",
      "Epoch 310/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 24.8226 - val_loss: 129.0104\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 100.85207\n",
      "Epoch 311/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 25.0570 - val_loss: 140.8762\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 100.85207\n",
      "Epoch 312/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 24.8769 - val_loss: 134.6982\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 100.85207\n",
      "Epoch 313/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 25.1446 - val_loss: 134.7448\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 100.85207\n",
      "Epoch 314/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 25.9453 - val_loss: 137.2835\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 100.85207\n",
      "Epoch 315/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 26.5822 - val_loss: 136.3256\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 100.85207\n",
      "Epoch 316/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 27.0352 - val_loss: 141.6375\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 100.85207\n",
      "Epoch 317/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 28.9907 - val_loss: 140.9078\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 100.85207\n",
      "Epoch 318/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 29.4497 - val_loss: 150.9519\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 100.85207\n",
      "Epoch 319/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 29.5230 - val_loss: 139.5017\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 100.85207\n",
      "Epoch 320/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 30.1255 - val_loss: 143.9617\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 100.85207\n",
      "Epoch 321/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 28.0440 - val_loss: 143.4741\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 100.85207\n",
      "Epoch 322/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 28.0646 - val_loss: 141.5691\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 100.85207\n",
      "Epoch 323/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 29.9638 - val_loss: 157.1099\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 100.85207\n",
      "Epoch 324/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 29.9072 - val_loss: 145.9777\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 100.85207\n",
      "Epoch 325/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 27.5757 - val_loss: 149.1071\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 100.85207\n",
      "Epoch 326/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 29.6843 - val_loss: 141.5593\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 100.85207\n",
      "Epoch 327/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 27.3803 - val_loss: 144.2097\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 100.85207\n",
      "Epoch 328/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 25.1352 - val_loss: 147.8402\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 100.85207\n",
      "Epoch 329/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 25.5502 - val_loss: 143.2293\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 100.85207\n",
      "Epoch 330/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 25.7259 - val_loss: 146.8046\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 100.85207\n",
      "Epoch 331/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 24.8350 - val_loss: 141.2692\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 100.85207\n",
      "Epoch 332/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 23.5047 - val_loss: 143.5235\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 100.85207\n",
      "Epoch 333/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 22.7423 - val_loss: 147.9678\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 100.85207\n",
      "Epoch 334/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 23.5188 - val_loss: 145.3553\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 100.85207\n",
      "Epoch 335/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 23.5162 - val_loss: 151.6957\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 100.85207\n",
      "Epoch 336/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 22.7659 - val_loss: 142.9811\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 100.85207\n",
      "Epoch 337/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 22.5881 - val_loss: 145.6429\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 100.85207\n",
      "Epoch 338/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 22.5658 - val_loss: 147.1257\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 100.85207\n",
      "Epoch 339/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 21.5772 - val_loss: 150.8886\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 100.85207\n",
      "Epoch 340/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 22.2434 - val_loss: 149.0046\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 100.85207\n",
      "Epoch 341/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 552us/step - loss: 22.2363 - val_loss: 156.9110\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 100.85207\n",
      "Epoch 342/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 22.9880 - val_loss: 147.4546\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 100.85207\n",
      "Epoch 343/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 23.2228 - val_loss: 141.0070\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 100.85207\n",
      "Epoch 344/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 23.6043 - val_loss: 153.3223\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 100.85207\n",
      "Epoch 345/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 23.1350 - val_loss: 151.9262\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 100.85207\n",
      "Epoch 346/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 23.5089 - val_loss: 153.2947\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 100.85207\n",
      "Epoch 347/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 23.4730 - val_loss: 150.8416\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 100.85207\n",
      "Epoch 348/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 24.4534 - val_loss: 148.4187\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 100.85207\n",
      "Epoch 349/1000\n",
      "96/96 [==============================] - 0s 583us/step - loss: 25.2517 - val_loss: 162.4240\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 100.85207\n",
      "Epoch 350/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 26.2885 - val_loss: 158.8833\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 100.85207\n",
      "Epoch 351/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 27.1115 - val_loss: 172.1524\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 100.85207\n",
      "Epoch 352/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 28.8653 - val_loss: 153.6287\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 100.85207\n",
      "Epoch 353/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 26.6784 - val_loss: 161.8336\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 100.85207\n",
      "Epoch 354/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 25.6238 - val_loss: 157.8373\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 100.85207\n",
      "Epoch 355/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 23.5532 - val_loss: 155.5879\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 100.85207\n",
      "Epoch 356/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 23.6106 - val_loss: 166.3344\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 100.85207\n",
      "Epoch 357/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 22.8884 - val_loss: 154.8116\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 100.85207\n",
      "Epoch 358/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 22.5202 - val_loss: 155.5049\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 100.85207\n",
      "Epoch 359/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 25.9337 - val_loss: 156.3733\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 100.85207\n",
      "Epoch 360/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 26.5515 - val_loss: 159.5355\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 100.85207\n",
      "Epoch 361/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 24.5137 - val_loss: 152.8269\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 100.85207\n",
      "Epoch 362/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 23.2752 - val_loss: 158.4625\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 100.85207\n",
      "Epoch 363/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 22.8168 - val_loss: 158.7507\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 100.85207\n",
      "Epoch 364/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 22.3416 - val_loss: 158.7800\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 100.85207\n",
      "Epoch 365/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 22.1660 - val_loss: 144.9622\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 100.85207\n",
      "Epoch 366/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 22.4335 - val_loss: 135.9807\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 100.85207\n",
      "Epoch 367/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 22.9955 - val_loss: 159.6956\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 100.85207\n",
      "Epoch 368/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 23.1854 - val_loss: 146.6128\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 100.85207\n",
      "Epoch 369/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 21.6750 - val_loss: 161.4969\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 100.85207\n",
      "Epoch 370/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 21.3269 - val_loss: 146.9253\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 100.85207\n",
      "Epoch 371/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 22.1437 - val_loss: 142.7854\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 100.85207\n",
      "Epoch 372/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 20.6243 - val_loss: 152.3830\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 100.85207\n",
      "Epoch 373/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 20.8667 - val_loss: 150.1214\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 100.85207\n",
      "Epoch 374/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 20.2655 - val_loss: 144.9647\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 100.85207\n",
      "Epoch 375/1000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 19.5929 - val_loss: 148.5914\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 100.85207\n",
      "Epoch 376/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 18.9862 - val_loss: 151.0247\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 100.85207\n",
      "Epoch 377/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 20.0246 - val_loss: 155.2081\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 100.85207\n",
      "Epoch 378/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 20.3100 - val_loss: 155.5900\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 100.85207\n",
      "Epoch 379/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 21.0674 - val_loss: 156.5743\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 100.85207\n",
      "Epoch 380/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 21.1466 - val_loss: 149.7158\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 100.85207\n",
      "Epoch 381/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 22.6189 - val_loss: 148.0660\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 100.85207\n",
      "Epoch 382/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 22.6217 - val_loss: 144.3467\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 100.85207\n",
      "Epoch 383/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 23.6478 - val_loss: 150.4332\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 100.85207\n",
      "Epoch 384/1000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 23.2555 - val_loss: 146.0904\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 100.85207\n",
      "Epoch 385/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 25.0903 - val_loss: 167.1758\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 100.85207\n",
      "Epoch 386/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 25.0849 - val_loss: 151.6189\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 100.85207\n",
      "Epoch 387/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 25.5332 - val_loss: 165.2693\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 100.85207\n",
      "Epoch 388/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 27.6393 - val_loss: 164.7097\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 100.85207\n",
      "Epoch 389/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 29.4037 - val_loss: 168.5489\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 100.85207\n",
      "Epoch 390/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 36.2511 - val_loss: 180.1781\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 100.85207\n",
      "Epoch 391/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 28.5187 - val_loss: 173.5322\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 100.85207\n",
      "Epoch 392/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 551us/step - loss: 26.7281 - val_loss: 164.4157\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 100.85207\n",
      "Epoch 393/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 26.0588 - val_loss: 159.4489\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 100.85207\n",
      "Epoch 394/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 24.0588 - val_loss: 166.1678\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 100.85207\n",
      "Epoch 395/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 25.1021 - val_loss: 169.3397\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 100.85207\n",
      "Epoch 396/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 23.0654 - val_loss: 168.2706\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 100.85207\n",
      "Epoch 397/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 24.4780 - val_loss: 158.3709\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 100.85207\n",
      "Epoch 398/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 24.5196 - val_loss: 151.8069\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 100.85207\n",
      "Epoch 399/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 23.9677 - val_loss: 161.4221\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 100.85207\n",
      "Epoch 400/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 25.2944 - val_loss: 171.5317\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 100.85207\n",
      "Epoch 401/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 25.1905 - val_loss: 164.7132\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 100.85207\n",
      "Epoch 402/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 26.5786 - val_loss: 151.1447\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 100.85207\n",
      "Epoch 403/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 31.7109 - val_loss: 184.7507\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 100.85207\n",
      "Epoch 404/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 34.3071 - val_loss: 147.3801\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 100.85207\n",
      "Epoch 405/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 28.7892 - val_loss: 155.7094\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 100.85207\n",
      "Epoch 406/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 26.7312 - val_loss: 147.7568\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 100.85207\n",
      "Epoch 407/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 27.3069 - val_loss: 131.3452\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 100.85207\n",
      "Epoch 408/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 25.3419 - val_loss: 178.8702\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 100.85207\n",
      "Epoch 409/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 30.0579 - val_loss: 132.3771\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 100.85207\n",
      "Epoch 410/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 28.4733 - val_loss: 163.6265\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 100.85207\n",
      "Epoch 411/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 27.0034 - val_loss: 150.6136\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 100.85207\n",
      "Epoch 412/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 25.7564 - val_loss: 154.9893\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 100.85207\n",
      "Epoch 413/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 24.5070 - val_loss: 153.5429\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 100.85207\n",
      "Epoch 414/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 24.8544 - val_loss: 151.8766\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 100.85207\n",
      "Epoch 415/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 25.3210 - val_loss: 164.4946\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 100.85207\n",
      "Epoch 416/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 23.9765 - val_loss: 141.5657\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 100.85207\n",
      "Epoch 417/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 22.6301 - val_loss: 162.3139\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 100.85207\n",
      "Epoch 418/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 21.0202 - val_loss: 147.8051\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 100.85207\n",
      "Epoch 419/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 19.4261 - val_loss: 153.1043\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 100.85207\n",
      "Epoch 420/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 18.9700 - val_loss: 139.8737\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 100.85207\n",
      "Epoch 421/1000\n",
      "96/96 [==============================] - 0s 585us/step - loss: 19.1770 - val_loss: 164.8002\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 100.85207\n",
      "Epoch 422/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 20.6031 - val_loss: 154.7302\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 100.85207\n",
      "Epoch 423/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 19.3193 - val_loss: 148.8236\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 100.85207\n",
      "Epoch 424/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 18.9719 - val_loss: 163.5462\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 100.85207\n",
      "Epoch 425/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 21.2671 - val_loss: 160.9170\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 100.85207\n",
      "Epoch 426/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 21.4341 - val_loss: 178.8928\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 100.85207\n",
      "Epoch 427/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 21.8075 - val_loss: 155.7812\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 100.85207\n",
      "Epoch 428/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 21.3567 - val_loss: 162.5095\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 100.85207\n",
      "Epoch 429/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 20.0027 - val_loss: 165.0559\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 100.85207\n",
      "Epoch 430/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 20.7273 - val_loss: 173.5912\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 100.85207\n",
      "Epoch 431/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 19.7872 - val_loss: 151.4581\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 100.85207\n",
      "Epoch 432/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 19.2975 - val_loss: 161.8490\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 100.85207\n",
      "Epoch 433/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 18.8290 - val_loss: 149.7860\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 100.85207\n",
      "Epoch 434/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 17.8277 - val_loss: 166.6101\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 100.85207\n",
      "Epoch 435/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 19.1453 - val_loss: 167.3608\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 100.85207\n",
      "Epoch 436/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 19.4398 - val_loss: 164.8656\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 100.85207\n",
      "Epoch 437/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 19.8947 - val_loss: 168.7132\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 100.85207\n",
      "Epoch 438/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 20.0570 - val_loss: 159.7796\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 100.85207\n",
      "Epoch 439/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 18.6068 - val_loss: 178.3425\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 100.85207\n",
      "Epoch 440/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 20.3335 - val_loss: 183.0523\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 100.85207\n",
      "Epoch 441/1000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 21.1192 - val_loss: 169.8494\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 100.85207\n",
      "Epoch 442/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 19.3431 - val_loss: 174.9381\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 100.85207\n",
      "Epoch 443/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 578us/step - loss: 20.6911 - val_loss: 163.0899\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 100.85207\n",
      "Epoch 444/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 20.1633 - val_loss: 179.8065\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 100.85207\n",
      "Epoch 445/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 19.3708 - val_loss: 157.8161\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 100.85207\n",
      "Epoch 446/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 19.3200 - val_loss: 167.3072\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 100.85207\n",
      "Epoch 447/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 17.8571 - val_loss: 166.1560\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 100.85207\n",
      "Epoch 448/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 16.9861 - val_loss: 170.0694\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 100.85207\n",
      "Epoch 449/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 16.6942 - val_loss: 160.1835\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 100.85207\n",
      "Epoch 450/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 16.9186 - val_loss: 170.8417\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 100.85207\n",
      "Epoch 451/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 16.2264 - val_loss: 169.0791\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 100.85207\n",
      "Epoch 452/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 17.9084 - val_loss: 171.5687\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 100.85207\n",
      "Epoch 453/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 17.2801 - val_loss: 163.3783\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 100.85207\n",
      "Epoch 454/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 16.6390 - val_loss: 181.9302\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 100.85207\n",
      "Epoch 455/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 18.2833 - val_loss: 164.1696\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 100.85207\n",
      "Epoch 456/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 17.3076 - val_loss: 177.4863\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 100.85207\n",
      "Epoch 457/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 17.3736 - val_loss: 165.4707\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 100.85207\n",
      "Epoch 458/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 16.7700 - val_loss: 170.2580\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 100.85207\n",
      "Epoch 459/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 17.0844 - val_loss: 171.4664\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 100.85207\n",
      "Epoch 460/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 17.2363 - val_loss: 185.2623\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 100.85207\n",
      "Epoch 461/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 17.2202 - val_loss: 167.5841\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 100.85207\n",
      "Epoch 462/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 18.6739 - val_loss: 189.2427\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 100.85207\n",
      "Epoch 463/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 19.2841 - val_loss: 169.4764\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 100.85207\n",
      "Epoch 464/1000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 18.7574 - val_loss: 179.8242\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 100.85207\n",
      "Epoch 465/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 20.6957 - val_loss: 172.4491\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 100.85207\n",
      "Epoch 466/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 23.2036 - val_loss: 188.3991\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 100.85207\n",
      "Epoch 467/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 22.9407 - val_loss: 177.4437\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 100.85207\n",
      "Epoch 468/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 22.6216 - val_loss: 188.1817\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 100.85207\n",
      "Epoch 469/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 24.1192 - val_loss: 195.1903\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 100.85207\n",
      "Epoch 470/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 19.3526 - val_loss: 178.1358\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 100.85207\n",
      "Epoch 471/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 20.0577 - val_loss: 162.8903\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 100.85207\n",
      "Epoch 472/1000\n",
      "96/96 [==============================] - 0s 612us/step - loss: 20.5558 - val_loss: 180.1148\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 100.85207\n",
      "Epoch 473/1000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 20.3332 - val_loss: 172.7006\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 100.85207\n",
      "Epoch 474/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 18.9986 - val_loss: 185.5492\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 100.85207\n",
      "Epoch 475/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 18.0556 - val_loss: 169.7082\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 100.85207\n",
      "Epoch 476/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 16.4782 - val_loss: 177.6548\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 100.85207\n",
      "Epoch 477/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 16.2984 - val_loss: 178.5567\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 100.85207\n",
      "Epoch 478/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 16.6334 - val_loss: 163.4783\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 100.85207\n",
      "Epoch 479/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 17.0595 - val_loss: 173.7818\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 100.85207\n",
      "Epoch 480/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 17.7606 - val_loss: 174.3698\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 100.85207\n",
      "Epoch 481/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 18.7935 - val_loss: 162.9989\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 100.85207\n",
      "Epoch 482/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 17.3150 - val_loss: 176.8355\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 100.85207\n",
      "Epoch 483/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 18.5154 - val_loss: 160.3100\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 100.85207\n",
      "Epoch 484/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 17.4816 - val_loss: 178.9357\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 100.85207\n",
      "Epoch 485/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 18.4114 - val_loss: 174.2002\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 100.85207\n",
      "Epoch 486/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 22.8629 - val_loss: 174.9963\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 100.85207\n",
      "Epoch 487/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 25.3827 - val_loss: 163.4725\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 100.85207\n",
      "Epoch 488/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 24.3931 - val_loss: 173.0768\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 100.85207\n",
      "Epoch 489/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 26.1877 - val_loss: 178.2365\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 100.85207\n",
      "Epoch 490/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 23.5606 - val_loss: 186.1369\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 100.85207\n",
      "Epoch 491/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 22.5048 - val_loss: 200.1684\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 100.85207\n",
      "Epoch 492/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 20.4141 - val_loss: 166.2560\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 100.85207\n",
      "Epoch 493/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 20.8072 - val_loss: 204.1173\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 100.85207\n",
      "Epoch 494/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 563us/step - loss: 22.2118 - val_loss: 186.3014\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 100.85207\n",
      "Epoch 495/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 26.3829 - val_loss: 191.8616\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 100.85207\n",
      "Epoch 496/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 24.5869 - val_loss: 179.9744\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 100.85207\n",
      "Epoch 497/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 23.9098 - val_loss: 191.4966\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 100.85207\n",
      "Epoch 498/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 28.1518 - val_loss: 190.7132\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 100.85207\n",
      "Epoch 499/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 35.8190 - val_loss: 157.9469\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 100.85207\n",
      "Epoch 500/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 46.9900 - val_loss: 216.4887\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 100.85207\n",
      "Epoch 501/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 59.9343 - val_loss: 153.4759\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 100.85207\n",
      "Epoch 502/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 48.7382 - val_loss: 157.9402\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 100.85207\n",
      "Epoch 503/1000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 43.5131 - val_loss: 149.9075\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 100.85207\n",
      "Epoch 504/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 32.7131 - val_loss: 152.5095\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 100.85207\n",
      "Epoch 505/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 29.0330 - val_loss: 134.4773\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 100.85207\n",
      "Epoch 506/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 25.4922 - val_loss: 133.6528\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 100.85207\n",
      "Epoch 507/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 21.1928 - val_loss: 135.0590\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 100.85207\n",
      "Epoch 508/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 20.1917 - val_loss: 147.4752\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 100.85207\n",
      "Epoch 509/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 20.1911 - val_loss: 141.3573\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 100.85207\n",
      "Epoch 510/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 18.3531 - val_loss: 140.8266\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 100.85207\n",
      "Epoch 511/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 17.1404 - val_loss: 140.5961\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 100.85207\n",
      "Epoch 512/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 15.8752 - val_loss: 138.8436\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 100.85207\n",
      "Epoch 513/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 15.6501 - val_loss: 140.8280\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 100.85207\n",
      "Epoch 514/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 15.5894 - val_loss: 142.7156\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 100.85207\n",
      "Epoch 515/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.8263 - val_loss: 132.5499\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 100.85207\n",
      "Epoch 516/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 14.9363 - val_loss: 140.1932\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 100.85207\n",
      "Epoch 517/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 14.6093 - val_loss: 147.5857\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 100.85207\n",
      "Epoch 518/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 14.4284 - val_loss: 139.7450\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 100.85207\n",
      "Epoch 519/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 14.6507 - val_loss: 145.7791\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 100.85207\n",
      "Epoch 520/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 14.5657 - val_loss: 157.2752\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 100.85207\n",
      "Epoch 521/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 15.8020 - val_loss: 154.5748\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 100.85207\n",
      "Epoch 522/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 16.6996 - val_loss: 162.5030\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 100.85207\n",
      "Epoch 523/1000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 17.7455 - val_loss: 139.7894\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 100.85207\n",
      "Epoch 524/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 17.6261 - val_loss: 172.3233\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 100.85207\n",
      "Epoch 525/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 15.5519 - val_loss: 152.6074\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 100.85207\n",
      "Epoch 526/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 16.4593 - val_loss: 151.8957\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 100.85207\n",
      "Epoch 527/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 16.1576 - val_loss: 150.7294\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 100.85207\n",
      "Epoch 528/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 16.9843 - val_loss: 158.1512\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 100.85207\n",
      "Epoch 529/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 16.4987 - val_loss: 160.3647\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 100.85207\n",
      "Epoch 530/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 18.4951 - val_loss: 163.8695\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 100.85207\n",
      "Epoch 531/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 19.2380 - val_loss: 158.7938\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 100.85207\n",
      "Epoch 532/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 18.4863 - val_loss: 171.6471\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 100.85207\n",
      "Epoch 533/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 17.1943 - val_loss: 143.2877\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 100.85207\n",
      "Epoch 534/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 17.2517 - val_loss: 149.3920\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 100.85207\n",
      "Epoch 535/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 15.5895 - val_loss: 157.5713\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 100.85207\n",
      "Epoch 536/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 15.6286 - val_loss: 150.4031\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 100.85207\n",
      "Epoch 537/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 14.2599 - val_loss: 152.3917\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 100.85207\n",
      "Epoch 538/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 14.8025 - val_loss: 155.5693\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 100.85207\n",
      "Epoch 539/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 15.0600 - val_loss: 170.9937\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 100.85207\n",
      "Epoch 540/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 14.7453 - val_loss: 158.9338\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 100.85207\n",
      "Epoch 541/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 14.7313 - val_loss: 162.5401\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 100.85207\n",
      "Epoch 542/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 14.6328 - val_loss: 152.7285\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 100.85207\n",
      "Epoch 543/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 14.3548 - val_loss: 157.2976\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 100.85207\n",
      "Epoch 544/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 14.1202 - val_loss: 157.2329\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 100.85207\n",
      "Epoch 545/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 561us/step - loss: 14.1526 - val_loss: 160.1496\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 100.85207\n",
      "Epoch 546/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.4506 - val_loss: 171.3413\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 100.85207\n",
      "Epoch 547/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 14.5678 - val_loss: 167.2453\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 100.85207\n",
      "Epoch 548/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 14.3795 - val_loss: 174.7448\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 100.85207\n",
      "Epoch 549/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 14.0949 - val_loss: 160.7121\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 100.85207\n",
      "Epoch 550/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 14.5742 - val_loss: 179.4838\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 100.85207\n",
      "Epoch 551/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.6438 - val_loss: 160.9422\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 100.85207\n",
      "Epoch 552/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.2378 - val_loss: 160.7079\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 100.85207\n",
      "Epoch 553/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 13.1672 - val_loss: 164.3477\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 100.85207\n",
      "Epoch 554/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 13.5695 - val_loss: 172.5450\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 100.85207\n",
      "Epoch 555/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.1750 - val_loss: 165.0267\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 100.85207\n",
      "Epoch 556/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 13.8432 - val_loss: 172.3139\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 100.85207\n",
      "Epoch 557/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 13.7516 - val_loss: 178.6671\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 100.85207\n",
      "Epoch 558/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 13.9975 - val_loss: 176.8182\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 100.85207\n",
      "Epoch 559/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 14.2664 - val_loss: 183.2147\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 100.85207\n",
      "Epoch 560/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 14.8844 - val_loss: 167.7185\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 100.85207\n",
      "Epoch 561/1000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 14.3741 - val_loss: 180.1713\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 100.85207\n",
      "Epoch 562/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 14.0049 - val_loss: 195.4338\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 100.85207\n",
      "Epoch 563/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 14.9418 - val_loss: 174.2759\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 100.85207\n",
      "Epoch 564/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 13.9555 - val_loss: 159.9172\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 100.85207\n",
      "Epoch 565/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.9772 - val_loss: 177.4605\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 100.85207\n",
      "Epoch 566/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 13.3953 - val_loss: 164.5075\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 100.85207\n",
      "Epoch 567/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.2182 - val_loss: 183.6825\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 100.85207\n",
      "Epoch 568/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 13.7997 - val_loss: 164.9828\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 100.85207\n",
      "Epoch 569/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 13.3698 - val_loss: 178.0697\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 100.85207\n",
      "Epoch 570/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 13.7757 - val_loss: 161.7827\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 100.85207\n",
      "Epoch 571/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 14.3478 - val_loss: 175.3650\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 100.85207\n",
      "Epoch 572/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 13.5651 - val_loss: 165.4866\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 100.85207\n",
      "Epoch 573/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 14.5579 - val_loss: 167.3499\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 100.85207\n",
      "Epoch 574/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 14.9370 - val_loss: 169.0345\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 100.85207\n",
      "Epoch 575/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 15.4456 - val_loss: 192.8924\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 100.85207\n",
      "Epoch 576/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 15.8950 - val_loss: 184.9286\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 100.85207\n",
      "Epoch 577/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 17.9848 - val_loss: 195.5216\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 100.85207\n",
      "Epoch 578/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 16.8724 - val_loss: 202.5086\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 100.85207\n",
      "Epoch 579/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 19.7312 - val_loss: 195.8354\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 100.85207\n",
      "Epoch 580/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 22.0697 - val_loss: 177.3135\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 100.85207\n",
      "Epoch 581/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 19.2083 - val_loss: 197.1328\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 100.85207\n",
      "Epoch 582/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 17.8259 - val_loss: 172.5622\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 100.85207\n",
      "Epoch 583/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 17.8148 - val_loss: 185.6886\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 100.85207\n",
      "Epoch 584/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 16.8648 - val_loss: 176.5332\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 100.85207\n",
      "Epoch 585/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 16.5789 - val_loss: 201.3207\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 100.85207\n",
      "Epoch 586/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 15.7288 - val_loss: 167.1972\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 100.85207\n",
      "Epoch 587/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.9349 - val_loss: 193.8667\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 100.85207\n",
      "Epoch 588/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 14.9593 - val_loss: 187.0418\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 100.85207\n",
      "Epoch 589/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.7998 - val_loss: 192.5651\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 100.85207\n",
      "Epoch 590/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 15.6782 - val_loss: 179.9525\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 100.85207\n",
      "Epoch 591/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 17.3635 - val_loss: 202.7806\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 100.85207\n",
      "Epoch 592/1000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 18.0459 - val_loss: 180.1072\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 100.85207\n",
      "Epoch 593/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 19.2815 - val_loss: 203.2189\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 100.85207\n",
      "Epoch 594/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 19.8397 - val_loss: 175.8997\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 100.85207\n",
      "Epoch 595/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 23.4022 - val_loss: 219.3024\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 100.85207\n",
      "Epoch 596/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 557us/step - loss: 26.6051 - val_loss: 201.0770\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 100.85207\n",
      "Epoch 597/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 28.9296 - val_loss: 263.6450\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 100.85207\n",
      "Epoch 598/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 25.0726 - val_loss: 189.0856\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 100.85207\n",
      "Epoch 599/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 26.1286 - val_loss: 215.0451\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 100.85207\n",
      "Epoch 600/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 24.9832 - val_loss: 187.3569\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 100.85207\n",
      "Epoch 601/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 24.8243 - val_loss: 218.7623\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 100.85207\n",
      "Epoch 602/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 23.2609 - val_loss: 181.2950\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 100.85207\n",
      "Epoch 603/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 21.3465 - val_loss: 217.8094\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 100.85207\n",
      "Epoch 604/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 22.6464 - val_loss: 202.6090\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 100.85207\n",
      "Epoch 605/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 19.0425 - val_loss: 189.6001\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 100.85207\n",
      "Epoch 606/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 17.6367 - val_loss: 191.0995\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 100.85207\n",
      "Epoch 607/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 16.6942 - val_loss: 203.1374\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 100.85207\n",
      "Epoch 608/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 15.6189 - val_loss: 186.9964\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 100.85207\n",
      "Epoch 609/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 14.9262 - val_loss: 195.0947\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 100.85207\n",
      "Epoch 610/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 15.1701 - val_loss: 192.5464\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 100.85207\n",
      "Epoch 611/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 15.5901 - val_loss: 194.3223\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 100.85207\n",
      "Epoch 612/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 16.4356 - val_loss: 183.8749\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 100.85207\n",
      "Epoch 613/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 16.2565 - val_loss: 184.1866\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 100.85207\n",
      "Epoch 614/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 15.6294 - val_loss: 191.6075\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 100.85207\n",
      "Epoch 615/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 15.6436 - val_loss: 176.2917\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 100.85207\n",
      "Epoch 616/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 14.7588 - val_loss: 186.5757\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 100.85207\n",
      "Epoch 617/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 14.4433 - val_loss: 188.5016\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 100.85207\n",
      "Epoch 618/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 14.9102 - val_loss: 172.5923\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 100.85207\n",
      "Epoch 619/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 13.1508 - val_loss: 186.6808\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 100.85207\n",
      "Epoch 620/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 13.3019 - val_loss: 186.4688\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 100.85207\n",
      "Epoch 621/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.5051 - val_loss: 169.4318\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 100.85207\n",
      "Epoch 622/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 12.8402 - val_loss: 183.9160\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 100.85207\n",
      "Epoch 623/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 13.2009 - val_loss: 178.4249\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 100.85207\n",
      "Epoch 624/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.1873 - val_loss: 184.4029\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 100.85207\n",
      "Epoch 625/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 13.4694 - val_loss: 188.6622\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 100.85207\n",
      "Epoch 626/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 13.6492 - val_loss: 175.3965\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 100.85207\n",
      "Epoch 627/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 13.2185 - val_loss: 177.8587\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 100.85207\n",
      "Epoch 628/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 13.1593 - val_loss: 171.5511\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 100.85207\n",
      "Epoch 629/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 13.4132 - val_loss: 181.0389\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 100.85207\n",
      "Epoch 630/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 12.3915 - val_loss: 186.6988\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 100.85207\n",
      "Epoch 631/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 14.1075 - val_loss: 191.4408\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 100.85207\n",
      "Epoch 632/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 15.6248 - val_loss: 185.2500\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 100.85207\n",
      "Epoch 633/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 15.9531 - val_loss: 182.7071\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 100.85207\n",
      "Epoch 634/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 14.4119 - val_loss: 183.0174\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 100.85207\n",
      "Epoch 635/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 14.4612 - val_loss: 189.7900\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 100.85207\n",
      "Epoch 636/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 14.1310 - val_loss: 189.7001\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 100.85207\n",
      "Epoch 637/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 14.2778 - val_loss: 200.1673\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 100.85207\n",
      "Epoch 638/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 12.9108 - val_loss: 184.2085\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 100.85207\n",
      "Epoch 639/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 13.1481 - val_loss: 190.7190\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 100.85207\n",
      "Epoch 640/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 12.8211 - val_loss: 186.3665\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 100.85207\n",
      "Epoch 641/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.0135 - val_loss: 177.0317\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 100.85207\n",
      "Epoch 642/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.0862 - val_loss: 182.6459\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 100.85207\n",
      "Epoch 643/1000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 11.6739 - val_loss: 195.3187\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 100.85207\n",
      "Epoch 644/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 12.1704 - val_loss: 185.9807\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 100.85207\n",
      "Epoch 645/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 12.1739 - val_loss: 185.0430\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 100.85207\n",
      "Epoch 646/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.3820 - val_loss: 181.6436\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 100.85207\n",
      "Epoch 647/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 551us/step - loss: 12.5128 - val_loss: 181.4148\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 100.85207\n",
      "Epoch 648/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 11.7025 - val_loss: 192.5351\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 100.85207\n",
      "Epoch 649/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.9221 - val_loss: 180.3897\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 100.85207\n",
      "Epoch 650/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 12.1310 - val_loss: 191.3226\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 100.85207\n",
      "Epoch 651/1000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 11.4813 - val_loss: 186.6763\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 100.85207\n",
      "Epoch 652/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 11.6727 - val_loss: 193.1298\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 100.85207\n",
      "Epoch 653/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.3415 - val_loss: 183.4072\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 100.85207\n",
      "Epoch 654/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 12.7056 - val_loss: 182.4354\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 100.85207\n",
      "Epoch 655/1000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 12.9323 - val_loss: 196.5311\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 100.85207\n",
      "Epoch 656/1000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 12.5258 - val_loss: 191.3123\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 100.85207\n",
      "Epoch 657/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.3784 - val_loss: 179.8483\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 100.85207\n",
      "Epoch 658/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.8377 - val_loss: 200.8816\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 100.85207\n",
      "Epoch 659/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 12.7292 - val_loss: 180.5383\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 100.85207\n",
      "Epoch 660/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 12.4551 - val_loss: 184.9997\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 100.85207\n",
      "Epoch 661/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 14.2387 - val_loss: 176.2583\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 100.85207\n",
      "Epoch 662/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 13.2818 - val_loss: 190.7540\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 100.85207\n",
      "Epoch 663/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 12.9910 - val_loss: 188.4166\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 100.85207\n",
      "Epoch 664/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 13.2320 - val_loss: 204.1717\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 100.85207\n",
      "Epoch 665/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 13.5895 - val_loss: 191.8150\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 100.85207\n",
      "Epoch 666/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.7138 - val_loss: 185.6584\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 100.85207\n",
      "Epoch 667/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 12.4165 - val_loss: 192.9459\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 100.85207\n",
      "Epoch 668/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 12.3612 - val_loss: 181.8555\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 100.85207\n",
      "Epoch 669/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 12.8547 - val_loss: 191.1409\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 100.85207\n",
      "Epoch 670/1000\n",
      "96/96 [==============================] - 0s 580us/step - loss: 12.6299 - val_loss: 196.9110\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 100.85207\n",
      "Epoch 671/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 13.0755 - val_loss: 199.7545\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 100.85207\n",
      "Epoch 672/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 13.0206 - val_loss: 200.5365\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 100.85207\n",
      "Epoch 673/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 13.9556 - val_loss: 183.8024\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 100.85207\n",
      "Epoch 674/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 12.8158 - val_loss: 182.6390\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 100.85207\n",
      "Epoch 675/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 12.4065 - val_loss: 196.4964\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 100.85207\n",
      "Epoch 676/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.7648 - val_loss: 183.8837\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 100.85207\n",
      "Epoch 677/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 11.8527 - val_loss: 193.0436\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 100.85207\n",
      "Epoch 678/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.4809 - val_loss: 192.3463\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 100.85207\n",
      "Epoch 679/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.6221 - val_loss: 193.3479\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 100.85207\n",
      "Epoch 680/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 11.1166 - val_loss: 193.9543\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 100.85207\n",
      "Epoch 681/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 11.1198 - val_loss: 193.4941\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 100.85207\n",
      "Epoch 682/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 11.6495 - val_loss: 203.4833\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 100.85207\n",
      "Epoch 683/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.0550 - val_loss: 192.3739\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 100.85207\n",
      "Epoch 684/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 12.0987 - val_loss: 199.8900\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 100.85207\n",
      "Epoch 685/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 11.8917 - val_loss: 220.6387\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 100.85207\n",
      "Epoch 686/1000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 12.8756 - val_loss: 181.8121\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 100.85207\n",
      "Epoch 687/1000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 12.0919 - val_loss: 193.4523\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 100.85207\n",
      "Epoch 688/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 12.5617 - val_loss: 181.6486\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 100.85207\n",
      "Epoch 689/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 13.6838 - val_loss: 192.5238\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 100.85207\n",
      "Epoch 690/1000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 13.3769 - val_loss: 179.0779\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 100.85207\n",
      "Epoch 691/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 12.7975 - val_loss: 200.6305\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 100.85207\n",
      "Epoch 692/1000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 14.8179 - val_loss: 183.5213\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 100.85207\n",
      "Epoch 693/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 15.1797 - val_loss: 202.1546\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 100.85207\n",
      "Epoch 694/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 14.4781 - val_loss: 199.2801\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 100.85207\n",
      "Epoch 695/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 15.4320 - val_loss: 201.8611\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 100.85207\n",
      "Epoch 696/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 17.0895 - val_loss: 218.4626\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 100.85207\n",
      "Epoch 697/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 20.9163 - val_loss: 202.0224\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 100.85207\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 558us/step - loss: 29.6951 - val_loss: 228.4822\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 100.85207\n",
      "Epoch 699/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 26.1894 - val_loss: 213.4281\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 100.85207\n",
      "Epoch 700/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 28.5478 - val_loss: 208.3055\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 100.85207\n",
      "Epoch 701/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 21.2487 - val_loss: 192.0452\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 100.85207\n",
      "Epoch 702/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 25.8371 - val_loss: 233.9412\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 100.85207\n",
      "Epoch 703/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 27.8793 - val_loss: 264.4265\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 100.85207\n",
      "Epoch 704/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 32.0632 - val_loss: 238.7392\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 100.85207\n",
      "Epoch 705/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 35.5593 - val_loss: 247.7108\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 100.85207\n",
      "Epoch 706/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 41.4309 - val_loss: 212.3227\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 100.85207\n",
      "Epoch 707/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 28.8956 - val_loss: 208.7762\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 100.85207\n",
      "Epoch 708/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 25.3438 - val_loss: 249.6857\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 100.85207\n",
      "Epoch 709/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 32.0453 - val_loss: 216.0113\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 100.85207\n",
      "Epoch 710/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 35.3054 - val_loss: 238.3920\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 100.85207\n",
      "Epoch 711/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 38.0949 - val_loss: 227.1753\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 100.85207\n",
      "Epoch 712/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 32.0977 - val_loss: 241.2174\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 100.85207\n",
      "Epoch 713/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 29.7233 - val_loss: 208.6418\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 100.85207\n",
      "Epoch 714/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 23.9265 - val_loss: 200.6854\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 100.85207\n",
      "Epoch 715/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 20.9150 - val_loss: 222.3050\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 100.85207\n",
      "Epoch 716/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 19.3824 - val_loss: 202.3464\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 100.85207\n",
      "Epoch 717/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 22.3558 - val_loss: 204.1314\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 100.85207\n",
      "Epoch 718/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 25.4002 - val_loss: 245.3538\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 100.85207\n",
      "Epoch 719/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 23.6473 - val_loss: 176.8313\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 100.85207\n",
      "Epoch 720/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 25.9998 - val_loss: 229.3816\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 100.85207\n",
      "Epoch 721/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 24.0791 - val_loss: 204.2758\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 100.85207\n",
      "Epoch 722/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 30.0668 - val_loss: 212.9314\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 100.85207\n",
      "Epoch 723/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 28.8391 - val_loss: 202.1700\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 100.85207\n",
      "Epoch 724/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 34.8384 - val_loss: 216.7722\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 100.85207\n",
      "Epoch 725/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 39.0762 - val_loss: 172.7685\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 100.85207\n",
      "Epoch 726/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 28.1433 - val_loss: 185.1679\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 100.85207\n",
      "Epoch 727/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 24.3141 - val_loss: 213.6940\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 100.85207\n",
      "Epoch 728/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 19.5813 - val_loss: 196.6190\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 100.85207\n",
      "Epoch 729/1000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 17.8483 - val_loss: 206.2257\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 100.85207\n",
      "Epoch 730/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 15.8657 - val_loss: 206.3641\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 100.85207\n",
      "Epoch 731/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 14.9934 - val_loss: 200.8431\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 100.85207\n",
      "Epoch 732/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 14.4651 - val_loss: 200.2886\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 100.85207\n",
      "Epoch 733/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 13.4095 - val_loss: 208.0100\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 100.85207\n",
      "Epoch 734/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 13.4471 - val_loss: 204.9350\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 100.85207\n",
      "Epoch 735/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 12.4539 - val_loss: 198.7738\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 100.85207\n",
      "Epoch 736/1000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 12.0414 - val_loss: 205.9633\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 100.85207\n",
      "Epoch 737/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.7032 - val_loss: 201.4321\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 100.85207\n",
      "Epoch 738/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.7742 - val_loss: 201.9412\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 100.85207\n",
      "Epoch 739/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 11.4380 - val_loss: 204.9338\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 100.85207\n",
      "Epoch 740/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.0208 - val_loss: 191.9440\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 100.85207\n",
      "Epoch 741/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 11.3805 - val_loss: 201.3823\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 100.85207\n",
      "Epoch 742/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 11.7219 - val_loss: 204.6099\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 100.85207\n",
      "Epoch 743/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.5541 - val_loss: 188.1449\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 100.85207\n",
      "Epoch 744/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 11.5037 - val_loss: 194.7468\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 100.85207\n",
      "Epoch 745/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 11.7064 - val_loss: 201.2549\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 100.85207\n",
      "Epoch 746/1000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 11.8038 - val_loss: 190.9848\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 100.85207\n",
      "Epoch 747/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.2420 - val_loss: 208.3498\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 100.85207\n",
      "Epoch 748/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 11.2725 - val_loss: 182.3579\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 100.85207\n",
      "Epoch 749/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 556us/step - loss: 11.6421 - val_loss: 193.8866\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 100.85207\n",
      "Epoch 750/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.5084 - val_loss: 204.8146\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 100.85207\n",
      "Epoch 751/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 11.3003 - val_loss: 201.8483\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 100.85207\n",
      "Epoch 752/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 11.0324 - val_loss: 196.9041\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 100.85207\n",
      "Epoch 753/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 11.0048 - val_loss: 199.7705\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 100.85207\n",
      "Epoch 754/1000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 10.8321 - val_loss: 193.0079\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 100.85207\n",
      "Epoch 755/1000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 10.6816 - val_loss: 196.1287\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 100.85207\n",
      "Epoch 756/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 10.3516 - val_loss: 207.7677\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 100.85207\n",
      "Epoch 757/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 10.4725 - val_loss: 185.7697\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 100.85207\n",
      "Epoch 758/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 10.8610 - val_loss: 216.0796\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 100.85207\n",
      "Epoch 759/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 10.7149 - val_loss: 195.1004\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 100.85207\n",
      "Epoch 760/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 10.8677 - val_loss: 192.0539\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 100.85207\n",
      "Epoch 761/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 10.9775 - val_loss: 204.8883\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 100.85207\n",
      "Epoch 762/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.7386 - val_loss: 201.3827\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 100.85207\n",
      "Epoch 763/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 10.5071 - val_loss: 184.5187\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 100.85207\n",
      "Epoch 764/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 10.8000 - val_loss: 204.8011\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 100.85207\n",
      "Epoch 765/1000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 11.4634 - val_loss: 200.2964\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 100.85207\n",
      "Epoch 766/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 10.9722 - val_loss: 200.3674\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 100.85207\n",
      "Epoch 767/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 10.8407 - val_loss: 210.3439\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 100.85207\n",
      "Epoch 768/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 10.7433 - val_loss: 193.5110\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 100.85207\n",
      "Epoch 769/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 10.6040 - val_loss: 210.5630\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 100.85207\n",
      "Epoch 770/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 10.7337 - val_loss: 204.4145\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 100.85207\n",
      "Epoch 771/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 10.4081 - val_loss: 192.1285\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 100.85207\n",
      "Epoch 772/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 11.2420 - val_loss: 214.1920\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 100.85207\n",
      "Epoch 773/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.4965 - val_loss: 203.7739\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 100.85207\n",
      "Epoch 774/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 11.1847 - val_loss: 200.7625\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 100.85207\n",
      "Epoch 775/1000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 10.8846 - val_loss: 196.9844\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 100.85207\n",
      "Epoch 776/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 10.9065 - val_loss: 215.0299\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 100.85207\n",
      "Epoch 777/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 11.1867 - val_loss: 206.8661\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 100.85207\n",
      "Epoch 778/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 11.0480 - val_loss: 212.0760\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 100.85207\n",
      "Epoch 779/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 11.3200 - val_loss: 202.6347\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 100.85207\n",
      "Epoch 780/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 11.2062 - val_loss: 200.0681\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 100.85207\n",
      "Epoch 781/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 11.6899 - val_loss: 231.7154\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 100.85207\n",
      "Epoch 782/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 11.8142 - val_loss: 211.4908\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 100.85207\n",
      "Epoch 783/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.3008 - val_loss: 201.7489\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 100.85207\n",
      "Epoch 784/1000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 11.3210 - val_loss: 197.2606\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 100.85207\n",
      "Epoch 785/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 10.7018 - val_loss: 203.1113\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 100.85207\n",
      "Epoch 786/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.4031 - val_loss: 213.7456\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 100.85207\n",
      "Epoch 787/1000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 10.6552 - val_loss: 191.5976\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 100.85207\n",
      "Epoch 788/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 10.6096 - val_loss: 192.0750\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 100.85207\n",
      "Epoch 789/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.7227 - val_loss: 213.0124\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 100.85207\n",
      "Epoch 790/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 10.9409 - val_loss: 196.3895\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 100.85207\n",
      "Epoch 791/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 10.2710 - val_loss: 205.4929\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 100.85207\n",
      "Epoch 792/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 9.9009 - val_loss: 205.8804\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 100.85207\n",
      "Epoch 793/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.2250 - val_loss: 220.9452\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 100.85207\n",
      "Epoch 794/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 10.9284 - val_loss: 212.7605\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 100.85207\n",
      "Epoch 795/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.5414 - val_loss: 218.2269\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 100.85207\n",
      "Epoch 796/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.2441 - val_loss: 214.7984\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 100.85207\n",
      "Epoch 797/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.0755 - val_loss: 201.0609\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 100.85207\n",
      "Epoch 798/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 11.8954 - val_loss: 207.2751\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 100.85207\n",
      "Epoch 799/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 10.6573 - val_loss: 216.5417\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 100.85207\n",
      "Epoch 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 548us/step - loss: 10.2995 - val_loss: 195.9188\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 100.85207\n",
      "Epoch 801/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.8838 - val_loss: 200.8846\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 100.85207\n",
      "Epoch 802/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.1797 - val_loss: 205.2349\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 100.85207\n",
      "Epoch 803/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 11.3915 - val_loss: 208.7402\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 100.85207\n",
      "Epoch 804/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 12.1205 - val_loss: 209.8144\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 100.85207\n",
      "Epoch 805/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 11.8789 - val_loss: 207.7967\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 100.85207\n",
      "Epoch 806/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 12.6714 - val_loss: 208.2399\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 100.85207\n",
      "Epoch 807/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 12.2638 - val_loss: 220.6548\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 100.85207\n",
      "Epoch 808/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.8889 - val_loss: 198.7703\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 100.85207\n",
      "Epoch 809/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 12.2189 - val_loss: 202.1871\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 100.85207\n",
      "Epoch 810/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 12.5597 - val_loss: 202.0831\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 100.85207\n",
      "Epoch 811/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 12.6245 - val_loss: 215.0076\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 100.85207\n",
      "Epoch 812/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 12.0107 - val_loss: 223.2993\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 100.85207\n",
      "Epoch 813/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 12.7076 - val_loss: 196.8398\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 100.85207\n",
      "Epoch 814/1000\n",
      "96/96 [==============================] - 0s 591us/step - loss: 11.9145 - val_loss: 223.4012\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 100.85207\n",
      "Epoch 815/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 11.3363 - val_loss: 228.7182\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 100.85207\n",
      "Epoch 816/1000\n",
      "96/96 [==============================] - 0s 582us/step - loss: 11.0218 - val_loss: 219.4801\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 100.85207\n",
      "Epoch 817/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 11.6143 - val_loss: 209.2939\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 100.85207\n",
      "Epoch 818/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.4523 - val_loss: 209.6406\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 100.85207\n",
      "Epoch 819/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 10.9261 - val_loss: 217.7782\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 100.85207\n",
      "Epoch 820/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.5313 - val_loss: 208.1124\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 100.85207\n",
      "Epoch 821/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 11.5392 - val_loss: 196.7819\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 100.85207\n",
      "Epoch 822/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.6942 - val_loss: 214.2263\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 100.85207\n",
      "Epoch 823/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.6462 - val_loss: 205.4414\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 100.85207\n",
      "Epoch 824/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 10.7454 - val_loss: 207.9877\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 100.85207\n",
      "Epoch 825/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 10.2714 - val_loss: 226.7544\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 100.85207\n",
      "Epoch 826/1000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 12.4372 - val_loss: 210.7304\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 100.85207\n",
      "Epoch 827/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 12.5237 - val_loss: 210.9873\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 100.85207\n",
      "Epoch 828/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 12.7557 - val_loss: 216.5476\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 100.85207\n",
      "Epoch 829/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 11.3830 - val_loss: 199.5295\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 100.85207\n",
      "Epoch 830/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.3784 - val_loss: 218.5366\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 100.85207\n",
      "Epoch 831/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 11.1709 - val_loss: 208.6260\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 100.85207\n",
      "Epoch 832/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 10.5952 - val_loss: 194.9572\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 100.85207\n",
      "Epoch 833/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 10.4208 - val_loss: 219.1733\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 100.85207\n",
      "Epoch 834/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 10.1805 - val_loss: 209.3219\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 100.85207\n",
      "Epoch 835/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 9.7117 - val_loss: 205.8733\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 100.85207\n",
      "Epoch 836/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 10.1490 - val_loss: 208.4482\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 100.85207\n",
      "Epoch 837/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 10.0121 - val_loss: 202.6309\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 100.85207\n",
      "Epoch 838/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 10.3182 - val_loss: 195.9007\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 100.85207\n",
      "Epoch 839/1000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 10.1860 - val_loss: 240.7532\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 100.85207\n",
      "Epoch 840/1000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 10.8487 - val_loss: 238.9547\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 100.85207\n",
      "Epoch 841/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 10.1525 - val_loss: 204.3506\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 100.85207\n",
      "Epoch 842/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.5448 - val_loss: 218.3898\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 100.85207\n",
      "Epoch 843/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 11.4564 - val_loss: 230.0805\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 100.85207\n",
      "Epoch 844/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 11.8570 - val_loss: 245.1975\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 100.85207\n",
      "Epoch 845/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 12.5585 - val_loss: 217.8301\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 100.85207\n",
      "Epoch 846/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 11.6689 - val_loss: 222.5142\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 100.85207\n",
      "Epoch 847/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 11.6848 - val_loss: 231.6053\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 100.85207\n",
      "Epoch 848/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 11.1193 - val_loss: 232.8127\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 100.85207\n",
      "Epoch 849/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 10.5819 - val_loss: 211.9424\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 100.85207\n",
      "Epoch 850/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.7159 - val_loss: 215.1667\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 100.85207\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 542us/step - loss: 10.4967 - val_loss: 202.8260\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 100.85207\n",
      "Epoch 852/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 11.3476 - val_loss: 209.2382\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 100.85207\n",
      "Epoch 853/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 10.6070 - val_loss: 230.4567\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 100.85207\n",
      "Epoch 854/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.5953 - val_loss: 213.1921\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 100.85207\n",
      "Epoch 855/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 10.5837 - val_loss: 222.4200\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 100.85207\n",
      "Epoch 856/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 10.3754 - val_loss: 216.4716\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 100.85207\n",
      "Epoch 857/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 10.5115 - val_loss: 210.7156\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 100.85207\n",
      "Epoch 858/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 11.2917 - val_loss: 218.5853\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 100.85207\n",
      "Epoch 859/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.4098 - val_loss: 214.4932\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 100.85207\n",
      "Epoch 860/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.8387 - val_loss: 237.1122\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 100.85207\n",
      "Epoch 861/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.2276 - val_loss: 220.5343\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 100.85207\n",
      "Epoch 862/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 12.2244 - val_loss: 217.9684\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 100.85207\n",
      "Epoch 863/1000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 10.8573 - val_loss: 207.4212\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 100.85207\n",
      "Epoch 864/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 10.8328 - val_loss: 218.9715\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 100.85207\n",
      "Epoch 865/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 11.2370 - val_loss: 205.1194\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 100.85207\n",
      "Epoch 866/1000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 10.2454 - val_loss: 214.1214\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 100.85207\n",
      "Epoch 867/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.8929 - val_loss: 212.0966\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 100.85207\n",
      "Epoch 868/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 9.9380 - val_loss: 208.4879\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 100.85207\n",
      "Epoch 869/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 10.1145 - val_loss: 228.2896\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 100.85207\n",
      "Epoch 870/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 9.9347 - val_loss: 227.0911\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 100.85207\n",
      "Epoch 871/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 10.4120 - val_loss: 244.9629\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 100.85207\n",
      "Epoch 872/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 10.3038 - val_loss: 225.5763\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 100.85207\n",
      "Epoch 873/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 10.6174 - val_loss: 224.7442\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 100.85207\n",
      "Epoch 874/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 10.6011 - val_loss: 213.0442\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 100.85207\n",
      "Epoch 875/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 11.4391 - val_loss: 211.4973\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 100.85207\n",
      "Epoch 876/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 14.1114 - val_loss: 238.2995\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 100.85207\n",
      "Epoch 877/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 13.3705 - val_loss: 217.2113\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 100.85207\n",
      "Epoch 878/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 14.8207 - val_loss: 248.9552\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 100.85207\n",
      "Epoch 879/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 16.2421 - val_loss: 227.3400\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 100.85207\n",
      "Epoch 880/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 19.3179 - val_loss: 252.5807\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 100.85207\n",
      "Epoch 881/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 20.0143 - val_loss: 227.7886\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 100.85207\n",
      "Epoch 882/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 24.3688 - val_loss: 263.0799\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 100.85207\n",
      "Epoch 883/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 24.0232 - val_loss: 209.3313\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 100.85207\n",
      "Epoch 884/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 26.8316 - val_loss: 275.0398\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 100.85207\n",
      "Epoch 885/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 31.2071 - val_loss: 240.4594\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 100.85207\n",
      "Epoch 886/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 35.8534 - val_loss: 267.7295\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 100.85207\n",
      "Epoch 887/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 32.1331 - val_loss: 189.2813\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 100.85207\n",
      "Epoch 888/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 27.8266 - val_loss: 257.8065\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 100.85207\n",
      "Epoch 889/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 24.6967 - val_loss: 181.7735\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 100.85207\n",
      "Epoch 890/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 24.7968 - val_loss: 222.0671\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 100.85207\n",
      "Epoch 891/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 23.7213 - val_loss: 231.0492\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 100.85207\n",
      "Epoch 892/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 25.3273 - val_loss: 241.8765\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 100.85207\n",
      "Epoch 893/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 23.7358 - val_loss: 217.8065\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 100.85207\n",
      "Epoch 894/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 20.6592 - val_loss: 220.4914\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 100.85207\n",
      "Epoch 895/1000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 27.1487 - val_loss: 196.6275\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 100.85207\n",
      "Epoch 896/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 24.0255 - val_loss: 249.6926\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 100.85207\n",
      "Epoch 897/1000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 18.9834 - val_loss: 224.9890\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 100.85207\n",
      "Epoch 898/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 16.5312 - val_loss: 172.7749\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 100.85207\n",
      "Epoch 899/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 16.7494 - val_loss: 217.8025\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 100.85207\n",
      "Epoch 900/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 14.4344 - val_loss: 223.6940\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 100.85207\n",
      "Epoch 901/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 12.9058 - val_loss: 221.8775\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 100.85207\n",
      "Epoch 902/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 565us/step - loss: 12.3506 - val_loss: 210.2232\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 100.85207\n",
      "Epoch 903/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 11.1009 - val_loss: 204.9147\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 100.85207\n",
      "Epoch 904/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 11.0889 - val_loss: 199.4477\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 100.85207\n",
      "Epoch 905/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 10.6830 - val_loss: 204.2814\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 100.85207\n",
      "Epoch 906/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.7614 - val_loss: 211.9674\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 100.85207\n",
      "Epoch 907/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.9533 - val_loss: 228.7122\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 100.85207\n",
      "Epoch 908/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 11.0439 - val_loss: 204.2645\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 100.85207\n",
      "Epoch 909/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 10.2378 - val_loss: 205.7535\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 100.85207\n",
      "Epoch 910/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.5197 - val_loss: 203.6915\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 100.85207\n",
      "Epoch 911/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 11.1664 - val_loss: 220.0451\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 100.85207\n",
      "Epoch 912/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 10.6783 - val_loss: 207.6429\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 100.85207\n",
      "Epoch 913/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 9.9348 - val_loss: 204.5991\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 100.85207\n",
      "Epoch 914/1000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 10.0487 - val_loss: 209.5678\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 100.85207\n",
      "Epoch 915/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.2520 - val_loss: 215.6718\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 100.85207\n",
      "Epoch 916/1000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 10.2965 - val_loss: 206.6225\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 100.85207\n",
      "Epoch 917/1000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 9.8569 - val_loss: 208.1961\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 100.85207\n",
      "Epoch 918/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 9.9777 - val_loss: 209.9124\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 100.85207\n",
      "Epoch 919/1000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 9.8320 - val_loss: 207.5114\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 100.85207\n",
      "Epoch 920/1000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 10.0561 - val_loss: 199.1691\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 100.85207\n",
      "Epoch 921/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 9.5690 - val_loss: 205.7073\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 100.85207\n",
      "Epoch 922/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 9.7914 - val_loss: 216.0026\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 100.85207\n",
      "Epoch 923/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 9.5339 - val_loss: 211.3328\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 100.85207\n",
      "Epoch 924/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 9.4196 - val_loss: 221.5634\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 100.85207\n",
      "Epoch 925/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 9.3657 - val_loss: 226.7122\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 100.85207\n",
      "Epoch 926/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 9.8693 - val_loss: 226.2709\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 100.85207\n",
      "Epoch 927/1000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 10.3792 - val_loss: 217.8798\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 100.85207\n",
      "Epoch 928/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 10.3207 - val_loss: 216.5343\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 100.85207\n",
      "Epoch 929/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 10.2589 - val_loss: 218.4820\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 100.85207\n",
      "Epoch 930/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 10.6096 - val_loss: 184.9369\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 100.85207\n",
      "Epoch 931/1000\n",
      "96/96 [==============================] - 0s 577us/step - loss: 10.3553 - val_loss: 231.7857\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 100.85207\n",
      "Epoch 932/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 10.3524 - val_loss: 229.3930\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 100.85207\n",
      "Epoch 933/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 10.2679 - val_loss: 200.3853\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 100.85207\n",
      "Epoch 934/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.0212 - val_loss: 213.0144\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 100.85207\n",
      "Epoch 935/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 10.0220 - val_loss: 209.0607\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 100.85207\n",
      "Epoch 936/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.4785 - val_loss: 208.9784\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 100.85207\n",
      "Epoch 937/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 9.9853 - val_loss: 212.4574\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 100.85207\n",
      "Epoch 938/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.9457 - val_loss: 226.5163\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 100.85207\n",
      "Epoch 939/1000\n",
      "96/96 [==============================] - 0s 578us/step - loss: 11.4486 - val_loss: 217.5634\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 100.85207\n",
      "Epoch 940/1000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 13.3809 - val_loss: 223.2085\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 100.85207\n",
      "Epoch 941/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 13.6021 - val_loss: 197.2549\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 100.85207\n",
      "Epoch 942/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 12.9094 - val_loss: 213.8969\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 100.85207\n",
      "Epoch 943/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 11.9832 - val_loss: 255.2310\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 100.85207\n",
      "Epoch 944/1000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 12.4594 - val_loss: 216.0634\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 100.85207\n",
      "Epoch 945/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.1757 - val_loss: 219.1726\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 100.85207\n",
      "Epoch 946/1000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 10.2074 - val_loss: 225.2412\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 100.85207\n",
      "Epoch 947/1000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 9.9677 - val_loss: 192.6281\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 100.85207\n",
      "Epoch 948/1000\n",
      "96/96 [==============================] - 0s 570us/step - loss: 9.5961 - val_loss: 209.5414\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 100.85207\n",
      "Epoch 949/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 9.3213 - val_loss: 208.5593\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 100.85207\n",
      "Epoch 950/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 10.7583 - val_loss: 212.7456\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 100.85207\n",
      "Epoch 951/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 10.2904 - val_loss: 221.9923\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 100.85207\n",
      "Epoch 952/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 9.7732 - val_loss: 221.1407\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 100.85207\n",
      "Epoch 953/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 566us/step - loss: 9.5738 - val_loss: 229.5804\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 100.85207\n",
      "Epoch 954/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 9.6133 - val_loss: 217.1431\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 100.85207\n",
      "Epoch 955/1000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 9.2900 - val_loss: 231.2693\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 100.85207\n",
      "Epoch 956/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 10.5531 - val_loss: 247.2534\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 100.85207\n",
      "Epoch 957/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 11.6916 - val_loss: 248.7481\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 100.85207\n",
      "Epoch 958/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 14.4219 - val_loss: 221.5774\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 100.85207\n",
      "Epoch 959/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.6802 - val_loss: 242.3745\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 100.85207\n",
      "Epoch 960/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 18.6601 - val_loss: 243.6645\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 100.85207\n",
      "Epoch 961/1000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 18.7209 - val_loss: 235.9911\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 100.85207\n",
      "Epoch 962/1000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 14.9313 - val_loss: 241.5408\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 100.85207\n",
      "Epoch 963/1000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 13.8730 - val_loss: 204.1477\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 100.85207\n",
      "Epoch 964/1000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 13.5816 - val_loss: 251.3402\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 100.85207\n",
      "Epoch 965/1000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 12.7986 - val_loss: 253.5736\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 100.85207\n",
      "Epoch 966/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.6218 - val_loss: 214.9712\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 100.85207\n",
      "Epoch 967/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 11.3537 - val_loss: 228.9665\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 100.85207\n",
      "Epoch 968/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 11.0050 - val_loss: 226.7227\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 100.85207\n",
      "Epoch 969/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 11.4144 - val_loss: 227.2234\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 100.85207\n",
      "Epoch 970/1000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 10.4367 - val_loss: 233.3705\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 100.85207\n",
      "Epoch 971/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.6593 - val_loss: 228.0080\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 100.85207\n",
      "Epoch 972/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 10.8797 - val_loss: 204.3319\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 100.85207\n",
      "Epoch 973/1000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 10.4345 - val_loss: 247.7003\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 100.85207\n",
      "Epoch 974/1000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 10.2216 - val_loss: 231.7392\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 100.85207\n",
      "Epoch 975/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 9.9718 - val_loss: 211.0541\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 100.85207\n",
      "Epoch 976/1000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 10.0876 - val_loss: 232.5537\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 100.85207\n",
      "Epoch 977/1000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 9.9904 - val_loss: 237.9439\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 100.85207\n",
      "Epoch 978/1000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 9.7033 - val_loss: 218.3760\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 100.85207\n",
      "Epoch 979/1000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 9.4783 - val_loss: 218.5520\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 100.85207\n",
      "Epoch 980/1000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 9.8759 - val_loss: 213.9579\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 100.85207\n",
      "Epoch 981/1000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 9.3261 - val_loss: 223.3081\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 100.85207\n",
      "Epoch 982/1000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 9.9741 - val_loss: 207.9833\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 100.85207\n",
      "Epoch 983/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 9.6068 - val_loss: 220.2977\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 100.85207\n",
      "Epoch 984/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 9.3059 - val_loss: 226.3467\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 100.85207\n",
      "Epoch 985/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 10.1025 - val_loss: 249.1540\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 100.85207\n",
      "Epoch 986/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 9.8356 - val_loss: 250.1832\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 100.85207\n",
      "Epoch 987/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 9.7208 - val_loss: 229.1133\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 100.85207\n",
      "Epoch 988/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 9.8840 - val_loss: 234.5545\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 100.85207\n",
      "Epoch 989/1000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 10.0206 - val_loss: 238.9103\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 100.85207\n",
      "Epoch 990/1000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 9.7825 - val_loss: 225.9112\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 100.85207\n",
      "Epoch 991/1000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 10.3144 - val_loss: 246.9063\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 100.85207\n",
      "Epoch 992/1000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 9.7455 - val_loss: 229.7361\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 100.85207\n",
      "Epoch 993/1000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.7508 - val_loss: 230.2148\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 100.85207\n",
      "Epoch 994/1000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.0371 - val_loss: 209.4945\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 100.85207\n",
      "Epoch 995/1000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 16.2911 - val_loss: 215.1840\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 100.85207\n",
      "Epoch 996/1000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 15.7330 - val_loss: 201.9961\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 100.85207\n",
      "Epoch 997/1000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 16.0308 - val_loss: 233.6163\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 100.85207\n",
      "Epoch 998/1000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 16.7589 - val_loss: 200.9389\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 100.85207\n",
      "Epoch 999/1000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 15.5277 - val_loss: 231.3433\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 100.85207\n",
      "Epoch 1000/1000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 12.4580 - val_loss: 221.4363\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 100.85207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e140411d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "vae.fit(x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None), \n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_output = sess.run( z_mean, feed_dict={'encoder_input: 0':x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12817204, 0.14783967], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m = np.mean(z_output,axis=0)\n",
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.84460267, 0.39013891],\n",
       "       [0.39013891, 2.372855  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(z_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Pearson correlation among Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e168f4c88>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(np.corrcoef(z_output.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12817204, 0.14783967], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, latent_dim):\n",
    "    plot_results(models,\n",
    "                 latent_dim,\n",
    "                 latent_num = i,\n",
    "                 z_m_m = z_m_m ,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
